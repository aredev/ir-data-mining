1 en 767 self organization of associative database and its applications hisashi suzuki and suguru arimoto osaka university  toyonaka  osaka 560  japan abstract an efficient method of self organizing associative databases is proposed together with applications to robot eyesight systems  the proposed databases can associate any input with some output  in the first half part of discussion  an algorithm of self organization is proposed  from an aspect of hardware  it produces a new style of neural network  in the latter half part  an applicability to handwritten letter recognition and that to an autonomous mobile robot system are demonstrated  introduction let a mapping f  x   y be given  here  x is a finite or infinite set  and y is another finite or infinite set  a learning machine observes any set of pairs lrb x  y rrb sampled randomly from x x y lrb x x y means the cartesian product of x and y rrb and  it computes some estimate j  x   y of f to make small  the estimation error in some measure  usually we say that  the faster the decrease of estimation error with increase of the number of samples  the better the learning machine  however  such expression on performance is incomplete  since  it lacks consideration on the candidates of j of j assumed preliminarily  then  how should we find out good learning machines  to clarify this conception  let us discuss for a while on some types of learning machines  and  let us advance the understanding of the self organization of associative database   parameter type an ordinary type of learning machine assumes an equation relating x s and y s with parameters being indefinite  namely  a structure of f  it is equivalent to define implicitly a set f of candidates of lrb f is some subset of mappings from x to y rrb and  it computes values of the parameters based on the observed samples  we call such type a parameter type  for a learning machine defined well  if f 3 f  j approaches f as the number of samples increases  in the alternative case  however  some estimation error remains eternally  thus  a problem of designing a learning machine returns to find out a proper structure of f in this sense  on the other hand  the assumed structure of f is demanded to be as compact as possible to achieve a fast learning  in other words  the number of parameters should be small  since  if the parameters are few  some j can be uniquely determined even though the observed samples are few  however  this demand of being proper contradicts to that of being compact  consequently  in the parameter type  the better the compactness of the assumed structure that is proper  the better the learning machine  this is the most elementary conception when we design learning machines  1   universality and ordinary neural networks now suppose that a sufficient knowledge on f is given though j itself is unknown  in this case  it is comparatively easy to find out proper and compact structures of j  in the alternative case  however  it is sometimes difficult  a possible solution is to give up the compactness and assume an almighty structure that can cover various 1 s  a combination of some orthogonal bases of the infinite dimension is such a structure  neural networks 1 2 are its approximations obtained by truncating finitely the dimension for implementation   american institute of physics 1988 768 a main topic in designing neural networks is to establish such desirable structures of 1  this work includes developing practical procedures that compute values of coefficients from the observed samples  such discussions are  flourishing since 1980 while many efficient methods have been proposed  recently  even hardware units computing coefficients in parallel for speed up are sold  e g  anza  mark iii  odyssey and e 1  nevertheless  in neural networks  there always exists a danger of some error remaining eternally in estimating   precisely speaking  suppose that a combination of the bases of a finite number can define a structure of 1 essentially  in other words  suppose that f 3   or 1 is located near f  in such case  the estimation error is none or negligible  however  if 1 is distant from f  the estimation error never becomes negligible  indeed  many researches report that the following situation appears when 1 is too complex  once the estimation error converges to some value lrb  0 rrb as the number of samples increases  it decreases hardly even though the dimension is heighten  this property sometimes is a considerable defect of neural networks   recursi ve type the recursive type is founded on another methodology of learning that should be as follows  at the initial stage of no sample  the set fa lrb instead of notation f rrb of candidates of i equals to the set of all mappings from x to y  after observing the first sample lrb xl  yl rrb e x x y  fa is reduced to fi so that i lrb xt rrb  yl for any i e f  after observing the second sample lrb x2  y2 rrb e x x y  fl is further reduced to f2 so that i lrb xt rrb  yl and i lrb x2 rrb  y2 for any i e f thus  the candidate set f becomes gradually small as observation of samples proceeds  the after observing i samples  which we write is one of the most likelihood estimation of 1 selected in fi   hence  contrarily to the parameter type  the recursive type guarantees surely that j approaches to 1 as the number of samples increases  the recursive type  if observes a sample lrb x  yd  rewrites values 1 l lrb x rrb  s to i  lrb x rrb s for some x s correlated to the sample  hence  this type has an architecture composed of a rule for rewriting and a free memory space  such architecture forms naturally a kind of database that builds up management systems of data in a self organizing way  however  this database differs from ordinary ones in the following sense  it does not only record the samples already observed  but computes some estimation of l lrb x rrb for any x e x  we call such database an associative database  the first subject in constructing associative databases is how we establish the rule for rewri ting  for this purpose  we adap t a measure called the dissimilari ty  here  a dissimilari ty means a mapping d  x x x   lcb reals  o rcb such that for any lrb x  x rrb e x x x  d lrb x  x rrb  0 whenever l lrb x rrb t  lrb x rrb  however  it is not necessarily defined with a single formula  it is definable with  for example  a collection of rules written in forms of  if    then     the dissimilarity d defines a structure of 1 locally in x x y hence  even though the knowledge on f is imperfect  we can re  flect it on d in some heuristic way  hence  contrarily to neural networks  it is possible to accelerate the speed of learning by establishing d well  especially  we can easily find out simple d s for those l s which process analogically information like a human  lrb see the applications in this paper  rrb and  for such  s  the recursive type shows strongly its effectiveness  we denote a sequence of observed samples by lrb xl  yd  lrb x2  y2 rrb    one of the simplest constructions of associative databases after observing i samples lrb i  1 2     rrb is as follows  i i  i  algorithm 1  at the initial stage  let so be the empty set  for every i  1 2     let i l lrb x rrb for any x e x equal some y  such that lrb x   y  rrb e s l and d lrb x  x  rrb  min lrb   y rrb es t d lrb x  x rrb  furthermore  add lrb x  y  rrb to s   l to produce sa  i e  s   s   l u lcb lrb x  lrb 1 rrb y  n 769 another version improved to economize the memory is as follows  algorithm 2  at the initial stage  let so be composed of an arbitrary element in x x y for every i  1 2    let ii lex rrb for any x e x equal some y such that lrb x   y rrb e si l and d lrb x  x  rrb  min d lrb x  x rrb  lrb i  i rrb es l furthermore  if ii l lrb xi rrb  yi then let si  si l  or add lrb xi  yi rrb to si l to produce si  i e  si  si l u lcb lrb xi  yi rrb rcb  in either construction  ii approaches to f as i increases  however  the computation time grows proportionally to the size of si  the second subject in constructing associative databases is what addressing rule we should employ to economize the computation time  in the subsequent chapters  a construction of associative database for this purpose is proposed  it manages data in a form of binary tree  self organization of associative database given a sample sequence lrb xl  yl rrb  lrb x2  y2 rrb     the algorithm for constructing associative database is as follows  algorithm 3   step i lrb initialization rrb  let lrb x lsb root rsb  y lsb root rsb rrb  lrb xl  yd  here  x lsb  rsb and y lsb  rsb are variables assigned for respective nodes to memorize data   furthermore  let t  1  step 2  increase t by 1  and put x  in  after reset a pointer n to the root  repeat the following until n arrives at some terminal node  i e  leaf  notations nand d lrb xt  x lsb n rrb rrb  let n n mean the descendant nodes of n  n otherwise  let n  n  if d lrb x  r lsb n rrb rrb  step 3  display yin rsb as the related information  next  put y  in  if yin rsb  y  back to step 2  otherwise  first establish new descendant nodes n and n secondly  let lrb x lsb n rsb  yin rrb rrb lrb x lsb n rsb  yin rrb rrb lrb x lsb n rsb  yin rrb rrb  lrb xt  y  rrb  lrb 2 rrb lrb 3 rrb finally  back to step 2  here  the loop of step 2 3 can be stopped at any time and also can be continued  now  suppose that gate elements  namely  artificial  synapses  that play the role of branching by d are prepared  then  we obtain a new style of neural network with gate elements being randomly connected by this algorithm  letter recognition recen tly  the vertical slitting method for recognizing typographic english letters3  the elastic matching method for recognizing hand written discrete english letters4  the global training and fuzzy logic search method for recognizing chinese characters written in square styles  etc are published  the self organization of associative database realizes the recognition of handwritten continuous english letters  770 9  wn  nov     xk  la t       dw1lo       of      4      4fig  1  source document  2     loo    h o o fig 2  windowing  1000 2000 3000 4000 number of samples o 1000 2000 3000 4000 nualber of sampl es fig 3  an experiment result  an image scanner takes a document image lrb fig 1 rrb  the letter recognizer uses a parallelogram window that at least can cover the maximal letter lrb fig 2 rrb  and processes the sequence of letters while shifting the window  that is  the recognizer scans a word in a slant direction  and  it places the window so that its left vicinity may be on the first black point detected  then  the window catches a letter and some part of the succeeding letter  if recognition of the head letter is performed  its end position  namely  the boundary line between two letters becomes known  hence  by starting the scanning from this boundary and repeating the above operations  the recognizer accomplishes recursively the task  thus the major problem comes to identifying the head letter in the window  considering it  we define the following   regard window images as x s  and define x accordingly   for a lrb x  x rrb e x x x  denote by b a black point in the left area from the boundary on window image x project each b onto window image x then  measure the euclidean distance 6 between fj and a black point b on x being the closest to b let d lrb x  x rrb be the summation of 6 s for all black points b s on x divided by the number of b s   regard couples of the  reading  and the position of boundary as y s  and define y accordingly  an operator teaches the recognizer in interaction the relation between window image and reading  boundary with algorithm 3  precisely  if the recalled reading is incorrect  the operator teaches a correct reading via the console  moreover  if the boundary position is incorrect  he teaches a correct position via the mouse  fig 1 shows partially a document image used in this experiment  fig 3 shows the change of the number of nodes and that of the recognition rate defined as the relative frequency of correct answers in the past 1000 trials  speciiications of the window are height  20dot  width  10dot  and slant angular  68deg  in this example  the levels of tree were distributed in 6 19 at time 4000 and the recognition rate converged to about 74   experimentally  the recognition rate converges to about 60 85  in most cases  and to 95  at a rare case  however  it does not attain 100  since  e g   c  and  e  are not distinguishable because of excessive lluctuation in writing  if the consistency of the x  y relation is not assured like this  the number of nodes increases endlessly lrb d fig 3 rrb  hence  it is clever to stop the learning when the recognition rate attains some upper limit  to improve further the recognition rate  we must consider the spelling of words  it is one of future subjects  771 obstacle avoiding movement various systems of camera type autonomous mobile robot are reported flourishingly6 1o  the system made up by the authors lrb fig 4 rrb also belongs to this category  now  in mathematical methodologies  we solve usually the problem of obstacle avoiding movement as a cost minimization problem under some cost criterion established artificially  contrarily  the self organization of associative database reproduces faithfully the cost criterion of an operator  therefore  motion of the robot after learning becomes very natural  now  the length  width and height of the robot are all about o 7m  and the weight is about 30kg  the visual angle of camera is about 55deg  the robot has the following three factors of motion  it turns less than  30deg  advances less than 1m  and controls speed less than 3km h  the experiment was done on the passageway of wid th 2 5 m inside a building which the authors  laboratories exist in lrb fig 5 rrb  because of an experimental intention  we arrange boxes  smoking stands  gas cylinders  stools  handcarts  etc on the passage way at random  we let the robot take an image through the camera  recall a similar image  and trace the route preliminarily recorded on it  for this purpose  we define the following   let the camera face 28deg downward to take an image  and process it through a low pass filter  scanning vertically the filtered image from the bottom to the top  search the first point c where the luminance changes excessively  then  su bstitu te all points from the bottom to c for white  and all points from c to the top for black lrb fig 6 rrb  lrb if no obstacle exists just in front of the robot  the white area shows the  free  area where the robot can move around  rrb regard binary 32 x 32dot images processed thus as x s  and define x accordingly   for every lrb x  x rrb e x x x  let d lrb x  x rrb be the number of black points on the exclusive or image between x and x  regard as y s the images obtained by drawing routes on images x s  and define y accordingly  the robot superimposes  on the current camera image x  the route recalled for x  and inquires the operator instructions  the operator judges subjectively whether the suggested route is appropriate or not  in the negative answer  he draws a desirable route on x with the mouse to teach a new y to the robot  this opera tion defines implicitly a sample sequence of lrb x  y rrb reflecting the cost criterion of the operator     l    iibube    22 11 roan 12 lcb  13 stationary uni t fig 4  configuration of autonomous mobile robot system   i  23 24 north 14 rmbi ie unit lrb robot rrb  roan y t fig 5  experimental environment  772 wall camera image preprocessing a     fa  preprocessing 0 o course suggest ion    search a fig 6  processing for obstacle avoiding movement  x fig 1  processing for position identification  we define the satisfaction rate by the relative frequency of acceptable suggestions of route in the past 100 trials  in a typical experiment  the change of satisfaction rate showed a similar tendency to fig 3  and it attains about 95  around time 800  here  notice that the rest 5  does not mean directly the percentage of collision  lrb in practice  we prevent the collision by adopting some supplementary measure  rrb at time 800  the number of nodes was 145  and the levels of tree were distributed in 6 17  the proposed method reflects delicately various characters of operator  for example  a robot trained by an operator 0 moves slowly with enough space against obstacles while one trained by another operator 0  brushes quickly against obstacles  this fact gives us a hint on a method of printing  characters  into machines  position identification the robot can identify its position by recalling a similar landscape with the position data to a camera image  for this purpose  in principle  it suffices to regard camera images and position data as x s and y s  respectively  however  the memory capacity is finite in actual compu ters  hence  we can not but compress the camera images at a slight loss of information  such compression is admittable as long as the precision of position identification is in an acceptable area  thus  the major problem comes to find out some suitable compression method  in the experimental environment lrb fig 5 rrb  juts are on the passageway at intervals of 3 6 m  and each section between adjacent juts has at most one door  the robot identifies roughly from a surrounding landscape which section itself places in  and  it uses temporarily a triangular surveying technique if an exact measure is necessary  to realize the former task  we define the following   turn the camera to take a panorama image of 360deg  scanning horizontally the center line  substitute the points where the luminance excessively changes for black and the other points for white lrb fig 1 rrb  regard binary 360dot line images processed thus as x s  and define x accordingly   for every lrb x  x rrb e x x x  project each black point a on x onto x and  measure the euclidean distance 6 between a and a black point a on x being the closest to a let the summation of 6 be s similarly  calculate s by exchanging the roles of x and x denoting the numbers of a s and a s respectively by nand n  define 773 d lrb x  x rrb   lrb    rrb  2 n n lrb 4 rrb  regard positive integers labeled on sections as y s lrb cf fig 5 rrb  and define y accordingly  in the learning mode  the robot checks exactly its position with a counter that is reset periodically by the operator  the robot runs arbitrarily on the passageways within 18m area and learns the relation between landscapes and position data  lrb position identification beyond 18m area is achieved by crossing plural databases one another  rrb this task is automatic excepting the periodic reset of counter  namely  it is a kind of learning without teacher  we define the identification rate by the relative frequency of correct recalls of position data in the past 100 trials  in a typical example  it converged to about 83  around time 400  at time 400  the number of levels was 202  and the levels oftree were distributed in 522  since the identification failures of 17  can be rejected by considering the trajectory  no pro blem arises in practical use  in order to improve the identification rate  the compression ratio of camera images must be loosened  such possibility depends on improvement of the hardware in the future  fig 8 shows an example of actual motion of the robot based on the database for obstacle avoiding movement and that for position identification  this example corresponds to a case of moving from 14 to 23 in fig 5  here  the time interval per frame is about 40sec       lrb    i        i i    i    1 t  i        ii fig 8  actual motion of the robot  774 conclusion a method of self organizing associative databases was proposed with the application to robot eyesight systems  the machine decomposes a global structure unknown into a set of local structures known and learns universally any input output response  this framework of problem implies a wide application area other than the examples shown in this paper  a defect of the algorithm 3 of self organization is that the tree is balanced well only for a subclass of structures of f  a subject imposed us is to widen the class  a probable solution is to abolish the addressing rule depending directly on values of d and  instead  to establish another rule depending on the distribution function of values of d  it is now under investigation  references 1  hopfield  j j and d w tank   computing with neural circuit  a model   science 233 lrb 1986 rrb  pp 625 633  2  rumelhart  d e et al   learning representations by back propagating errors   nature 323 lrb 1986 rrb  pp 533 536  3  hull  j j   hypothesis generation in a computational model for visual word recognition   ieee expert  fall lrb 1986 rrb  pp 63 70  4  kurtzberg  j m   feature analysis for symbol recognition by elastic matching   ibm j res  develop  31 1 lrb 1987 rrb  pp 91 95  5  wang  q r and c y suen   large tree classifier with heuristic search and global training   ieee trans  pattern  anal   mach  intell  pami 9 1 lrb 1987 rrb pp 91 102  6  brooks  r a et al   self calibration of motion and stereo vision for mobile robots   4th int  symp  of robotics research lrb 1987 rrb  pp 267 276  7  goto  y and a stentz   the cmu system for mobile robot navigation   1987 ieee int  conf  on robotics  automation lrb 1987 rrb  pp 99 105  8  madarasz  r et al   the design of an autonomous vehicle for the disabled   ieee jour  of robotics  automation ra 2 3 lrb 1986 rrb  pp 117 125  9  triendl  e and d j kriegman   stereo vision and navigation within buildings   1987 ieee int  conf  on robotics  automation lrb 1987 rrb  pp 1725 1730  10  turk  m a et al   video road following for the autonomous land vehicle   1987 ieee int  conf  on robotics  automation lrb 1987 rrb  pp 273 279 
2 en 184 the capacity of the kanerva associative memory is exponential p a choul stanford university  stanford  ca 94305 abstract the capacity of an associative memory is defined as the maximum number of vords that can be stored and retrieved reliably by an address vithin a given sphere of attraction  it is shown by sphere packing arguments that as the address length increases  the capacity of any associati ve memory is limited to an exponential grovth rate of 1  h2 lrb 0 rrb  vhere h2 lrb 0 rrb is the binary entropy function in bits  and 0 is the radius of the sphere of attraction  this exponential grovth in capacity can actually be achieved by the kanerva associative memory  if its parameters are optimally set  formulas for these op timal values are provided  the exponential grovth in capacity for the kanerva associative memory contrasts sharply vith the sub linear grovth in capacity for the hopfield associative memory  associative memory and its capacity our model of an associative memory is the folloving  let lrb rrb lrb  y rrb be an lrb address  datum rrb pair  vhere rrb lrb is a vector of n  ls and y is a vector of m  ls  and let lrb rrb lrb l rrb  y lrb i rrb rrb    lrb rrb lrb m rrb  y lrb m rrb rrb  be m lrb address  datum rrb pairs stored in an associative memory  if the associative memory is presented at the input vith an address rrb lrb that is close to some stored address rrb lrb w then it should produce at the output a vord y that is close to the corresponding contents y lrb j rrb  to be specific  let us say that an associative memory can correct fraction 0 errors if an rrb lrb vi thin hamming distance no of rrb lrb lrb j rrb retrieves y equal to y lrb j rrb  the hamming sphere around each rrb lrb w vill be called the sphere of attraction  and 0 viii be called the radius of attraction  one notion of the capacity of this associative memory is the maximum number of vords that it can store vhile correcting fraction 0 errors  unfortunately  this notion of capacity is ill defined  because it depends on exactly vhich lrb address  datum rrb pairs have been stored  clearly  no associative memory can correct fraction 0 errors for every sequence of stored lrb address  datum rrb pairs  consider  for example  a sequence in vhich several different vords are vritten to the same address  no memory can reliably retrieve the contents of the overvritten vords  at the other extreme  any associative memory  can store an unlimited number of vords and retrieve them all reliably  if their contents are identical  a useful definition of capacity must lie somevhere betveen these tvo extremes  in this paper  ve are interested in the largest m such that for most sequences of addresses xu rrb      x lrb m rrb and most sequences of data y lrb l rrb    y lrb m rrb  the memory can correct fraction 0 errors  we define ithis vork vas supported by the national science foundation under nsf grant ist 8509860 and by an ibm doctoral fellovship   american institute of physics 1988 185 most sequences  in a probabilistic sense  as some set of sequences yi th total probability greater than say  99  when all sequences are equiprobab1e  this reduces to the deterministic version  991  of all sequences  in practice it is too difficult to compute the capacity of a given associative memory yith inputs of length n and outputs of length tn  fortunately  though  it is easier to compute the asymptotic rate at which a1 increases  as n and tn increase  for a given family of associative memories  this is the approach taken by mceliece et al lsb 1 rsb toyards the capacity of the hopfield associative memory  we take the same approach tovards the capacity of the kanerva associative memory  and tovards the capacities of associative memories in general  in the next section ve provide an upper bound on the rate of grovth of the capacity of any associative memory fitting our general model  it is shown by sphere packing arguments that capacity is limited to an exponential rate of grovth of 1  h2 lrb t5 rrb  vhere h2 lrb t5 rrb is the binary entropy function in bits  and 8 is the radius of attraction  in a later section it vill turn out that this exponential grovth in capacity can actually be achieved by the kanerva associative memory  if its parameters are optimally set  this exponential grovth in capacity for the kanerva associative memory contrasts sharply yith the sub linear grovth in capacity for the hopfield associative memory lsb 1 rsb  i a universal upper bound on capacity recall that our definition of the capacity of an associative memory is the largest a1 such that for most sequences of addresses x lrb 1 rrb    x lrb m rrb and most sequences of data y lrb l rrb    y lrb m rrb  the memory can correct fraction 8 errors  clearly  an upper bound to this capacity is the largest af for vhich there exists some sequence of addresses x lrb 1 rrb    x lrb m rrb such that for most sequences of data y lrb l rrb    y lrb m rrb  the memory can correct fraction 8 errors  we nov derive an expression for this upper bound  let 8 be the radius of attraction and let dh lrb x lrb i rrb  d rrb be the sphere of attraction  i e  the set of all xs at most hamming distance d  ln8j from  y lrb j rrb  since by assumption the memory corrects fraction 8 errors  every address x e dh lrb xu rrb  d rrb retrieves the vord yw  the size of dh lrb xu rrb  d rrb is easily shown to be independent of xu rrb and equal to vn d  2    0 vhere is the binomial coefficient n jk  lrb n  k rrb   thus n out of a total of 2 n bit addresses  at least vn d addresses retrieve y lrb l rrb  at least vn d addresses retrieve y lrb 2 rrb  at least vn d addresses retrieve y lrb   and so forth  it fol10vs that the total number of distinct yu rrb s can be at most 2 n jv n  d  nov  from stirling s formula it can be shovn that if d  s  nj2  then vn d  2nh2 lrb d n rrb  o lrb logn rrb  vhere h 2 lrb 8 rrb  81 og 2 8  lrb 1  8 rrb log2 lrb 1  8 rrb is the binary entropy function in bits  and o lrb logn rrb is some function yhose magnitude grovs more slovly than a constant times log n thus the total number of distinct y lrb j rrb s can be at most 2 n lrb 1 h2 lrb s   o lrb logn rrb since any set containing i most sequences  of af tn bit vords vill contain a large number of distinct vords lrb if tn is lrb 1 rrb  lrb i rrb 186 figure 1  neural net representation of the kanerva associative memory  signals propagate from the bottom lrb input rrb to the top lrb output rrb  each arc multiplies the signal by its weight  each node adds the incoming signals and then thresholds  sufficiently large  see lsb 2 rsb for details rrb  it follovs that m 5 2 n lrb l h 2 lrb o   o lrb logn rrb  lrb 1 rrb in general a function fen rrb is said to be o lrb g lrb n rrb rrb if f lrb n rrb fg lrb n rrb is bounded  i e  if there exists a constant a such that if lrb n rrb 1 5 a  g lrb n rrb 1 for all n thus lrb 1 rrb says that there exists a constant a such that m 5 2 n lrb l h 2 lrb s   alogn  it should be emphasized that since a is unknow  this bound has no meaning for fixed n hovever  it indicates that asymptotically in n  the maximum exponential rate of grovth of m is 1  h2 lrb 6 rrb  intui ti vely  only a sequence of addresses x lrb l rrb    x lrb m rrb that optimally pack the address space lcb  l   l rcb n can hope to achieve this upper bound  remarkably  most such sequences are optimal in this sense  vhen n is large  the kanerva associative memory can take advantage of this fact  the kanerva associative memory the kanerva associative memory lsb 3 4 rsb can be regarded as a tvo layer neural netvork  as shovn in figure 1  vhere the first layer is a preprocessor and the second layer is the usual hopfield style array  the preprocessor essentially encodes each n bit input address into a very large k bit internal representation  k  n  vhose size will be permitted to grov exponentially in n  it does not seem surprising  then  that the capacity of the kanerva associative memory can grov exponentially in n  for it is knovn that the capacity of the hopfield array grovs almost linearly in k  assuming the coordinates of the k vector are dravn at random by independent flips of a fair coin lsb 1 rsb  187 figure 2  matrix representation of the kanerva associative memory  signals propagate from the right lrb input rrb to the left lrb output rrb  dimensions are shown in the box corners  circles stand for functional composition  dots stand for matrix multiplication  in this situation  hovever  such an assumption is ridiculous  since the k bit internal representation is a function of the n bit input address  it can contain at most n bits of information  whereas independent flips of a fair coin contain k bits of information  kanerva s primary contribution is therefore the specification of the preprocessor  that is  the specification of how to map each n bit input address into a very large k bit internal representation  the operation of the preprocessor is easily described  consider the matrix representation shovn in figure 2  the matrix z is randomly populated vith  ls  this randomness assumption is required to ease the analysis  the function fr is 1 in the ith coordinate if the ith row of z is within hamming distance r of x  and is oothervise  this is accomplished by thresholding the ith input against n 2r  the parameters rand k are two essential parameters in the kanerva associative memory  if rand k are set correctly  then the number of 1s in the representation fr lrb zx rrb vill be very small in comparison to the number of os  hence fr lrb z  y rrb can be considered to be a sparse internal representation of x  the second stage of the memory operates in the usual way  except on the internal representation of x  that is  y  g lrb w fr lrb zx rrb rrb  vhere m l v  lyu rrb lsb jr lrb zxu rrb rrb rsb t  lrb 2 rrb i  l and 9 is the threshold function whose ith coordinate is 1 if the ith input is greater than 0 and 1 is the ith input is less than o  the ith column of l v can be regarded as a memory location vhose address is the ith row of z every x vi thin hamming distance r of the ith rov of z accesses this location  hence r is known as the access radius  and k is the number of memory locations  the approach taken in this paper is to fix the linear rate p at which r grovs vith n  and to fix the exponential rate  at which k grovs with n  it turns out that the capacity then grovs at a fixed exponential rate cp   lrb t5 rrb  depending on p    and 15  these exponential rates are sufficient to overcome the standard loose but simple polynomial bounds on the errors due to combinatorial approximations  188 the capacity of the kanerva associative memory fix 0  k  1  0  p  1 2  and 0  0  min lcb 2p 1  2 rcb  let n be the input address length  and let tn be the output word length  it is assumed that tn is at most polynomial in n  i e  tn  exp lcb o lrb logn rrb rcb  let r  ijmj be the access radius  let k  2 l  nj be the number of memory locations  and let d  lonj be the radius of attraction  let afn be the number of stored words  the components of the n vectors x lrb l rrb      x lrb mn rrb  the m vectors y lrb l rrb    y lrb  yn rrb  and the k x n matrix z are assumed to be lid equiprobable  1 random variables  finally  given an n vector x  let y  g lrb w fr lrb zx rrb rrb where w  ef  nl yu rrb lsb jr lrb zxw rrb jf  define the quantity cp   lrb 0 rrb  lcb 26  2 lrb 1  0 rrb h lrb p    2 rrb  cp  ico lrb p rrb lrb o rrb where ko lrb p rrb 2h lrb p rrb 2   2 lrb 1   rrb h lrb p  242 rrb  2h lrb p rrb  and     theorem   k   if af j 196  if k   k  o lrb p rrb if k  k  o lrb p rrb   1  he rrb lrb 3 rrb lrb 4 rrb 2p lrb 1  p rrb   2ncp  lrb 5 rrb  o lrb logn rrb n  then for all f  o  all sufficiently large n  all je lcb l    afn rcb  and all x e dh lrb x lrb j rrb  d rrb  p lcb y    j y lrb j rrb rcb  f see lsb 2 rsb  interpretation  if the exponential growth rate of the number of stored words afn is asymptotically less than c p    lrb 0 rrb  then for every sufficiently large address length n there is some realization of the nx 2n  preprocessor matrix z such that the associative memory can correct fraction 0 errors for most sequences of afn lrb address  datum rrb pairs  thus cp  ic lrb 0 rrb is a lover bound on the exponential growth rate of the capacity of the kanerva associative memory with access radius np and number of memory locations 2nic  figure 3 shows cp  ic lrb o rrb as a function of the radius of attraction 0  for k   k  o lrb p rrb and p o  l  0 2  0 3  0 4 and 0 45  for  any fixed access radius p  cp  ico lrb p rrb lrb 0 rrb decreases as 0 increases  this reflects the fact that fewer lrb address  datum rrb pairs can be stored if a greater fraction of errors must be corrected  as p increases  cp    o lrb p rrb lrb o rrb begins at a lower point but falls off less steeply  in a moment we shall see that p can be adjusted to provide the optimal performance for a given o not shovil in figure 3 is the behavior of cp    lrb 0 rrb as a function of k   however  the behavior is simple  for k   k  o lrb p rrb  cp    lrb o rrb remains unchanged  while for k k  o lrb p rrb  cp    lrb o rrb is simply shifted dovil by the difference ko lrb p rrb  k   this establishes the conditions under which the kanerva associative memory is robust against random component failures  although increasing the number of memory locations beyond 2rl11  o lrb p rrb does not increase the capacity  it does increase robustness  random proof  189 0 8 0 6   i 2        1 1il 2 iil s 1il 3 figure 3  graphs of cp  lco lrb p rrb lrb o rrb as defined by lrb 3 rrb  the upper envelope is 1  h2 lrb 0 rrb  component failures will not affect the capacity until so many components have failed that the number of surviving memory locations is less than 2nlco lrb p rrb  perhaps the most important curve exhibited in figure 3 is the sphere packing upper bound 1  h2 lrb 0 rrb  which is achieved for a particular j p by b    196  2p lrb 1  p rrb  equivalently  the upper bound is achieved for a particular 0 by p equal to poco rrb  t  jt  io lrb l   o rrb  lrb 5 rrb thus lrb 4 rrb and lrb 5 rrb specify the optimal values of the parameters k and p respectively  these functions are shown in figure 4  with these optimal values  lrb 3 rrb simplifies to the sphere packing bound  it can also be seen that for 0  0 in lrb 3 rrb  the exponential growth rate of the capacity is asymptotically equal to k which is the exponential growth rate of the number of memory locations  k n  that is  mn  2n1c  o lrb logn rrb  k n  20 lrb logn rrb  kanerva lsb 3 rsb and keeler lsb 5 rsb have argued that the capacity at 8  0 is proportional to the number of memory locations  i e   mn  k n  lrb 3  for some constant lrb 3  thus our results are consistent with those of kanerva and keeler  provided the  polynomial  20 lrb logn rrb can be proved to be a constant  however  the usual statement of their result  m  k  lrb 3  that the capacity is simply proportional to the number of memory locations  is false  since in light of the universal 190 lils o rij s figure 4  graphs of ko lrb p rrb and co lrb p rrb  the inverse of po lrb  5 rrb  as defined by lrb 4 rrb and lrb 5 rrb  upper bound  it is impossible for the capacity to grow without bound  with no dependence on the dimension n  in our formulation  this difficulty does not arise because we have explicitly related the number of memory locations to the input dimension  kn  2n   in fact  our formulation provides explicit  coherent relationships between all of the following variables  the capacity    the number of memory locations k  the input and output dimensions n and tn  the radius of attraction c  and the access radius p  we are therefore able to generalize the results of lsb 3 5 rsb to the case c  0  and provide explicit expressions for the asymptotically optimal values of p and k as well  conclusion we described a fairly general model of associative memory and selected a useful definition of its capacity  a universal upper bound on the growth of the capacity of such an associative memory was shown by a sphere packing argument to be exponential with rate 1  h 2 lrb c rrb  where h2 lrb c rrb is the binary entropy function and 8 is the radius of attraction  we reviewed the operation of the kanerva associative memory  and stated a lower bound on the exponential growth rate of its capacity  this lower bound meets the universal upper bound for optimal values of the memory parameters p and k  we provided explicit formulas for these optimal values  previous results for  5  0 stating that the capacity of the kanerva associative memory is proportional to the number of memory locations can not be strictly true  our formulation corrects the problem and generalizes those results to the case c  o 191 references 1  r j mceliece  e c posner  e r rodemich  and s s venkatesh   the capacity of the hopfield associative memory   ieee transactions on information theory  submi tt ed  2  p a chou   the capacity of the kanerva associative memory   ieee transactions on information theory  submitted  3  p kanerva   self propagating search  a unified theory of memory   tech  rep csli 84 7  stanford center for the study of language and information  stanford  ca  march 1984  4  p kanerva   parallel structures in human and computer memory   in neural networks for computing  lrb j  s denker  ed  rrb  nev york  american institute of physics  1986  5  j d keeler   comparison betveen sparsely distributed memory and hopfield type neural netvork models   tech  rep riacs tr 86  31  nasa research institute for advanced computer science  mountain viev  ca  dec 1986 
3 en 52 supervised learning of probability distributions by neural networks eric b baum jet propulsion laboratory  pasadena ca 91109 frank wilczek t department of physics  harvard university  cambridge ma 02138 abstract  we propose that the back propagation algorithm for supervised learning can be generalized  put on a satisfactory conceptual footing  and very likely made more efficient by defining the values of the output and input neurons as probabilities and varying the synaptic weights in the gradient direction of the log likelihood  rather than the  error   in the past thirty years many researchers have studied the question of supervised learning in  neural   like networks  recently a learning algorithm called  back propagation h  4 or the  general  ized delta rule  has been applied to numerous problems including the mapping of text to phonemes 5  the diagnosis of illnesses 6 and the classification of sonar targets 7  in these applications  it would often be natural to consider imperfect  or probabilistic information  we believe that by considering supervised learning from this slightly larger perspective  one can not only place back propagat permanent address  institute for theoretical physics  univer  sity of california  santa barbara ca 93106  american institute of physics 1988 53 tion on a more rigorous and general basis  relating it to other well studied pattern recognition algorithms  but very likely improve its performance as well  the problem of supervised learning is to model some mapping between input vectors and output vectors presented to us by some real world phenomena  to be specific  coqsider the question of medical diagnosis  the input vector corresponds to the symptoms of the patient  the i th component is defined to be 1 if symptom i is present and 0 if symptom i is absent  the output vector corresponds to the illnesses  so that its j th component is 1 if the j th illness is present and 0 otherwise  given a data base consisting of a number of diagnosed cases  the goal is to construct lrb learn rrb a mapping which accounts for these examples and can be applied to diagnose new patients in a reliable way  one could hope  for instance  that such a learning algorithm might yield an expert system to simulate the performance of doctors  little expert advice would be required for its design  which is advantageous both because experts  time is valuable and because experts often have extraodinary difficulty in describing how they make decisions  a feedforward neural network implements such a mapping between input vectors and output vectors  such a network has a set of input nodes  one or several layers of intermediate nodes  and a layer of output nodes  the nodes are connected in a forward directed manner  so that the output of a node may be connected to the inputs of nodes in subsequent layers  but closed loops do not occur  see figure 1  the output of each node is assumed to be a bounded semilinear function of its inputs  that is  if the output of the j th node and wij vj denotes denotes the weight associated with the connection of the output of the j th node to the input of 54 the i th  then the i th neuron takes value vi  g lrb l  i wi  jv  j rrb  where g is a bounded  differentiable function called the activation function  g lrb x rrb  1  lrb 1  e  x rrb  called the logistic function  is frequently used  given a fixed set of weights lcb wi  j rcb  we set the input node values to equal some input vector  compute the value of the nodes layer by layer until we compute the output nodes  and so generate an output vector  figure 1  a 5 layer network  note bottleneck at layer 3  55 such networks have been studied because of analogies to neurobiology  because it may be easy to fabricate them in hardware  and because learning algorithms such as the perceptron learning algorithm 8  widrow  hoff9  and backpropagation have been able to choose weights wi   that solve interesting problems  given a set of input vectors values tj  sr  together with associated target back propagation attempts to adjust the weights so as to minimize the error e in achieving these target values  defined as e  e ejl  e lrb tj  oj rrb 2 jl where oj input  lrb 1 rrb jl  i is the output of the j th node when sjl is presented as back propagation starts with randomly chosen wi   and then varies in the gradient direction of e until a local minimum is obtained  although only a locally optimal set of weights is obtained  in a number of experiments the neural net so generated has performed surprisingly well not only on the training set but on subsequent data  4  6 this performance is probably the main reason for widespread interest in backpropagation  it seems to us natural  in the context of the medical diagnosis pro blem  the other real world problems to which backpropagation has been applied  and indeed in any mapping problem where one desires to generalize from a limited and noisy set of examples  to interpret the output vector in probabilistic terms  such an interpretation is standard in the literature on pattern classification  1o indeed  the examples might even be probabilistic themselves  that is to say it might not be certain whether symptom i was present in case  l or not  let sr represent the probability symptom i is present in case  l  and let tj represent the probability disease j ocurred in case 56 fl  consider for the moment the case where the tj are 1 or 0  a so that the cases are in fact fully diagnosed  let ii lrb s  0 rrb be our prediction of the probability of disease i given input vector 5  where lcb  is some set of parameters determined by our learning algorithm  in the neural network case  the lcb  are the connection weights and ii lrb sl   lcb wi i rcb rrb  oj  now lacking a priori knowledge of good 0  the best one can do is to choose the parameters lcb  to maximize the likelihood that the given set of examples should have occurred  10 the formula for this likelihood  p  is immediate  or the extension of equation lrb 2 rrb  and thus equation lrb 3 rrb to the case where the f are probabilities  taking values in lsb 0 1 rsb  is straight  57 forward  1 and yields log lrb p rrb   lsb tjlog lrb jj lrb s   0 rrb rrb  lrb 1  tj rrb log lrb 1  ij lrb w  0 rrb rrb rsb lrb 4 rrb p 3 expressions of this sort often arise in physics and information theory and are generally interpreted as an entropy  11 we may now vary the lcb o rcb in the gradient direction of the entropy  the back propagation algorithm generalizes immediately from minimizing  error  or  energy  to maximizing entropy or log likelihood  or indeed any other function of the outputs and the inputs 12  of course it remains true that the gradient can be computed by back propagation with essentially the same number of computations as are required to compute the output of the network  a backpropagation algorithm based on log likelihood is not only more intuitively appealing than one based on an ad hoc definition of error  but will make quite different and more accurate predictions as well  consider e g training the net on an example which it already understands fairly well   j lrb 80 rrb  l now  from eqn lrb l rrb be b j say tj  2   so using  0  and  error  as a  1 we may see this by constructing an equivalent larger set of examples with the f taking only values 0 or 1 with the appropriate frequency  thus assume the tj are rational numbers with denomi  nator dj and numerator nj and let p  iip  j dj  what we mean by the set of examples lcb tp   j t  1    m rcb can be represented by con  ij  0 for p lrb j t  1 rrb  v  pj t and 1  vmod lrb dj rrb  lrb dj  nj rrb  and ij  1 sidering a set of n  mp examples lcb ij rcb where for each j t  otherwise  n ow applying equation lrb 3 rrb gives equation lrb 4 rrb  up to an overall normalization  58 criterion the net learns very little from this example  whereas  using eqn lrb 3 rrb  blog lrb p rrb  b   j  1  lrb 1  f rrb  so the net continues to learn and can in fact converge to predict probabilities near 1  indeed because back propagation using the standard  error  measure can not converge to generate outputs of 1 or 0  it has been customary in the literature 4 to round the target values so that a target of 1 would be presented in the learning algorithm as some ad hoc number such as 8  whereas a target of 0 would be presented as 2  in the context of our general discussion it is natural to ask whether using a feedforward network and varying the weights is in fact the most effective alternative  anderson and abrahams 13 have discussed this issue from a bayesian viewpoint  from this point of view  fitting output to input using normal distributions and varying the means and covariance matrix may seem to be more logical  feedforward networks do however have several advantages for complex problems  experience with neural networks has shown the importance of including hidden units wherein the network can form an internal representation of the world  if one simply uses normal distributions  any hidden variables included will simply integrate out in calculating an output  it will thus be necessary to include at least third order correlations to implement useful hidden variables  unfortunately  the number of possible third order correlations is very large  so that there may be practical obstacles to such an approach  indeed it is well known folklore in curve fitting and pattern classification that the number of parameters must be small compared to the size of the data set if any generalization to future cases is expected  10 in feedforward nets the question takes a different form  there can be bottlenecks to information flow  specifically  if the net is 59 constructed with an intermediate layer which is not bypassed by any connections lrb i e there are no connections from layers preceding to layers subsequent rrb  and if furthermore the activation functions are chosen so that the values of each of the intermediate nodes tend towards either 1 or 0  2  then this layer serves as a bottleneck to information flow  no matter how many input nodes  output nodes  or free parameters there are in the net  the output will be constrained to take on no more than 21 different patterns  where i is the number of nodes in the bottleneck layer  thus if i is small  some sort of  generalization  must occur even if the number of weights is large  one plausible reason for the success of back propagation in adequately solving tasks  in spite of the fact that it finds only local minima  is its ability to vary a large number of parameters  this freedom may allow back propagation to escape from many putative traps and to find an acceptable solution  a good expert system  say for medical diagnosis  should not only give a diagnosis based on the available information  but should be able to suggest  in questionable cases  which lab tests might be performed to clarify matters  actually back propagation inherently has such a capability  back propagation involves calculation of 81og lrb p rrb  8wij  this information allows one to compute immediately 81og lrb p rrb  8s j  those input nodes for which this partial derivative is large correspond to important experiments  in conclusion  we propose that back propagation can be generalized  put on a satisfactory conceptual footing  and very likely made more efficient  by defining the values of the output and in  2 alternatively when necessary this can be enforced by adding an energy term to the log likelihood to constrain the parameter variation so that the neuronal values are near either 1 or o 60 put neurons as probabilities  and replacing the  error  by the loglikelihood  acknowledgement  e b baum was supported in part by darpa through arrangement with nasa and by nsf grant dmb 840649  802  f wilczek was supported in part by nsf grant phy82 17853 references lrb 1 rrb werbos  p   beyond regression  new tools for prediction and analysis in the behavioral sciences   harvard university dissertation lrb 1974 rrb lrb 2 rrb parker d b   learning logic   mit tech report tr 47  center for computationl research in economics and management science  mit  1985 lrb 3 rrb le cun  y  proceedings of cognitiva 85 p599 604  paris lrb 1985 rrb lrb 4 rrb rumelhart  d e  hinton  g e  williams  g e   learning internal representations by error propagation   in  parallel distributed processing   vol 1  eds  rumelhart  d e  mcclelland  j l  mit press  cambridge ma  lrb 1986 rrb lrb 5 rrb sejnowski  t j  rosenberg  c r  complex systems  v 1  pp 145 168 lrb 1987 rrb lrb 6 rrb lecun  y  address at 1987 snowbird conference on neural networks lrb 7 rrb gorman  p  sejnowski  t j   learned classification of sonar targets using a massively parallel network   in  workshop on neural network devices and applications   jpld 4406  lrb 1987 rrb pp224 237 lrb 8 rrb rosenblatt  f   principles of neurodynamics  perceptrons and 61 the theory of brain mechanisms   spartan books  washington dc lrb 1962 rrb lrb 9 rrb widrow  b  hoff  m e  1960 ire wescon cony  record  part 4  96 104 lrb 1960 rrb lrb 10 rrb duda  r 0  hart  p e   pattern classification and scene analysis   john wiley and sons  n y  lrb 1973 rrb lrb 11 rrb guiasu  s   information theory with applications   mcgraw hill  ny  lrb 1977 rrb lrb 12 rrb baum  e b   generalizing back propagation to computation   in  neural networks for computing   alp conf  proc  151  snowbird ut lrb 1986 rrb pp47 53 lrb 13 rrb anderson  c h  abrahams  e   the bayes connection   proceedings of the ieee international conference on neural n etwor ks  san diego  lrb 1987 rrb
4 en 612 constrained differential optimization john c platt alan h barr california institute of technology  pasadena  ca 91125 abstract many optimization models of neural networks need constraints to restrict the space of outputs to a subspace which satisfies external criteria  optimizations using energy methods yield  forces  which act upon the state of the neural network  the penalty method  in which quadratic energy constraints are added to an existing optimization energy  has become popular recently  but is not guaranteed to satisfy the constraint conditions when there are other forces on the neural model or when there are multiple constraints  in this paper  we present the basic differential multiplier method lrb bdmm rrb  which satisfies constraints exactly  we create forces which gradually apply the constraints over time  using  neurons  that estimate lagrange multipliers  the basic differential multiplier method is a differential version of the method of multipliers from numerical analysis  we prove that the differential equations locally converge to a constrained minimum  examples of applications of the differential method of multipliers include enforcing permutation codewords in the analog decoding problem and enforcing valid tours in the traveling salesman problem  1  introduction optimization is ubiquitous in the field of neural networks  many learning algorithms  such as back propagation 18 optimize by minimizing the difference between expected solutions and observed solutions  other neural algorithms use differential equations which minimize an energy to solve a specified computational problem  such as associative memory  d differential solution of the traveling salesman problem  s  lo analog decoding  ls and linear programming  1d furthennore  lyapunov methods show that various models of neural behavior find minima of particular functions  4  d solutions to a constrained optimization problem are restricted to a subset of the solutions of the corresponding unconstrained optimization problem  for example  a mutual inhibition circuits requires one neuron to be  on  and the rest to be  off   another example is the traveling salesman problem  ls where a salesman tries to minimize his travel distance  subject to the constraint that he must visit every city exactly once  a third example is the curve fitting problem  where elastic splines are as smooth as possible  while still going through data points s finally  when digital decisions are being made on analog data  the answer is constrained to be bits  either 0 or 1  14 a constrained optimization problem can be stated as minimize  lrb  rrb  subject to g lrb  rrb  0  lrb 1 rrb where  is the state of the neural network  a position vector in a high dimensional space  f lrb  rrb is a scalar energy  which can be imagined as the height of a landscape as a function of position   g lrb  rrb  0 is a scalar equation describing a subspace of the state space  during constrained optimization  the state should be attracted to the subspace g lrb  rrb  0  then slide along the subspace until it reaches the locally smallest value of f lrb  rrb on g lrb  rrb  o  in section 2 of the paper  we describe classical methods of constrained optimization  such as the penalty method and lagrange multipliers  section 3 introduces the basic differential multiplier method lrb bdmm rrb for constrained optimization  which calcuiates a good local minimum  if the constrained optimization problem is convex  then the local minimum is the global minimum  in general  finding the global minimum of non convex problems is fairly difficult  in section 4  we show a lyapunov function for the bdmm by drawing on an analogy from physics   american institute of physics 1988 613 in section 5  augmented lagrangians  an idea from optimization theory  enhances the convergence properties of the bdmm  in section 6  we apply the differential algorithm to two neural problems  and discuss the insensitivity of bdmm to choice of parameters  parameter sensitivity is a persistent problem in neural networks  2  classical methods of constrained optimization this section discusses two methods of constrained optimization  the penalty method and lagrange multipliers  the penalty method has been previously used in differential optimization  the basic differential multiplier method developed in this paper applies lagrange multipliers to differential optimization  2  l  the penalty method the penalty method is analogous to adding a rubber band which attracts the neural state to the subspace g lrb  rrb  o  the penalty method adds a quadratic energy term which penalizes violations of constraints  8 thus  the constrained minimization problem lrb 1 rrb is converted to the following unconstrained minimization problem  lrb 2 rrb figure 1  the penalty method makes a trough in state space the penalty method can be extended to fulfill multiple constraints by using more than one rubber band  namely  the constrained optimization problem minimize f lrb   rrb  8ubject to go lrb  rrb  oj a  1 2    n  lrb 3 rrb is converted into unconstrained optimization problem n minimize l pena1ty lrb  rrb  f lrb  rrb  l co lrb go lrb  rrb rrb 2  lrb 4 rrb 0   1 the penalty method has several convenient features  first  it is easy to use  second  it is globally convergent to the correct answer as co  00  8 third  it allows compromises between constraints  for example  in the case of a spline curve fitting input data  there can be a compromise between fitting the data and making a smooth spline  614 however  the penalty method has a number of disadvantages  first  for finite constraint strengths it does n t fulfill the constraints exactly  using multiple rubber band constraints is like building a machine out of rubber bands  the machine would not hold together perfectly  second  as more constraints are added  the constraint strengths get harder to set  especially when the size of the network lrb the dimensionality of gets large  in addition  there is a dilemma to the setting of the constraint strengths  if the strengths are small  then the system finds a deep local minimum  but does not fulfill all the constraints  if the strengths are large  then the system quickly fulfills the constraints  but gets stuck in a poor local minimum  col   u 2 2  lagrange multipliers lagrange multiplier methods also convert constrained optimization problems into unconstrained extremization problems  namely  a solution to the equation lrb 1 rrb is also a critical point of the energy lrb 5 rrb rrb  is called the lagrange multiplier for the constraint g lrb  rrb  0 8 a direct consequence of equation lrb 5 rrb is that the gradient of f is collinear to the gradient of 9 at the constrained extrema lrb see figure 2 rrb  the constant of proportionality between  i1 f and  i1 9 is  rrb    i1  lagrange  0   i1 f  rrb   i1 g lrb 6 rrb we use the collinearity of  i1 f and  i1 9 in the design of the bdmm  figure 2  at the constrained minimum   i1 f   rrb   i1 9 a simple example shows that lagrange multipliers provide the extra degrees of freedom necessary to solve constrained optimization problems  consider the problem of finding a point lrb x  y rrb on the line x  y  1 that is closest to the origin  using lagrange multipliers   lagrange  x 2  y2  rrb  lrb x  y  1 rrb lrb 7 rrb now  take the derivative with respect to all variables  x  y  and a aelagrange  2x  a  0 a  lagrange  2y  a  0 ax ay a  lagrange  a rrb  x  y  1  0 lrb 8 rrb 615 with the extra variable a  there are now three equations in three unknowns  in addition  the last equation is precisely the constraint equation  3  the basic differential multiplier method for constrained optimization this section presents a new  neural  algorithm for constrained optimization  consisting of differential equations which estimate lagrange multipliers  the neural algorithm is a variation of the method of multipliers  first presented by hestenes 9 and powell 16  3 1  gradient descent does not work with lagrange multipliers the simplest differential optimization algorithm is gradient descent  where the state variables of the network slide downhill  opposite the gradient  applying gradient descent to the energy in equation lrb 5 rrb yields x   a lagrange  ax     a lagrange  aa j    al  a ag ax   ax     g   lrb 9 rrb lrb rrb note that there is a auxiliary differential equation for a  which is an additional  neuron  necessary to apply the constraint g lrb  rrb  o also  recall that when the system is at a constrained extremum  vi   avg  hence  x  o energies involving lagrange multipliers  however  have critical points which tend to be saddle points  consider the energy in equation lrb 5 rrb  if  is frozen  the energy can be decreased by sending a to 00 or 00  gradient descent does not work with lagrange multipliers  because a critical point of the energy in equation lrb 5 rrb need not be an attractor for lrb 9 rrb  a stationary point must be a local minimum in order for gradient descent to converge  3 2  the new algorithm  the basic differential multiplier method we present an alternative to differential gradient descent that estimates the lagrange multipliers  so that the constrained minima are attractors of the differential equations  instead of  repulsors   the differential equations that solve lrb 1 rrb is  al  ax  i   g lrb  rrb  ag ax   x      a  lrb 10 rrb equation lrb 10 rrb is similar to equation lrb 9 rrb  as in equation lrb 9 rrb  constrained extrema of the energy lrb 5 rrb are stationary points of equation lrb 10 rrb  notice  however  the sign inversion in the equation for i  as compared to equation lrb 9 rrb  the equation lrb 10 rrb is performing gradient ascent on a  the sign flip makes the bdmm stable  as shown in section 4  equation lrb 10 rrb corresponds to a neural network with anti symmetric connections between the a neuron and all of the  neurons  3 3  extensions to the algorithm one extension to equation lrb 10 rrb is an algorithm for constrained minimization with multiple constraints  adding an extra neuron for every equality constraint and summing all of the constraint forces creates the energy lrb 11 rrb  multiple   lrb  rrb  ao  ga lrb  rrb  i  0  which yields differential equations x    al    a agcr    ax    0  0  ax  rrb  lrb 12 rrb 616 another extension is constrained minimization with inequality constraints  as in traditional optimization theory 8 one uses extra slack variables to convert inequality constraints into equality constraints  namely  a constraint of the form h lrb  rrb  0 can be expressed as lrb 13 rrb since z2 must always be positive  then h lrb  rrb is constrained to be positive  the slack variable z is treated like a component of  in equation lrb 10 rrb  an inequality constraint requires two extra neurons  one for the slack variable  and one for the lagrange multiplier   alternatively  the inequality constraint can be represented as an equality constraint for example  if h lrb  rrb  0  then the optimization can be constrained with g lrb  rrb  h lrb   rrb  when h lrb  rrb  0  and g lrb   rrb  0 otherwise  4  why the algorithm works the system of differential equations lrb 10 rrb lrb the bdmm rrb gradually fulfills the constraints  notice that the function g lrb  rrb can be replaced by kg lrb  rrb  without changing the location of the constrained minimum  as k is increased  the state begins to undergo damped oscillation about the constraint subspace g lrb  rrb  o  as k is increased further  the frequency of the oscillations increase  and the time to convergence increases  constraint subspace     initial state     path of algorithm    figure 3  the state is attracted to the constraint subspace the damped oscillations of equation lrb 10 rrb can be explained by combining both of the differential equations into one second order differential equation  lrb 14 rrb equation lrb 14 rrb is the equation for a damped mass system  with an inertia term xi  a damping matrix lrb 15 rrb and an internal force  gog o  i  which is the derivative of the internal energy lrb 16 rrb 617 if the system is damped and the state remains bounded  the state falls into a constrained minima  as in physics  we can construct a total energy of the system  which is the sum of the kinetic and potential energies  e  t  u  l  i lrb xd 2  i lrb g lrb  rrb rrb 2  lrb 17 rrb if the total energy is decreasing with time and the state remains bounded  then the system will dissipate any extra energy  and will settle down into the state where lrb 18 rrb which is a constrained extremum of the original problem in equation lrb 1 rrb  the time derivative of the total energy in equation lrb 17 rrb is   lrb 19 rrb lx  a  jxj    i if damping matrix aii is positive definite  the system converges to fulfill the constraints  bdmm always converges for a special case of constrained optimization  quadratic programming  a quadratic programming problem has a quadratic function f lrb  rrb and a piecewise linear continuous function g lrb  rrb such that lrb 20 rrb under these circumstances  the damping matrix aii is positive definite for all system converges to the constraints   and a  so that the 4 1  multiple constraints for the case of multiple constraints  the total energy for equation lrb 12 rrb is e  t  u  l i i lrb xi rrb 2  l igo lrb  rrb 2  lrb 21 rrb 0 and the time derivative is lrb 22 rrb again  bdmm solves a quadratic programming problem  if a solution exists  however  it is possible to pose a problem that has contradictory constraints  for example  gdx rrb  x  0  g2 lrb x rrb  x  i  0 lrb 23 rrb in the case of conflicting constraints  the bdmm compromises  trying to make each constraint go as small as possible  however  the lagrange multipliers ao goes to  oo as the constraints oppose each other  it is possible  however  to arbitrarily limit the ao at some large absolute value  618 lasalle s invariance theorem 12 is used to prove that the bdmm eventually fulfills the constraints  let g be an open subset of rn  let f be a subset of g   the closure of g  where the system of differential equations lrb 12 rrb is at an equilibrium  lrb 24 rrb if the damping matrix a2 f    a a2 ga     ax  ax   a ax  ax  lrb 25 rrb is positive definite in g  if xa lcb t rrb and aa lrb t rrb are bounded  and remain in g for all time  and  f f is non empty  then f is the largest invariant set in g   hence  by lasalle s invariance theorem  the system lrb t rrb  aa lrb t rrb approaches fast   00  x  5  the modified differential method of multipliers this section presents the modified differemiai multiplier method lrb mdmm rrb  which is a modification of the bdmm with more robust convergence properties  for a given constrained optimization problem  it is frequently necessary to alter the bdmm to have a region of positive damping surrounding the constrained minima  the non differential method of multipliers from numerical analysis also has this difficulty  2 numerical analysis combines the multiplier method with the penalty method to yield a modified multiplier method that is locally convergent around constrained minima  2 the bdmm is completely compatible with the penalty method  if one adds a penalty force to equation lrb 10 rrb corresponding to an quadratic energy epenalty   lrb g lrb  rrb rrb 2  lrb 26 rrb then the set of differential equations for mdmm is  af ag x    ax   a ax   j  g lrb  rrb  ag ax  cg   lrb 27 rrb the extra force from the penalty does not change the position of the stationary points of the differential equations  because the penalty force is 0 when g lrb  rrb  o  the damping matrix is modified by the penalty force to be lrb 28 rrb there is a theorem 1 that states that there exists a c   0 such that if c  c   the damping matrix in equation lrb 28 rrb is positive definite at constrained minima  using continuity  the damping matrix is positive definite in a region r surrounding each constrained minimum  if the system starts in the region r and remains bounded and in r  then the convergence theorem at the end of section 4 is applicable  and mdmm will converge to a constrained minimum  the minimum necessary penalty strength c for the mdmm is usually much less than the strength needed by the penalty method alone  2 6  examples this section contains two examples which illustrate the use of the bdmm and the mdmm  first  the bdmm is used to find a good solution to the planar traveling salesman problem  second  the mdmm is used to enforcing mutual inhibition and digital results in the task of analog decoding  6 1  planar traveling salesman the traveling salesman problem lrb fsp rrb is  given a set of cities lying in the plane  find the shortest closed path that goes through every city exactly once  finding the shortest path is np complete  619 finding a nearly optimal path  however  is much easier than finding a globally optimal path  there exist many heuristic algorithms for approximately solving the traveling salesman problem  5 10 11 13 the solution presented in this section is moderately effective and illustrates the independence of bdmm to changes in parameters  following durbin and willshaw 5 we use an elastic snake to solve the tsp  a snake is a discretized curve which lies on the plane  the elements of the snake are points on the plane  lrb xi  yd  a snake is a locally connected neural network  whose neural outputs are positions on the plane  the snake minimizes its length 2  rrb xi 1  x  rrb 2  lrb yi  l  yi rrb 2  lrb 29 rrb i subject to the constraint that the snake must lie on the cities  k lrb x   xc rrb  0  k lrb y   yc rrb  0  lrb 30 rrb where lrb x   y  rrb are city coordinates  lrb xc  yc rrb is the closest snake point to the city  and k is the constraint strength  the minimization in equation lrb 29 rrb is quadratic and the constraints in equation lrb 30 rrb are piecewise linear  corresponding to a co continuous potential energy in equation lrb 21 rrb  thus  the damping is positive definite  and the system converges to a state where the constraints are fulfilled  in practice  the snake starts out as a circle  groups of cities grab onto the snake  deforming it as the snake gets close to groups of cities  it grabs onto a specific ordering of cities that locally minimize its length lrb see figure 4 rrb  the system of differential equations that solve equations lrb 29 rrb and lrb 30 rrb are piecewise linear  the differential equations for xi and yi are solved with implicit euler s method  using tridiagonal lv decomposition to solve the linear system  17 the points of the snake are sorted into bins that divide the plane  so that the computation of finding the nearest point is simplified  figure 4  the snake eventually attaches to the cities the constrained minimization in equations lrb 29 rrb and lrb 30 rrb is a reasonable method for approximately solving the tsp  for 120 cities distributed in the unti square  and 600 snake points  a numerical step size of 100 time units  and a constraint strength of 5 x 10  3  the tour lengths are 6   2  longer than that yielded by simulated annealing 11  empirically  for 30 to 240 cities  the time needed to compute the final city ordering scales as n1 6  as compared to the kernighan lin method13  which scales roughly as n 2 2  the constraint strength is usable for both a 30 city problem and a 240 city problem  although changing the constraint strength affects the performance  the snake attaches to the cities for any nonzero constraint strength  parameter adjustment does not seem to be an issue as the number of cities increases  unlike the penalty method  620 6 2  analog decoding analog decoding uses analog signals from a noisy channel to reconstruct codewords  analog decoding has been performed neurally 15 with a code space of permutation matrices  out of the possible space of binary matrices  to perform the decoding of permutation matrices  the nearest permutation matrix to the signal matrix must be found  in other words  find the nearest matrix to the signal matrix  subject to the constraint that the matrix has on off binary elements  and has exactly one  on  per row and one  on  per column  if the signal matrix is ii  and the result is vi   then minimize   v  l  j   1     lrb 31 rrb i   subject to constraints vi   lrb l  vi rrb  oj lvi  1  lrb 32 rrb o  in this example  the first constraint in equation lrb 32 rrb forces crisp digital decisions  the second and third constraints are mutual inhibition along the rows and columns of the matrix  the optimization in equation lrb 31 rrb is not quadratic  it is linear  in addition  the first constraint in equation lrb 32 rrb is non linear  using the bdmm results in undamped oscillations  in order to converge onto a constrained minimum  the mdmm must be used  for both a 5 x 5 and a 20 x 20 system  a c  0 2 is adequate for damping the oscillations  the choice of c seems to be reasonably insensitive to the size of the system  and a wide range of c  from 0 02 to 2 0  damps the oscillations                                                                             e           e                              e                       r                                              figure 5  the decoder finds the nearest permutation matrix in a test of the mdmm  a signal matrix which is a permutation matrix plus some noise  with a signal to noise ratio of 4 is supplied to the network  in figure 5  the system has turned on the correct neurons but also many incorrect neurons  the constraints start to be applied  and eventually the system reaches a permutation matrix  the differential equations do not need to be reset  if a new signal matrix is applied to the network  the neural state will move towards the new solution  7  conclusions in the field of neural networks  there are differential optimization algorithms which find local solutions to non convex problems  the basic differential multiplier method is a modification of a standard constrained optimization algorithm  which improves the capability of neural networks to perform constrained optimization  the bdmm and the mdmm offer many advantages over the penalty method  first  the differential equations lrb 10 rrb are much less stiff than those of the penalty method  very large quadratic terms are not needed by the mdmm in order to strongly enforce the constraints  the energy terrain for the 621 penalty method looks like steep canyons  with gentle floors  finding minima of these types of energy surfaces is numerically difficult in addition  the steepness of the penalty tenns is usually sensitive to the dimensionality of the space  the differential multiplier methods are promising techniques for alleviating stiffness  the differential multiplier methods separate the speed of fulfilling the constraints from the accuracy of fulfilling the constraints  in the penalty method  as the strengths of a constraint goes to 00  the constraint is fulfilled  but the energy has many undesirable local minima  the differential multiplier methods allow one to choose how quickly to fulfill the constraints  the bdmm fulfills constraints exactly and is compatible with the penalty method  addition of penalty tenns in the mdmm does not change the stationary points of the algorithm  and sometimes helps to damp oscillations and improve convergence  since the bdmm and the mdmm are in the form of first order differential equations  they can be directly implemented in hardware  performing constrained optimization at the raw speed of analog vlsi seems like a promising technique for solving difficult perception problems  14 there exist lyapunov functions for the bdmm and the mdmm  the bdmm converges globally for quadratic programming  the mdmm is provably convergent in a local region around the constrained minima other optimization algorithms  such as newton s method 17 have similar local convergence properties  the global convergence properties of the bdmm and the mdmm are currently under investigation  in summary  the differential method of multipliers is a useful way of enforcing constraints on neural networks for enforcing syntax of solutions  encouraging desirable properties of solutions  and making crisp decisions  acknowledgments this paper was supported by an at t bell laboratories fellowship lrb jcp rrb  references 1  k j arrow  l hurwicz  h uzawa  studies in linear and nonlinear programming  lrb stanford university press  stanford  ca  1958 rrb  2  d p bertsekas  automatica  12  133 145  lrb 1976 rrb  3  c de boor  a practical guide to splines  lrb springer verlag  ny  1978 rrb  4  m a cohen  s grossberg  ieee trans  systems  man  and cybernetics  815 826  lrb 1983 rrb  5  r durbin  d willshaw  nature  326  689 691  lrb 1987 rrb  6  j c eccles  the physiology of nerve cells  lrb johns hopkins press  baltimore  1957 rrb  7  m r hestenes  j opt  theory appl  4  303 320  lrb 1969 rrb  8  m r hestenes  optimization theory  lrb wiley  sons  ny  1975 rrb  9  j j hopfield  pnas  81  3088  lrb 1984 rrb  10  j j hopfield  d w tank  biological cybernetics  52  141  lrb 1985 rrb  11  s kirkpatrick  c d gelatt  c m vecchi  science  220  671 680  lrb 1983 rrb  12  j lasalle  the stability of dynamical systems  lrb siam  philadelphia  1976 rrb  13  s lin  b w kernighan  oper  res  21 498 516 lrb 1973 rrb  14  c a mead  analog vlsi and neural systems  lrb addison wesley  reading  ma  tba rrb  15  j c platt  j j hopfield  in alp con   proc 151  neural networksfor computing lrb 1  denker ed  rrb 364 369  lrb american institute of physics  ny  1986 rrb  16  m 1  powell  in optimization  lrb r fletcher  ed  rrb  283 298  lrb academic press  ny  1969 rrb  17  w h press  b p flannery  s a teukolsky  w t vetterling  numerical recipes  lrb cambridge university press  cambridge  1986 rrb  18  d rumelhart  g hinton  r williams  in parallel distributed processing  lrb d rumelhart  ed rrb  1  318 362  lrb mit press  cambridge  ma  1986 rrb  19  d w tank  j j hopfield  ieee trans  cir   sys  cas 33  no 5 533 541 lrb 1986 rrb 
5 en 485 towards an organizing principle for a layered perceptual network ralph linsker ibm thomas j watson research center  yorktown heights  ny 10598 abstract an information theoretic optimization principle is proposed for the development of each processing stage of a multilayered perceptual network  this principle of  maximum information preservation  states that the signal transformation that is to be realized at each stage is one that maximizes the information that the output signal values lrb from that stage rrb convey about the input signals values lrb to that stage rrb  subject to certain constraints and in the presence of processing noise  the quantity being maximized is a shannon information rate  i provide motivation for this principle and  for some simple model cases  derive some of its consequences  discuss an algorithmic implementation  and show how the principle may lead to biologically relevant neural architectural features such as topographic maps  map distortions  orientation selectivity  and extraction of spatial and temporal signal correlations  a possible connection between this information theoretic principle and a principle of minimum entropy production in nonequilibrium thermodynamics is suggested  introduction this paper describes some properties of a proposed information theoretic organizing principle for the development of a layered perceptual network  the purpose of this paper is to provide an intuitive and qualitative understanding of how the principle leads to specific feature analyzing properties and signal transformations in some simple model cases  more detailed analysis is required in order to apply the principle to cases involving more realistic patterns of signaling activity as well as specific constraints on network connectivity  this section gives a brief summary of the results that motivated the formulation of the organizing principle  which i call the principle of  maximum information preservation   in later sections the principle is stated and its consequences studied  in previous work l i analyzed the development of a layered network of model cells with feedforward connections whose strengths change in accordance with a hebb type synaptic modification rule  i found that this development process can produce cells that are selectively responsive to certain input features  and that these feature analyzing properties become progressively more sophisticated as one proceeds to deeper cell layers  these properties include the analysis of contrast and of edge orientation  and are qualitatively similar to properties observed in the first several layers of the mammalian visual pathway 2 why does this happen  does a hebb type algorithm lrb which adjusts synaptic strengths depending upon correlations among signaling activities 3 rrb cause a developing perceptual network to optimize some property that is deeply connected with the mature network s functioning as an information processing system   american institute ofphvsics 1988 486 further analysis 4  s has shown that a suitable hebb type rule causes a linear response cell in a layered feedforward network lrb without lateral connections rrb to develop so that the statistical variance of its output activity lrb in response to an ensemble of inputs from the previous layer rrb is maximized  subject to certain constraints  the mature cell thus performs an operation similar to principal component analysis lrb pca rrb  an approach used in statistics to expose regularities lrb e g  clustering rrb present in high dimensional input data  lrb oja 6 had earlier demonstrated a particular form of hebb type rule that produces a model cell that implements pca exactly  rrb furthermore  given a linear device that transforms inputs into an output  and given any particular output value  one can use optimal estimation theory to make a  best estimate  of the input values that gave rise to that output  of all such devices  i have found that an appropriate hebb type rule generates that device for which this  best estimate  comes closest to matching the input values  4  s under certain conditions  such a cell has the property that its output preserves the maximum amount of information about its input values  s maximum information preservation the above results have suggested a possible organizing principle for the development of each layer of a multilayered perceptual network  s the principle can be applied even if the cells of the network respond to their inputs in a nonlinear fashion  and even if lateral as well as feedforward connections are present  lrb feedback from later to earlier layers  however  is absent from this formulation  rrb this principle of  maximum information preservation  states that for a layer of cells l that is connected to and provides input to another layer m  the connections should develop so that the transformation of signals from l to m lrb in the presence of processing noise rrb has the property that the set of output values m conveys the maximum amount of information about the input values l  subject to various constraints on  e g  the range of lateral connections and the processing power of each cell  the statistical properties of the ensemble of inputs l are assumed stationary  and the particular l to m transformation that achieves this maximization depends on those statistical properties  the quantity being maximized is a shannon information rate  7 an equivalent statement of this principle is  the l to m transformation is chosen so as to minimize the amount of information that would be conveyed by the input values l to someone who already knows the output values m  we shall regard the set of input signal values l lrb at a given time rrb as an input  message   the message is processed to give an output message m each message is in general a set of real valued signal activities  because noise is introduced during the processing  a given input message may generate any of a range of different output messages when processed by the same set of connections  the shannon information rate lrb i e  the average information transmitted from l to m per message rrb is7 r  ll lmp lrb l  m rrb log lsb p lrb l  m rrb  p lrb l rrb p lrb m rrb rsb  lrb 1 rrb for a discrete message space  pel rrb lsb resp  p lrb m rrb rsb is the probability of the input lrb resp  output rrb message being l lrb resp  m rrb  and p lrb l  m rrb is the joint probability of the input being l and the output being m lsb for a continuous message space  probabilities are 487 replaced by probability densities  and sums lrb over states rrb by integrals  rsb this rate can be written as lrb 2 rrb where h   ll p lrb l rrb log p lrb l rrb lrb 3 rrb is the average information conveyed by message land lrb 4 rrb is the average information conveyed by message l to someone who already knows m  since ii  is fixed by the properties of the input ensemble  maximizing r means minimizing i lim  as stated above  the information rate r can also be written as lrb 5 rrb where 1m and imi l are defined by interchanging land m in eqns  3 and 4  this form is heuristically useful  since it suggests that one can attempt to make r large by lrb if possible rrb simultaneously making 1m large and imi l small  the term 1m is largest when each message m occurs with equal probability  the term 1   1   is smallest when each l is transformed into a unique m  and more generally is made small by  sharpening  the p lrb m il rrb distribution  so that for each l  p lrb m il rrb is near zero except for a small set of messages m  how can one gain insight into biologically relevant properties of the l  m transformation that may follow from the principle of maximum information preservation lrb which we also call the  infomax  principle rrb  in a network  this l  m transformation may be a function of the values of one or a few variables lrb such as a connection strength rrb for each of the allowed connections between and within layers  and for each cell  the search space is quite large  particularly from the standpoint of gaining an intuitive or qualitative understanding of network behavior  we shall therefore consider a simple model in which the dimensionalities of the land m signal spaces are greatly reduced  yet one for which the infomax analysis exhibits features that may also be important under more general conditions relevant to biological and synthetic network development  the next four sections are organized as follows  lrb i rrb a model is introduced in which the land m messages  and the l to m transformation  have simple forms  the infomax principle is found to be satisfied when some simple geometric conditions lrb on the transformation rrb are met  lrb ii rrb i relate this model to the analysis of signal processing and noise in an interconnection network  the formation of topographic maps is discussed  lrb iii rrb the model is applied to simplified versions of biologically relevant problems  such as the emergence of orientation selectivity  lrb iv rrb i show that the main properties of the infomax principle for this model can be realized by certain local algorithms that have been proposed to generate topographic maps using lateral interactions  488 a simple geometric model in this model  each input message l is described by a point in a low dimensional vector space  and the output message m is one of a number of discrete states  for definiteness  we will take the l space to be two dimensional lrb the extension to higher dimensionality is straightforward rrb  the l  m transformation consists of two steps  lrb i rrb a noise process alters l to a message l lying within a neighborhood of radius v centered on l lrb ii rrb the altered message l is mapped deterministically onto one of the output messages m  a given l  m mapping corresponds to a partitioning of the l space into regions labeled by the output states m lrb we do not exclude a priori the possibility that multiple disjoint regions may be labeled by the same m rrb let a denote the total area of the l state space  for each m  let a lrb m rrb denote the area of l space that is labeled by m let scm rrb denote the total border length that the region lrb s rrb labeled m share with regions of unlike m  label  a point l lying within distance v of a border can be mapped onto either m value lrb because of the noise process l  l rrb  call this a  borderline  l  a point l that is more than a distance v from every border can only be mapped onto the m value of the region containing it  suppose v is sufficiently small that lrb for the partitionings of interest rrb the area occupied by borderline l states is small compared to the total area of the l space  consider first the case in which pel rrb is uniform over l  then the information rate r lrb using eqn  5 rrb is given approximately lrb through terms of order v rrb by r    m lsb a lrb m rrb  a rsb 10g lsb a lrb m rrb  a rsb  lrb yv a rrb  ms lrb m rrb  lrb 6 rrb to see this  note that p lrb m rrb  a lrb m rrb  a and that p lrb m il rrb log p lrb m il rrb is zero except for borderline l lrb since 0 log 0  1 log 1  0 rrb  here y is a positive number whose value depends upon the details of the noise process  which determines p lrb m i l rrb for borderline l as a function of distance from the border  for small v lrb low noise rrb the first term lrb 1m rrb on the rhs of eqn  6 dominates  it is maximized when the a lrb m rrb lsb and hence the p lrb m rrb rsb values are equal for all m  the second term lrb with its minus sign rrb  which equals lrb    4il rrb  is maximized when the sum of the border lengths of all m regions is minimized  this corresponds to  sharpening  the p lrb m il rrb distribution in our earlier  more general  discussion  this suggests that the infomax solution is obtained by partitioning the l space into m regions lrb one for each m value rrb that are of substantially equal area  with each m region tending to have near minimum border length  although this simple analysis applies to the low noise case  it is plausible that even when v is comparable to the spatial scale of the m regions  infomax will favor making the m regions have approximately the same extent in all directions lrb rather than be elongated rrb  in order to  sharpen  p lrb mi l rrb and reduce the probability of the noise process mapping l onto many different m states  what if pel rrb is nonuniform  then the same result lrb equal areas  minimum border rrb is obtained except that both the area and border length elements must now be weighted by the local value of pel rrb  therefore the infomax principle tends to produce maps in which greater representation in the output space is given to regions of the input signal space that are activated more frequently  to see how lateral interactions within the m layer can affect these results  let us suppose that the l  m mapping has three  not two  process steps  l  l 489  m  m  where the first two steps are as above  and the third step changes the output m into any of a number of states m lrb which by definition comprise the  m neighborhood  of m rrb  we consider the case in which this m neighborhood relation is symmetric  this type of  lateral interaction  between m states causes the infomax principle to favor solutions for which m regions sharing a border in l space are m neighbors in the sense defined  for a simple example in which each state m has n m neighbors lrb including itself rrb  and each m neighbor has an equal chance of being the final state lrb given m rrb  infomax tends to favor each m neighborhood having similar extent in all directions lrb in l space rrb  relation between the geometric model and network properties the previous section dealt with certain classes of transformations from one message space to another  and made no specific reference to the implementation of these transformations by an interconnected network of processor cells  here we show how some of the features discussed in the previous section are related to network properties  for simplicity suppose that we have a two dimensional layer of uniformly distributed cells  and that the signal activity of each cell at any given time is either 1 lrb active rrb or 0 lrb quiet rrb  we need to specify the ensemble of input patterns  let us first consider a simple case in which each pattern consists of a disk of activity of fixed radius  but arbitrary center position  against a quiet background  in this case the pattern is fully defined by specifying the coordinates of the disk center  in a two dimensional l state space lrb previous section rrb  each pattern would be represented by a point having those coordinates  now suppose that each input pattern consists not of a sharply defined disk of activity  but of a  fuzzy  disk whose boundary lrb and center position rrb are not sharply defined  lsb such a pattern could be generated by choosing lrb from a specified distribution rrb a position xc as the nominal disk center  then setting the activity of the cell at position x to 1 with a probability that decreases with distance i x  xc i  rsb any such pattern can be described by giving the coordinates of the  center of activity  along with many other values describing lrb for example rrb various moments of the activity pattern relative to the center  for the noise process l  l we suppose that the activity of an l cell can be  misread  lrb by the cells of the m layer rrb with some probability  this set of distorted activity values is the  message  l  we then suppose that the set of output activities m is a deterministic function of l  we have constructed a situation in which lrb for an appropriate choice of noise level rrb two of the dimensions of the l state space  namely  those defined by the disk center coordinates  have large variance compared to the variance induced by the noise process  while the other dimensions have variance comparable to that induced by noise  in other words  the center position of a pattern is changed only a small amount by the noise process lrb compared to the typical difference between the center positions of two patterns rrb  whereas the values of the other attributes of an input pattern differ as much from their noise altered values as two typical input patterns differ from each other  lrb those attributes are  lost in the noise   rrb since the distance between l states in our geometric model lrb previous section rrb corresponds to the likelihood of one l state being changed into the other by the noise 490 process  we can heuristically regard the l state space lrb for the present example rrb as a  slab  that is elongated in two dimensions and very thin in all other dimensions  lrb in general this space could have a much more complicated topology  and the noise process which we here treat as defining a simple metric structure on the l state space need not do so  these complications are beyond the scope of the present discussion  rrb this example  while simple  illustrates a feature that is key to understanding the operation of the infomax principle  the character of the ensemble statistics and of the noise process jointly determine which attributes of the input pattern are statistically most significant  that is  have largest variance relative to the variance induced by noise  we shall see that the infomax principle selects a number of these most significant attributes to be encoded by the l  m transformation  we turn now to a description of the output state space m  we shall assume that this space is also of low dimensionality  for example  each m pattern may also be a disk of activity having a center defined within some tolerance  a discrete set of discriminable center coordinate values can then be used as the m region  labels  in our geometric model  restricting the form of the output activity in this particular way restricts us to considering positional encodings l  m  rather than encodings that make use of the shape of the output pattern  its detailed activity values  etc  however  this restriction on the form of the output does not determine which features of the input patterns are to be encoded  nor whether or not a topographic lrb neighbor preserving rrb mapping is to be used  these properties will be seen to emerge from the operation of the infomax principle  in the previous section we saw that the infomax principle will tend to lead to a partitioning of the l space into m regions having equal areas lsb if pel rrb is uniform in the coordinates of the l disk center rsb and minimum border length  for the present case this means that the m regions will tend to  tile  the two long dimensions of the l state space  slab   and that a single m value will represent all points ill l space that differ only in their low variance coordinates  if pel rrb is nonuniform  then the area of the m region at l will tend to be inversely proportional to pel rrb  furthermore  if there are local lateral connections between m cells  then lrb depending upon the particular form of such interaction rrb m states corresponding to nearby localized regions of layer m activity can be m neighbors in the sense of the previous section  in this case the mapping from the two high variance coordinates of l space to m space will tend to be topographic  examples  orientation selectivity and temporal feature maps the simple example in the previous section illustrates how infomax can lead to topographic maps  and to map distortions lsb which provide greater m space representation for regions of l having large pel rrb rsb  let us now consider a case in which information about input features is positionally encoded in the output layer as a result of the infomax principle  consider a model case in which an ensemble of patterns is presented to the input layer l each pattern consists of a rectangular bar of activity lrb of fixed length and width rrb against a quiet background  the bar s center position and orientation are chosen for each pattern from uniform distributions over some spatial interval for the position  and over all orientation angles lrb i e  from 0  to 180  rrb  the bar need not be sharply defined  but can be  fuzzy  in the sense described above  we assume  however  that all 491 properties that distinguish different patterns of the ensemble  except for center position and orientation  are  lost in the noise  in the sense we discussed  to simplify the representation of the solution  we further assume that only one coordinate is needed to describe the center position of the bar for the given ensemble  for example  the ensemble could consist of bar patterns all of which have the same y coordinate of center position  but differ in their x coordinate and in orientation 0  we can then represent each input state by a point in a rectangle lrb the l state space defined in a previous section rrb whose abscissa is the center position coordinate x and whose ordinate is the angle 0  the horizontal sides of this rectangle are identified with each other  since orientations of 0 0 and 180 0 are identical  lrb the interior of the rectangle can thus be thought of as the surface of a horizontal cylinder  rrb the number n x of different x positions that are discriminable is given by the range of x values in the input ensemble divided by the tolerance with which x can be measured lrb given the noise process l  l rrb  similarly for no  the relative lengths llx and mj of the sides of the l state space rectangle are given by llx  mj  nj no  we discuss below the case in which nx   no  if no were   nx the roles of x and 0 in the resulting mappings would be reversed  there is one complicating feature that should be noted  although in the interest of clarity we will not include it in the present analysis  two horizontal bar patterns that are displaced by a horizontal distance that is small compared with the bar length  are more likely to be rendered indiscriminable by the noise process than are two vertical bar patterns that are displaced by the same horizontal distance lrb which may be large compared with the bar s width rrb  the hamming distance  or number of binary activity values that need to be altered to change one such pattern into the other  is greater in the latter case than in the former  therefore  the distance in l state space between the two unoriented receptive fields figure 1  orientation selectivity in a simple model  as the input domain size lrb see text rrb is reduced lsb from lrb a rrb upper left  to lrb b rrb upper right  to lrb c rrb lower left figure rsb  infomax favors the emergence of an orientation selective l  m mapping  lrb d rrb lower right figure shows a solution obtained by applying kohonen s relaxation algorithm with 50 m points lrb shown as dots rrb to this mapping problem  492 states should be greater in the latter case  this leads to a  warped  rather than simple rectangular state space  we ignore this effect here  but it must be taken into account in a fuller treatment of the emergence of orientation selectivity  consider now an l  m transformation that consists of the three step process lrb discussed above rrb lrb i rrb noise induced l  l  lrb ii rrb deterministic l  m   lrb iii rrb lateral interaction induced m   m step lrb ii rrb maps the two dimensional l state space of points lrb x  0 rrb onto a one dimensional m state space  for the present discussion  we  consider l  m  maps satisfying the following ansatz  points corresponding to the m states are spaced uniformly  and in topographic order  along a helical line in l state space lrb which we recall is represented by the surface of a horizontal cylinder rrb  the pitch of the helix lrb or the slope do dx rrb remains to be determined by the infomax principle  each m neighborhood of m states lrb previous section rrb then corresponds to an interval on such a helix  a state l is mapped onto a state in a particular m neighborhood if l is closer lrb in l space rrb to the corresponding interval of the helix than to any other portion of the helix  we call this set of l states lrb for an m neighborhood centered on m rrb the  input domain  of m  it has rectangular shape and lies on the cylindrical surface of the l space  we have seen lrb previous sections rrb that infomax tends to produce maps having lrb i rrb equal m region areas  lrb ii rrb topographic organization  and lrb iii rrb an input domain lrb for each m neighborhood rrb that has similar extent in all directions lrb in l space rrb  our choice of ansatz enforces lrb i rrb and lrb ii rrb explicitly  criterion lrb iii rrb is satisfied by choosing do  dx such that the input domain is square lrb for a given m neighborhood size rrb  figure 1a lrb having do dx  0 rrb shows a map in which the output m encodes only information about bar center position x  and is independent of bar orientation o  the size of the m  neighborhood is relatively large in this case  the input domain of the state m denoted by the  x  is shown enclosed by dotted lines  lrb the particular 0 value at which we chose to draw the m line in fig 1a is irrelevant  rrb for this m neighborhood size  the length of the border of the input domain is as small as it can be  as the m  neighborhood size is reduced  the dotted lines move closer together  a vertically oblong input domain lrb which would result if we kept do dx  0 rrb would not satisfy the infomax criterion  the helix for which the input domain is square lrb for this smaller choice of m neighborhood size rrb is shown in fig  lb  the m states for this solution encode information about bar orientation as well as center position  if each m state corresponds to a localized output activity pattern centered at some position in a one dimensional array of m cells  then this solution corresponds to orientation selective cells organized in  orientation columns  lrb really  orientation intervals  in this one dimensional model rrb  a  labeling  of the linear array of cells according to whether their orientation preferences lie between 0 and 60  60 and 120  or 120 and 180 degrees is indicated by the bold  light  and dotted line segments beneath the rectangle in fig 1b lrb and 1c rrb  as the m neighborhood size is decreased still further  the mapping shown in fig  ie becomes favored over that of either fig 1a or lb  the  orientation columns  shown in the lower portion of fig 1c are narrower than in fig 1b  a more detailed analysis of the information rate function for various mappings confirms the main features we have here obtained by a simple geometric argument  the same type of analysis can be applied to different types of input pattern ensembles  to give just one other example  consider a network that receives an ensemble of simple patterns of acoustic input  each such pattern consists of a tone of 493 some frequency that is sensed by two  ears  with some interaural time delay  suppose that the initial network layers organize the information from each ear lrb separately rrb into tonotopic maps  and that lrb by means of connections having a range of different time delays rrb the signals received by both ears over some time interval appear as patterns of cell activity at some intermediate layer l  we can then apply the infomax principle to the signal transformation from layer l to the next layer m  the l state space can lrb as before rrb be represented as a rectangle  whose axes are now frequency and interaural delay lrb rather than spatial position and bar orientation rrb  apart from certain differences lrb the density of l states may be nonuniform  and states at the top and bottom of the rectangle are no longer identical rrb  the infomax analysis can be carried out as it was for the simplified case of orientation selectivity  local algorithms the information rate lrb eqn  i rrb  which the infomax principle states is to be maximized subject to constraints lrb and possibly as part of an optimization function containing other cost terms not discussed here rrb  has a very complicated mathematical form  how might this optimization process  or an approximation to it  be implemented by a network of cells and connections each of which has limited computational power  the geometric form in which we have cast the infomax principle for some very simple model cases  suggests how this might be accomplished  an algorithm due to kohonen 8 demonstrates how topographic maps can emerge as a result of lateral interactions within the output layer  i applied this algorithm to a one dimensional m layer and a two dimensional l layer  using a euclidean metric and imposing periodic boundary conditions on the short dimension of the l layer  a resulting map is shown in fig  id  this map is very similar to those of figs 1band ic  except for one reversal of direction  the reversal is not surprising  since the algorithm involves only local moves lrb of the m points rrb while the infomax principle calls for a globally optimal solution  more generally  kohonen s algorithm tends empirically8 to produce maps having the property that if one constructs the voronoi diagram corresponding to the positions of the m points lrb that is  assigns each point l to an m region based on which m point l is closest to rrb  one obtains a set of m regions that tend to have areas inversely proportional to p lrb l rrb  and neighborhoods lrb corresponding to our input domains rrb that tend to have similar extent in all directions rather than being elongated  the kohonen algorithm makes no reference to noise  to information content  or even to an optimization principle  nevertheless  it appears to implement  at least in a qualitative way  the geometric conditions that infomax imposes in some simple cases  this suggests that local algorithms along similar lines may be capable of implementing the infomax principle in more general situations  our geometric formulation of the infomax principle also suggests a connection with an algorithm proposed by von der malsburg and willshaw9 to generate topographic maps  in their  tea trade  model  neighborhood relationships are postulated within the source and the target spaces  and the algorithm s operation leads to the establishment of a neighborhood preserving mapping from source to target space  such neighborhood relationships arise naturally in our analysis when the infomax principle is applied to our the noise process induces a three step l  l  m   m transformation  494 neighborhood relation on the l space  and lateral connections in the m cell layer can induce a neighborhood relation on the m space  more recently  durbin and willshaw lo have devised an approach to solving certain geometric optimization problems lrb such as the traveling salesman problem rrb by a gradient descent method bearing some similarity to kohonen s algorithm  there is a complementary relationship between the infomax principle and a local algorithm that may be found to implement it  on the one hand  the principle may explain what the algorithm is  for   that is  how the algorithm may contribute to the generation of a useful perceptual system  this in turn can shed light on the system level role of lateral connections and synaptic modification mechanisms in biological networks  on the other hand  the existence of such a local algorithm is important for demonstrating that a network of relatively simple processors  biological or synthetic  can in fact find global near maxima of the shannon information rate  a possible connection between infomax and a thermodynamic principle the principle of  maximum preservation of information  can be viewed equivalently as a principle of  minimum dissipation of information   when the principle is satisfied  the loss of information from layer to layer is minimized  and the flow of information is in this sense as  nearly reversible  as the constraints allow  there is a resemblance between this principle and the principle of  minimum entropy production  ii in nonequilibrium thermodynamics  it has been suggested by prigogine and others that the latter principle is important for understanding self organization in complex systems  there is also a resemblance  at the algorithmic level  between a hebb type modification rule and the autocatalytic processes l2 considered in certain models of evolution and natural selection  this raises the possibility that the connection i have drawn between synaptic modification rules and an information theoretic optimization principle may be an example of a more general relationship that is important for the emergence of complex and apparently  goal oriented if structures and behaviors from relatively simple local interactions  in both neural and non neural systems  references lsb 1 rsb lsb 2 rsb lsb 3 rsb lsb 4 rsb lsb 5 rsb lsb 6 rsb lsb 7 rsb lsb 8 rsb lsb 9 rsb lsb 10 rsb lsb 11 rsb lsb 12 rsb r linsker  proc  natl acad  sci  usa 83 7508 8390 8779 lrb 1986 rrb  d h  hubel and t n wiesel  proc  roy  soc  london 8198 1 lrb 1977 rrb  d o hebb  the organization of behavior lrb wiley  n y  1949 rrb  r linsker  in  r cotterill lrb ed  rrb  computer simulation in brain science 20 22 august 1986  cambridge univ  press  in press rrb  p 416  r linsker  computer lrb march 1988  in press rrb  e oja  j math  bioi  15  267 lrb 1982 rrb  c  e shannon  bell syst  tech  j 27  623 lrb 1948 rrb  t  kohonen  self organization and associative memory lrb springer verlag  c von der malsburg and d j willshaw  proc  nat i  a cad  sci  usa 74  r durbin and d j willshaw  nature 326 689 lrb 1987 rrb  p glansdorff and i prigogine  thermodynamic theory of structure  fluctuations lrb wiley interscience  n y  1971 rrb  m eigen and p schuster  die naturwissenschaften 64  541 lrb 1977 rrb  lrb copenhagen  n y   19s4 rrb  5176 lrb 1977 rrb  stabili lrb v and
6 en 775 a neural network solution to the concentrator assignnlent problem gene a tagliarini edward w page department of computer science  clemson university  clemson  sc 29634 1906 abstract networks of simple analog processors having neuron like properties have been employed to compute good solutions to a variety of optimization problems  this paper presents a neural net solution to a resource allocation problem that arises in providing local access to the backbone of a wide area communication network  the problem is described in terms of an energy function that can be mapped onto an analog computational network  simulation results characterizing the performance of the neural computation are also presented  introduction this paper presents a neural network solution to a resource allocation problem that arises in providing access to the backbone of a communication network  1 in the field of operations research  this problem was first known as the warehouse location problem and heuristics for finding feasible  suboptimal solutions have been developed previously 2  3 more recently it has been known as the multifacility location problem4 and as the concentrator assignment problem 1 the hopfield neural network model the general structure of the hopfield neural network model5  6 7 is illustrated in fig 1  neurons are modeled as amplifiers that have a sigmoid input  output curve as shown in fig 2  synapses are modeled by permitting the output of any neuron to be connected to the input of any other neuron  the strength of the synapse is modeled by a resistive connection between the output of a neuron and the input to another  the amplifiers provide integrative analog summation of the currents that result from the connections to other neurons as well as connection to external inputs  to model both excitatory and inhibitory synaptic links  each amplifier provides both a normal output v and an inverted output v  the normal outputs range between 0 and 1 while the inverting amplifier produces corresponding values between 0 and 1  the synaptic link between the output of one amplifier and the input of another is defined by a conductance tij which connects one of the outputs of amplifier j to the input of amplifier i  in the hopfield model  the connection between neurons i and j is made with a resistor having a value rij  1rrij  to provide an excitatory synaptic connection lrb positive tij rrb  the resistor is connected to the normal output of this research was supported by the u s army strategic defense command   american institute of physics 1988 776 13 14 inputs 1 v o vi v2 v3 v4 outputs fig 1  schematic for a simplified hopfield network with four neurons  o  u  u fig 2  amplifier input output relationship amplifier j to provide an inhibitory connection lrb negative tij rrb  the resistor is connected to the inverted output of amplifier j  the connections among the neurons are defined by a matrix t consisting of the conductances t ij  hopfield has shown that a symmetric t matrix lrb tij  tji rrb whose diagonal entries are all zeros  causes convergence to a stable state in which the output of each amplifier is either 0 or 1  additionally  when the amplifiers are operated in the high gain mode  the stable states of a network of n neurons correspond to the local minima of the quantity n e  lrb 112 rrb n l l i  l j  l n t v v  ij 1 j l v i  i 1 lrb 1 rrb where vi is the output of the ith neuron and ii is the externally supplied input to the ph neuron  hopfield refers to e as the computational energy of the system  the concentrator assignment problem consider a collection of n sites that are to be connected to m concentrators as illustrated in fig 3 lrb a rrb  the sites are indicated by the shaded circles and the concentrators are indicated by squares  the problem is to find an assignment of sites to concentrators that minimizes the total cost of the assignment and does not exceed the capacity of any concentrator  the constraints that must be met can be summarized as follows  a rrb each site i lrb i and  1  2    n rrb is connected to exactly one concentrator  777 b rrb each concentrator j lrb j  1  2    m rrb is connected to no more than kj sites lrb where kj is the capacity of concentrator d figure 3 lrb b rrb illustrates a possible solution to the problem represented in fig 3 lrb a rrb   0 0    0      0 o concentrators  sites lrb a rrb  site concentrator map lrb b rrb  possible assignment fig 3  example concentrator assignment problem if the cost of assigning site i to concentrator j is cij  then the total cost of a particular assignment is total cost n m i  l j  l  l l x  ij c  ij lrb 2 rrb where xij  1 only if we actually decide to assign site i to concentrator j and is 0 otherwise  there are mn possible assignments of sites to concentrators that satisfy constraint a rrb  exhaustive search techniques are therefore impractical except for relatively small values of m and n  the neural network solution this problem is amenable to solution using the hopfield neural network model  the hopfield model is used to represent a matrix of possible assignments of sites to concentrators as illustrated in fig 4  each square corresponds 778 concentrators 1 2 j m r        r sites 1  ii 11   iii  iii  2        i       i the darkly shaded neu    i   n  iii 11   ii  ii iron corresponds to the      ii   ii  ii     n  l slack   n 2   n  k j     hypothesis that site i should be as  igned to   j concentrator j  ii 111  11  11 iii ii       ii 11   iii  iii fig 4  concentrator assignment array to a neuron and a neuron in row i and column j of the upper n rows of the array represents the hypothesis that site i should be connected to concentrator j  if the neuron in row i and column j is on  then site i should be assigned to concentrator j  if it is off  site i should not be assigned to concentrator j  the neurons in the lower sub array  indicated as  slack   are used to implement individual concentrator capacity constraints  the number of slack neurons in a column should equal the capacity lrb expressed as the number sites which can be accommodated rrb of the corresponding concentrator  while it is not necessary to assume that the concentrators have equal capacities  it was assumed here that they did and that their cumulative capacity is greater than or equal to the number of sites  to ena  le the neurons in the network illustrated above to compute solutions to the concentrator problem  the network must realize an energy function in which the lowest energy states correspond to the least cost assignments  the energy function must therefore favor states which satisfy constraints a rrb and b rrb above as well as states that correspond to a minimum cost assignment  the energy function is implemented in terms of connection strengths between neurons  the following section details the construction of an appropriate energy function  779 the energy function consider the following energy equation  n e  a m l lrb  l1 y 1j    1 1  2  1 rrb  j  b m n  k  j  1 i  1 l lrb l j y    k  rrb 2 ij j lrb 3  m n  kj  c l l y  lrb 1  yij rrb j  1 i  1 1j where y ij is the output of the amplifier in row i and column j of the neuron matrix  m and n are the number of concentrators and the number of sites respectively  and kj is the capacity of concentrator j  the first term will be minimum when the sum of the outputs in each row of neurons associated with a site equals one  notice that this term influences only those rows of neurons which correspond to sites  no term is used to coerce the rows of slack neurons into a particular state  the second term of the equation will be minimum when the sum of the outputs in each column equals the capacity kj of the corresponding concentrator  the presence of the kj slack neurons in each column allows this term to enforce the concentrator capacity restrictions  the effect of this term upon the upper sub array of neurons lrb those which correspond to site assignments rrb is that no more than kj sites will be assigned to concentrator j  the number of neurons to be turned on in column j is kj  consequently  the number of neurons turned on in column j of the assignment sub array will be less than or equal to kj  the third term causes the energy function to favor the  zero  and  one  states of the individual neurons by being minimum when all neurons are in one or the other of these states  this term influences all neurons in the network  in summary  the first term enforces constraint a rrb and the second term enforces constraint b rrb above  the third term guarantees that a choice is actually made  it assures that each neuron in the matrix will assume a final state near zero or one corresponding to the xij term of the cost equation lrb eq  2 rrb  after some algebraic re arrangement  eq  3 can be written in the form of eq  1 where   lcb a  8 lrb i  k rrb  lrb 1 8u  i rrb  b  8u 1 rrb  lrb 1 8 lrb i  k   if i  n and k  n t ij kl  lrb 4 rrb  c  8u  i rrb  lrb 1 8 lrb i  k   if i  n or k  n  here quadruple subscripts are used for the entries in the matrix t each entry indicates the strength of the connection between the neuron in row i and column j and the neuron in row k and column i of the neuron matrix  the function delta is given by 780 1  if i  j lrb 5 rrb 0  otherwise  the a and b terms specify inhibitions within a row or a column of the upper sub array and the c term provides the column inhibitions required for the neurons in the sub array of slack neurons  8 lrb i  j rrb  lcb equation 3 specifies the form of a solution but it does not include a term that will cause the network to favor minimum cost assignments  to complete the formulation  the following term is added to each tij  kl  d  8 lrb j  i rrb  lrb 1  8 lrb i  k rrb rrb lrb cost lsb i  j rsb  cost lsb k  i rsb rrb where cost lsb i  j rsb is the cost of assigning site i to concentrator j  the effect of this term is to reduce the inhibitions among the neurons that correspond to low cost assignments  the sum of the costs of assigning both site i to concentrator j and site k to concentrator i was used in order to maintain the symmetry of t  the external input currents were derived from the energy equation lrb eq 3 rrb and are given by j  if i  n lrb 6 rrb ij 2  k j  1  otherwise  i   lcb 2  k this exemplifies a technique for combining external input currents which arise from combinations of certain basic types of constraints  an example the neural network solution for a concentrator assignment problem consisting of twelve sites and five concentrators was simulated  all sites and concentrators were located within the unit square on a randomly generated map  for this problem  it was assumed that no more than three sites could be assigned to a concentrator  the assignment cost matrix and a typical assignment resulting from the simulation are shown in fig 5  it is interesting to notice that the network proposed an assignment which made no use of concentrator 2  because the capacity of each concentrator kj was assumed to be three sites  the external input current for each neuron in the upper sub array was i ij  6 while in the sub array of slack neurons it was i ij  5  the other parameter values used in the simulation were a  b  c  2 and d  0 1  781 sites 1 concentrators 2 4 3 5  46 40 63 39 92 38 82 81 56  51 76 46 17 39 77 41 h  81 54 52 i 60 67 44 j  84 76 k 42 33 55 l  a 47 28 b 72 75 c 95 71 d 88 78    e 31 62 f 25 g 55 60 1 05 g b 66 71 g g 56 51 48 38 18 fig 5  the concentrator assignment cost matrix with choices circled  since this choice of parameters results in a t matrix that is symmetric and whose diagonal entries are all zeros  the network will converge to the minima of eq  3  furthermore  inclusion of the term which is weighted by the parameter d causes the network to favor minimum cost assignments  to evaluate the performance of the simulated network  an exhaustive search of all solutions to the problem was conducted using a backtracking algorithm  a frequency distribution of the solution costs associated with the assignments generated by the exhaustive search is shown in fig 6  for comparison  a histogram of the results of one hundred consecutive runs of the neural net simulation is shown in fig 7  although the neural net simulation did not find a global minimum  ninety two of the one hundred assignments which it did find were among the best 0 01  of all solutions and the remaining eight were among the best 0 3   conclusion neural networks can be used to find good  though not necessarily optimal  solutions to combinatorial optimization problems like the concentrator 782 frequency frequency 25 4000000 3500000 3000000 250000 20 15 10 1500000 100000 500000 5 ol  3 2 4 2 5 2 6 2 7 2 cost o 8 2 fig 6  distribution of assignment fig 7  distribution of assignment costs resulting from an exhaustive costs resulting from 100 consecutive executions of the neural net search of all possible solutions  simulation  assignment problem  in order to use a neural network to solve such problems  it is necessary to be able to represent a solution to the problem as a state of the network  here the concentrator assignment problem was successfully mapped onto a hopfield network by associating each neuron with the hypothesis that a given site should be assigned to a particular concentrator  an energy function was constructed to determine the connections that were needed and the resulting neural network was simulated  while the neural network solution to the concentrator assignment problem did not find a globally minimum cost assignment  it very effectively rejected poor solutions  the network was even able to suggest assignments which would allow concentrators to be removed from the communication network  references 1  a s tanenbaum  computer networks lrb prentice hall  englewood cliffs  new jersey  1981 rrb  p 83  2  e feldman  f a lehner and t l ray  manag  sci  v12  670 lrb 1966 rrb  3  a kuehn and m hamburger  manag  sci  v9  643 lrb 1966 rrb  4  t aykin and a 1  g babu  1  of the oper  res  soc  v38  n3  241 lrb 1987 rrb  5  j 1  hopfield  proc  natl acad  sci  u s a  v79  2554 lrb 1982 rrb  6  j 1  hopfield and d w tank  bio  cyber  v52  141 lrb 1985 rrb  7  d w tank and 1  1  hopfield  ieee trans  on cir  and sys  cas 33  n5  533 lrb 1986 rrb 
7 en 377 experimental demonstrations of optical neural computers ken hsu  david brady  and demetri psaltis department of electrical engineering california institute of technology pasadena  ca 91125 abstract we describe two expriments in optical neural computing  in the first a closed optical feedback loop is used to implement auto associative image recall  in the second a perceptron iike learning algorithm is implemented with photorefractive holography  introduction the hardware needs of many neural computing systems are well matched with the capabilities of optical systems l 2 3  the high interconnectivity required by neural computers can be simply implemented in optics because channels for optical signals may be superimposed in three dimensions with little or no cross coupling  since these channels may be formed holographically  optical neural systems can be designed to create and maintain interconnections very simply  thus the optical system designer can to a large extent avoid the analytical and topological problems of determining individual interconnections for a given neural system and constructing physical paths for these interconnections  an archetypical design for a single layer of an optical neural computer is shown in fig 1  nonlinear thresholding elements  neurons  are arranged on two dimensional planes which are interconnected via the third dimension by holographic elements  the key concerns in implementing this design involve the need for suitable nonlinearities for the neural planes and high capacity  easily modifiable holographic elements  while it is possible to implement the neural function using entirely optical nonlinearities  for example using etalon arrays  optoelectronic two dimensional spatial light modulators lrb 2d slms rrb suitable for this purpose are more readily available  and their properties  i e speed and resolution  are well matched with the requirements of neural computation and the limitations imposed on the system by the holographic interconnections 5 6  just as the main advantage of optics in connectionist machines is the fact that an optical system is generally linear and thus allows the superposition of connections  the main disadvantage of optics is that good optical nonlinearities are hard to obtain  thus most slms are optoelectronic with a non linearity mediated by electronic effects  the need for optical nonlinearities arises again when we consider the formation of modifiable optical interconnections  which must be an all optical process  in selecting  american institute of physics 1988 378 a holographic material for a neural computing application we would like to have the capability of real time recording and slow erasure  materials such as photographic film can provide this only with an impractical fixing process  photorefractive crystals are nonlinear optical materials that promise to have a relatively fast recording response and long term memory4 5 6 7  b                    w    7            fourier lens hologro phlc i  iealui  i fourier lens figure 1  optical neural computer architecture  in this paper we describe two experimental implementations of optical neural computers which demonstrate how currently available optical devices may be used in this application  the first experiment we describe involves an optical associative loop which uses feedback through a neural plane in the form of a pinhole array and a separate thresholding plane to implement associate regeneration of stored patterns from correlated inputs  this experiment demonstrates the input output dynamics of an optical neural computer similar to that shown in fig 1  implemented using the hughes liquid crystal light valve  the second experiment we describe is a single neuron optical perceptron implemented with a photorefractive crystal  this experiment demonstrates how the learning dynamics of long term memory may be controlled optically  by combining these two experiments we should eventually be able to construct high capacity adaptive optical neural computers  optical associative loop a schematic diagram of the optical associative memory loop is shown in fig 2  it is comprised of two cascaded vander lugt correlators9  the input section of the system from the threshold device p1 through the first hologram p2 to the pinhole array p3 forms the first correlator  the feedback section from p3 through the second hologram p4 back to the threshold device p1 forms the second correlator  an array of pinholes sits on the back focal plane of l2  which coincides with the front focal plane of l3  the purpose of the pinholes is to link the first and the second lrb reversed rrb correlator to form a closed optical feedback loop 10  there are two phases in operating this optical loop  the learning phase and the recal phase  in the learning phase  the images to be stored are spatially multiplexed and entered simultaneously on the threshold device  the 379 thresholded images are fourier transformed by the lens ll  the fourier spectrum and a plane wave reference beam interfere at the plane p2 and record a fourier transform hologram  this hologram is moved to plane p4 as our stored memory  we then reconstruct the images from the memory to form a new input to make a second fourier transform hologram that will stay at plane p2  this completes the learning phase  in the recalling phase an input is imaged on the threshold input            device  this image is correlated with the reference images in the hologram at p2  if the correlation between the input and one of the stored images is high a bright peak appears at one of the pinholes  this peak is sampled by        second pinhole hologram array     l z the pinhole to reconstruct the stored i i image from the hologram at p4  the reconstructed beam is then imaged back to the threshold device to form a closed loop  if the overall optical gain figure  2  all optical associative in the loop exceeds the loss the loop loop  the threshold device is a lclv  signal will grow until the threshold and the holograms are thermoplastic device is saturated  in this case  we plates  can cutoff the external input image and the optical loop will be latched at the stable memory  the key elements in this optical loop are the holograms  the pinhole array  and the threshold device  if we put a mirror 10 or a phase conjugate mirror 7 11 at the pinhole plane p3 to reflect the correlation signal back through the system then we only need one hologram to form a closed loop  the use of two holograms  however  improves system performance  we make the hologram at p2 with a high pass characteristic so that the input section of the loop has high spectral discrimination  on the other hand we want the images to be reconstructed with high fidelity to the original images  thus the hologram at plane p4 must have broadband characteristics  we use a diffuser to achieve this when making this hologram  fig 3a shows the original images  fig 3b and fig 3c are the images reconstructed from first and second holograms  respectively  as desired  fig 3b is a high pass version of the stored image while fig 3c is broadband  each of the pinholes at the correlation plane p3 has a diameter of 60 j lm  the separations between the pinholes correspond to the separations of the input images at plane p 1  if one of the stored images appears at p 1 there will be a bright spot at the corresponding pinhole on plane p3  if the input image shifts to the position of another image the correlation peak will also 380          a    j  a          lrb i      y        il      r i k     t l      b c figure 3  lrb a rrb the original images  lrb b rrb the reconstructed images from the highpass hologram p2  lrb c rrb the reconstructed images from the band pass hologram p4  shift to another pinhole  but if the shift is not an exact image spacing the correlation peak can not pass the pinhole and we lose the feedback signal  therefore this is a loop with  discrete  shift invariance  without the pinholes the cross correlation noise and the auto correlation peak will be fed back to the loop together and the reconstructed images wo n t be recognizable  there is a compromise between the pinhole size and the loop performance  small pinholes allow good memory discrimination and sharp reconstructed images  but can cut the signal to below the level that can be detected by the threshold device and reduce the tolerance of the system to shifts in the input  the function of the pinhole array in this system might also be met by a nonlinear spatial light modulator  in which case we can achieve full shift invariance 12  the threshold device at plane pi is a hughes liquid crystal light valve  the device has a resolution of 16 ip mm and uniform aperture of 1 inch diameter  this gives us about 160 000 neurons at pi  in order to compensate for the optical loss in the loop  which is on the order of 10  5  we need the neurons to provide gain on the order of 105  in our system this is achieved by placing a hamamatsu image intensifier at the write side of the lclv  since the microchannel plate of the image intensifier can give gains of 104  the combination of the lclv and the image intensifier can give gains of 10 6 with sensitivity down to n w  cm 2  the optical gain in the loop can be adjusted by changing the gain of the image intensifier  since the activity of neurons and the dynamics of the memory loop is a continuously evolving phenomenon  we need to have a real time device to monitor and record this behavior  we do this by using a prism beam splitter to take part of the read out beam from the lclv and image it onto a ccd camera  the output is displayed on a crt monitor and also recorded on a video tape recorder  unfortunately  in a paper we can only show static pictures taken from the screen  we put a window at the ccd plane so that each time we can pick up one of the stored images  fig 4a shows the read out image 381 a b c figure 4  lrb a rrb the external input to the optical loop  lrb b rrb the feedback image superimposed with the input image  lrb c rrb the latched loop image  from the lclv which comes from the external input shifted away from its stored position  this shift moves its correlation peak so that it does not match the position of the pinhole  thus there is no feedback signal going through the loop  if we cut off the input image the read out image will die out with a characteristic time on the order of 50 to 100 ms  corresponding to the response time of the lclv  now we shift the input image around trying to search for the correct position  once the input image comes close enough to the correct position the correlation peak passes through the right pinhole  giving a strong feedback signal superimposed with the external input on the neurons  the total signal then goes through the feedback loop and is amplified continuously until the neurons are saturated  depending on the optical gain of the neurons the time required for the loop to reach a stable state is between 100 ms and several seconds  fig 4b shows the superimposed images of the external input and the loop images  while the feedback signal is shifted somewhat with respect to the input  there is sufficient correlation to induce recall  if the neurons have enough gain then we can cut off the input and the loop stays in its stable state  otherwise we have to increase the neuron gain until the loop can sustain itself  fig 4c shows the image in the loop with the input removed and the memory latched  if we enter another image into the system  again we have to shift the input within the window to search the memory until we are close enough to the correct position  then the loop will evolve to another stable state and give a correct output  the input images do not need to match exactly with the memory  since the neurons can sense and amplify the feedback signal produced by a partial match between the input and a stored image  the stored memory can grow in the loop  thus the loop has the capability to recall the complete memory from a partial input  fig 5a shows the image of a half face input into the system  fig 5b shows the overlap of the input with the complete face from the memory  fig 5c shows the stable state of the loop after we cut off the external input  in order to have this associative behavior the input must have enough correlation with the stored memory to yield a strong feedback signal  for instance  the loop does not respond to the the presentation of a picture of 382 a c figure 5  lrb a rrb partial face used as the external input  lrb b rrb the superimposed images of the partial input with the complete face recalled by the loop  lrb c rrb the complete face latched in the loop  a b c figure 6  lrb a rrb rotated image used as the external input  lrb b rrb the superimposed images of the input with the recalled image from the loop  lrb c rrb the image latched in the optical loop  a person not stored in memory  another way to demonstrate the associative behavior of the loop is to use a rotated image as the input  experiments show that for a small rotation the loop can recognize the image very quickly  as the input is rotated more  it takes longer for the loop to reach a stable state  if it is rotated too much  depending on the neuron gain  the input wo n t be recognizable  fig 6a shows the rotated input  fig 6b shows the overlap of loop image with input after we turn on the loop for several seconds  fig 6c shows the correct memory recalled from the loop after we cut the input  there is a trade off between the degree of distortion at the input that the system can tolerate and its ability to discriminate against patterns it has not seen before  in this system the feedback gain lrb which can be adjusted through the image intensifier rrb controls this trade off  photorefractive perceptron holograms are recorded in photorefractive crystals via the electrooptic modulation of the index of refraction by space charge fields created by the migration of photogenerated charge 13 14  photorefractive crystals are attractive for optical neural applications because they may be used to store 383 long term interactions between a very large number of neurons  while photorefractive recording does not require a development step  the fact that the response is not instantaneous allows the crystal to store long term traces of the learning process  since the photorefractive effect arises from the reversible redistribution of a fixed pool of charge among a fixed set of optically addressable trapping sites  the photorefractive response of a crystal does not deteriorate with exposure  finally  the fact that photorefractive holograms may extend over the entire volume of the crystal has previously been shown to imply that as many as 10 10 interconnections may be stored in a single crystal with the independence of each interconnection guaranteed by an appropriate spatial arrangement of the interconnected neurons 6 5  in this section we consider a rudimentary optical neural system which uses the dynamics of photorefractive crystals to implement perceptron like learning  the architecture of this system is shown schematically in fig 7  the input to the system  x  corresponds to a two dimensional pattern recorded from a video monitor onto a liquid crystal light valve  the light valve transfers this pattern on a laser beam  this beam is split into two paths which cross in a photorefractive crystal  the light propagating along each path is focused such that an image of the input pattern is formed on the crystal  the images along both paths are of the same size and are superposed on the crystal  which is assumed to be thinner than the depth of focus of the images  the intensity diffracted from one of the two paths onto the other by a hologram stored in the crystal is isolated by a polarizer and spatially integrated by a single output detector  the thresholded output of this detector corresponds to the output of a neuron in a perceptron  laser    t   pb lcl v tv  f4hj ucl bs   col  lputer xtal pm figure 7  photorefractive perceptron  pb is a polarizing beam splitter  ll and l2 are imaging lenses  wp is a quarter waveplate  pm is a piezoelectric mirror  p is a polarizer  d is a detector  solid lines show electronic control  dashed lines show the optical path  the ith component of the input to this system corresponds to the intensity in the ith pixel of the input pattern  the interconnection strength  wi  between the ith input and the output neuron corresponds to the diffraction efficiency of the hologram taking one path into the other at the ith pixel of the image plane  while the dynamics of wi can be quite complex in some geometries 384 and crystals  it is possible to show from the band transport model for the photorefractive effect that under certain circumstances the time development of wi may be modeled by lrb 1 rrb where m lrb s rrb and 4  lrb s rrb are the modulation depth and phase  respectively  of the interference pattern formed in the crystal between the light in the two paths 15  t is a characteristic time constant for crystal  t is inversely proportional to the intensity incident on the ith pixel of the crystal  using eqn  1 it is possible to make wi lrb t rrb take any value between 0 and w m l1z by properly exposing the ith pixel of the crystal to an appropriate modulation depth and intensity  the modulation depth between two optical beams can be adjusted by a variety of simple mechanisms  in fig 7 we choose to control met rrb using a mirror mounted on a piezoelectric crystal  by varying the frequency and the amplitude of oscillations in the piezoelectric crystal we can electronically set both met rrb and 4  lrb t rrb over a continuous range without changing the intensity in the optical beams or interrupting readout of the system  with this control over met rrb it is possible via the dynamics described in eqn  lrb 1 rrb to implement any learning algorithm for which wi can be limited to the range lrb 0  w maz rrb  the architecture of fig 7 classifies input patterns into two classes according to the thresholded output of the detector  the goal of a learning algorithm for this system is to correctly classify a set of training patterns  the perceptron learning algorithm involves simply testing each training vector and adding training vectors which yield too iowan output to the weight vector and subtracting training vectors which yield too high an output from the weight vector until all training vectors are correctly classified 16  this training algorithm is described by the equation l  wi  axj where alpha is positive lrb negative rrb if the output for x is too low lrb high rrb  an optical analog of this method is implemented by testing each training pattern and exposing the crystal with each incorrectly classified pattern  training vectors that yield a high output when a low output is desired are exposed at zero modulation depth  training vectors that yield a low output when high output is desired are exposed at a modulation depth of one  the weight vector for the k  1th iteration when erasure occurs in the kth iteration is given by lrb 2 rrb where we assume that the exposure time  l  t  is much less than t note that since t is inversely proportional to the intensity in the ith pixel  the change in 385 wi is proportional to the ith input  the weight vector at the k  1th iteration when recording occurs in the kth iteration is given by 2  t   t     t   t wi lrb k  1 rrb  e r wi lrb k rrb 2 y wi lrb k rrb wmcue r  lrb l e r  rrb  wmaz lrb l e r  rrb to lowest order in 6  t    2 lrb 3 rrb and   eqn  lrb 3 rrb yields w m     t  t 2 wi lrb k  1 rrb  wi lrb k rrb  2y wi lrb k rrb wmaz lrb  rrb  wmaz lrb  rrb t t lrb 4 rrb once again the change in wi is proportional to the ith input  we have implemented the architecture of fig 7 using a sbn60  ce crystal provided by the rockwell international science center  we used the 488 nm line of an argon ion laser to record holograms in this crystal  most of the patterns we considered were laid out on 10 x 10 grids of pixels  thus allowing 100 input channels  ultimately  the number of channels which may be achieved using this architecture is limited by the number of pixels which may be imaged onto the crystal with a depth of focus sufficient to isolate each pixel along the length of the crystal       y     1 3 2    4 figure 8  training patterns   1  1 j ia  8   l t i  0 0 acoftcls  w ciii rrb figure 9  output in the second training cycle  using the variation on the perceptron learning algorithm described above with a fixed exposure times  tr and  te for recording and erasing  we have been able to correctly classify various sets of input patterns  one particular set which we used is shown in fig 8  in one training sequence  we grouped patterns 1 and 2 together with a high output and patterns 3 and 4 together with a low output  after all four patterns had been presented four times  the system gave the correct output for all patterns  the weights stored in the crystal were corrected seven times  four times by recording and three by erasing  fig  9a shows the output of the detector as pattern 1 is recorded in the second learning cycle  the dashed line in this figure corresponds to the threshold level  fig 9b shows the output of the detector as pattern 3 is erased in the second learning cycle  386 conclusion the experiments described in this paper demonstrate how neural network architectures can be implemented using currently available optical devices  by combining the recall dynamics of the first system with the learning capability of the second  we can construct sophisticated optical neural computers  acknowledgements the authors thank ratnakar neurgaonkar and rockwell international for supplying the sbn crystal used in our experiments and hamamatsu photonics k k for assistance with image intesifiers  we also thank eung gi paek and kelvin wagner for their contributions to this research  this research is supported by the defense advanced research projects agency  the army research office  and the air force office of scientific research  references 1  y s abu mostafa and d psaltis  scientific american  pp 88 95  march  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  1987  d psaltis and n h farhat  opt  lett  10  lrb 2 rrb 98 lrb 1985 rrb  a d fisher  r c fukuda  and j n lee  proc  spie 625  196 lrb 1986 rrb  k wagner and d psaltis  appl  opt  26 lrb 23 rrb  pp 5061 5076 lrb 1987 rrb  d psaltis  d brady  and k wagner  applied optics  march 1988  d psaltis  j yu  x g gu  and h lee  second topical meeting on optical computing  incline village  nevada  march 16 18 1987  a yariv  s k  kwong  and k kyuma  spie proc  613 01  lrb 1986 rrb  d z anderson  proceedings of the international conference on neural networks  san diego  june 1987  a b vander lugt  ieee trans  inform  theory  it i0 lrb 2 rrb  pp 139145 lrb 1964 rrb  e g paek and d psaltis  opt  eng  26 lrb 5 rrb  pp 428 433 lrb 1987 rrb  y owechko  g j dunning  e marom  and b h soffer  appl  opt  26  lrb 10 rrb 1900 lrb 1987 rrb  d psaltis and j hong  opt  eng  26 10 lrb 1987 rrb  n v kuktarev  v b markov  s g odulov  m s soskin  and v l vinetskii  ferroelectrics  22 949 lrb 1979 rrb  j feinberg  d heiman  a r tanguay  and r w hellwarth  j appl  phys  51 1297 lrb 1980 rrb  t j hall  r jaura  l m connors  p d foote  prog  quan  electr  10 77 lrb 1985 rrb  f rosenblatt   principles of neurodynamics  perceptron and the theory of brain mechanisms  spartan books  washington  lrb 1961 rrb 
8 en 242 the sigmoid nonlinearity in prepyriform cortex frank h eeckman university of california  berkeley  ca 94720 abslract we report a study  on the relationship between eeg amplitude values and unit spike output in the prepyriform cortex of awake and motivated rats  this relationship takes the form of a sigmoid curve  that describes normalized pulse output for normalized wave input  the curve is fitted using nonlinear regression and is described by its slope and maximum value  measurements were made for both excitatory and inhibitory neurons in the cortex  these neurons are known to form a monosynaptic negative feedback loop  both classes of cells can be described by the same parameters  the sigmoid curve is asymmetric in that the region of maximal slope is displaced toward the excitatory side  the data are compatible with freeman s model of prepyriform burst generation  other analogies with existing neural nets are being discussed  and the implications for signal processing are reviewed  in particular the relationship of sigmoid slope to efficiency of neural computation is examined  introduction the olfactory cortex of mammals generates repeated nearly sinusoidal bursts of electrical activity lrb eeg rrb in the 30 to 60 hz  range 1  these bursts ride on top of a slower lrb 1 to 2 hz  rrb  high amplitude wave related to respiration  each burst begins shortly after inspiration and terminates during expiration  they are generated locally in the cortex  similar bursts occur in the olfactory bulb lrb ob rrb and there is a high degree of correlation between the activity in the two structures   the two main cell types in the olfactory cortex are the superficial pyramidal cell lrb type a rrb  an excitatory neuron receiving direct input from the ob  and the cortical granule cell lrb type b rrb  an inhibitory interneuron  these cell groups are monosynaptically connected in a negative feedback loop2  superficial pyramidal cells are mutually excitatory3  4  5 as well as being excitatory to the granule cells  the granule cells are inhibitory to the pyramidal cells as well as to each other3  4  6  in this paper we focus on the analysis of amplitude dependent properties  how is the output of a cellmass lrb pulses rrb related to the synaptic potentials lrb ie  waves rrb  the concurrent recording of multi unit spikes and eeg allows us to study these phenomena in the olfactory cortex  the anatomy of the olfactory system has been extensively studied beginning with the work of s ramon y cajal 7  the regular geometry and the simple three layered architecture makes these structures ideally suitable for eeg recording 4  8  the eeg generators in the various olfactory regions have been identified and their synaptic connectivities have been extensively studied9  10 5 4  11 6  the eeg is the scalar sum of synaptic currents in the underlying cortex  it can be recorded using low impedance  5 mohm rrb cortical or depth electrodes  multiunit signals are recorded in the appropriate cell layers using high impedance lrb  5 mohm rrb electrodes and appropriate high pass filtering  here we derive a function that relates waves lrb eeg rrb to pulses in the olfactory cortex of the rat  this function has a sigmoidal shape  the derivative of this curve  american institute of physics 1988 243 gives us the gain curve for wave to pulse conversion  this is the forward gain for neurons embedded in the cortical cellmass  the product of the forward gain values of both sets of neurons lrb excitatory and inhibitory rrb gives us the feedback gain values  these ultimately determine the dynamics of the system under study  materials and meti iods a total of twenty nine rats were entered in this study  in each rat a linear array of 6 100 micron stainless steel electrodes was chronically implanted in the prepyriform lrb olfactory rrb cortex  the tips of the electrodes were electrolytically sharpened to produce a tip impedance on the order of 5 to 1 megaohm  the electrodes were implanted laterally in the midcortex  using stereotaxic coordinates  their position was verified electrophysiologically using a stimulating electrode in the olfactory tract  this procedure has been described earlier by freeman 12  at the end of the recording session a small iron deposit was made to help in histological verification  every electrode position was verified in this manner  each rat was recorded from over a two week period following implantation  all animals were awake and attentive  no stimulation lrb electrical or olfactory rrb was used  the background environment for recording was the animal s home cage placed in the same room during all sessions  for the present study two channels of data were recorded concurrently  channel 1 carried the eeg signal  filtered between 10 and 300 hz  and digitized at 1 ms intervals  channel 2 carried standard pulses 5 v  1 2 ms wide  that were obtained by passing the multi unit signal lrb filtered between 300 hz  and 3khz  rrb through a window discriminator  these two time series were stored on disk for off line processing using a perkinelmer 3220 computer  all routines were written in fortran  they were tested on data files containing standard sine wave and pulse signals  data processing the procedures for obtaining a two dimensional conditional pulse probability table have been described earlier 4  this table gives us the probability of occurrence of a spike conditional on both time and normalized eeg amplitude value  by counting the number of pulses at a fixed time delay  where the eeg is maximal in amplitude  and plotting them versus the normalized eeg amplitudes  one obtains a sigmoidal function  the pulse probability sigmoid curve lrb psc rrb 13  14  this function is normalized by dividing it by the average pulse level in the record  it is smoothed by passing it through a digital 1  1  1 filter and fitted by nonlinear regression  the equations are  q  qmax lrb 1  exp lsb  lrb ev  1 rrb i qmax rsb rrb for v   uo q  1 for v   uo lrb 1 rrb where uo is the steady state voltage  and q  lrb p po rrb  po  and qmax  lrb pmax po rrb  po  po is the background pulse count  pmax is the maximal pulse count  these equations rely on one parameter only  the derivation and justification for these equations were discussed in an earlier paper by freeman 13  244 results data were obtained from all animals  they express normalized pulse counts  a dimensionless value as a function of normalized eeg values  expressed as a z score lrb ie  ranging from  3 sd  to  3 sd  with mean of 0 0 rrb  the true mean for the eeg after filtering is very close to 0 0 m v and the distribution of amplitude values is very nearly gaussian  the recording convention was such that high eeg values lrb ie   0 0 to  3 0 sd  rrb corresponded to surface negative waves  these in turn occur with activity at the apical dendrites of the cells of interest  low eeg values lrb ie  from  3 0 sd  to  0 0 rrb corresponded to surface positive voltage values  representing inhibition of the cells  the data were smoothed and fitted with equation lrb 1 rrb  this yielded a qrnax value for every data file  there were on average 5 data files per animal  of these 5  an average of 3 7 per animal could be fitted succesfully with our technique  in 25  of the traces  each representing a different electrode pair  no correlations between spikes and the eeg were found  besides qmax we also calculated q  the maximum derivative of the psc  representing the maximal gain  there were 108 traces in all  in the first 61 cases the qrnax value described the wave to pulse conversion for a class of cells whose maximum firing probability is in phase with the eeg  these cells were labelled type a cells 2  these traces correspond to the excitatory pyramidal cells  the mean for qmax in that group was 14 6  with a standard deviation of 1 84  the range was 10 5 to 17 8  in the remaining 47 traces the qmax described the wave to pulse conversion for class b cells  class b is a label for those cells whose maximal firing probability lags the eeg maximum by approximately 1 4 cycle  the mean for qrnax in that group was 14 3  with a standard deviation of 2 05  the range in this group was 11 0 to 18 8  the overall mean for qmax was 14 4 with a standard deviation of 1 94  there is no difference in qmax between both groups as measured by the student t test  the nonparametric wilcoxon rank sum test also found no difference between the groups lrb p  0 558 for the t test  p  0 729 for the wilcoxon rrb  assuming that the two groups have qmax values that are normally distributed lrb in group a  mean  14 6  median  14 6  in group b  mean  14 3  median  14 1 rrb  and that they have equal variances lrb st deviation group a is 1 84  st deviation group b is 2 05 rrb but different means  we estimated the power of the t test to detect that difference in means  a difference of 3 points between the qmax s of the respective groups was considered to be physiologically significant  given these assumptions the power of the t test to detect a 3 point difference was greater than 999 at the alpha 05 level for a two sided test  we thus feel reasonably confident that there is no difference between the qmax values of both groups  the first derivative of the psc gives us the gain for wave to pulse conversion4  the maximum value for this first derivative was labelled q   the location at which the maximum q  occurs was labelled vmax  vmax is expressed in units of standard deviation of eeg amplitudes  the mean for q  in group a was 5 7  with a standard deviation of 67  in group b it was 5 6 with standard deviation of 73  since q  depends on qmax  the same statistics apply to both  there was no significant difference between the two groups for slope maxima  245 figure 1  distribution of qmax values group a 14 h cii  q      12 10  8 i  6 i  group b    r    r                  h cii  q  r              r   4 i       1        1       v  2       v           o 1011121314151617181920 qmax values   1         14  12  10  8 i  6 i  4  2   o                                                      l      f71   p   1011121314151617181920 qmax values the mean for vmax was at 2 15 sd     307  in every case vmax was on the excitatory side from 0 00  ie  at a positive value of eeg z scores  all values were greater than 1 00  a similar phenomenon has been reported in the olfactory bulb 4  14  15  figure 2  examples of sigmoid fits  a cell b cell 14 14 12 12 10 10 8 8 6 6 cii 4 4  2 2 0 0 2 2    rot  cii og 11  po 4 3 2 1 0 1 2 3 4 3 2 0 1 1 normalized eeg amplitude qm  14 0 qm  13 4 2 3 246 comparison with data from tiie ob previously we derived qrnax values for the mitral cell population in the olfactory bulb14  the mitral cells are the output neurons of the bulb and their axons form the lateral olfactory tract lrb lot rrb  the lot is the main input to the pyramidal cells lrb type a rrb in the cortex  for awake and motivated rats lrb n  10 rrb the mean qmax value was 6 34 and the standard deviation was 1 46  the range was 4 41  9 53  for anesthetized animals lrb n  8 rrb the mean was 2 36 and the standard deviation was 0 89  the range was 1 153 62  there was a significant difference between anesthetized and awake animals  furthermore there is a significant difference between the qmax value for cortical cells and the qmaxvalue for bulbar cells lrb non  overlapping distributions rrb  discussion an important characteristic of a feedback loop is its feedback gain  there is ample evidence for the existence of feedback at all levels in the nervous system  moreover specific feedback loops between populations of neurons have been described and analyzed in the olfactory bulb and the prepyriform cortex 3  9  4  a monosynaptic negative feedback loop has been shown to exist in the ppc  between the pyramidal cells and inhibitory cells  called granule cells 3  2  6  16  time series analysis of concurrent pulse and eeg recordings agrees with this idea  the pyramidal cells are in the forward limb of the loop  they excite the granule cells  they are also mutually excitatory 2 4 16  the granule cells are in the feedback limb  they inhibit the pyramidal cells  evidence for mutual inhibition lrb granule to granule rrb in the ppc also exists 17  6  the analysis of cell firings versus eeg amplitude at selected time lags allows one to derive a function lrb the psc rrb that relates synaptic potentials to output in a neural feedback system  the first derivative of this curve gives an estimate of the forward gain at that stage of the loop  the procedure has been applied to various structures in the olfactory system 4  13  15  14  the olfactory system lends itself well to this type of analysis due to its geometry  topology and well known anatomy  examination of the experimental gain curves shows that the maximal gain is displaced to the excitatory side  this means that not only will the cells become activated by excitatory input  but their mutual interaction strength will increase  the result is an oscillatory burst of high frequency lrb 30  60 hz  rrb activity  this is the mechanism behind bursting in the olfactory eeg 4  13  in comparison with the data from the olfactory bulb one notices that there is a significant difference in the slope and the maximum of the psc  in cortex the values are substantially higher  however the vmax is similar  c gray 15 found a mean value of 2 14    0 41 for v max in the olfactory bulb of the rabbit lrb n  6 rrb  our value in the present study is 2 15    31  the difference is not statistically significant  there are important aspects of nonlinear coupling of the sigmoid type that are of interest in cortical functioning  a sigmoid interaction between groups of elements lrb  neurons  rrb is a prominent feature in many artificial neural nets  s grossberg has extensively studied the many desirable properties of sigmoids in these networks  sigmoids can be used to contrast enhance certain features in the stimulus  together with a thresholding operation a sigmoid rule can effectively quench noise  sigmoids can also provide for a built in gain control mechanism 18  19  247 changing sigmoid slopes have been investigated by j hopfield  in his network changing the slope of the sigmoid interaction between the elements affects the number of attractors that the system can go to 20  we have previously remarked upon the similarities between this and the change in sigmoid slope between waking and anesthetized animals 14  here we present a system with a steep slope lrb the ppc rrb in series with a system with a shallow slope lrb the db rrb  present investigations into similarities between the olfactory bulb and hopfield networks have been reported 21  22  similarities between the cortex and hopfieldlike networks have also been proposed 23  spatial amplitude patterns of eeg that correlate with significant odors exist in the bulb 24  a transmission of  wave packets  from the bulb to the cortex is known to occur 25  it has been shown through cofrequency and phase analysis that the bulb can drive the cortex 25  26  it thus seeems likely that spatial patterns may also exist in the cortex  a steeper sigmoid  if the analogy with neural networks is correct  would allow the cortex to further classify input patterns coming from the olfactory bulb  in this view the bulb could form an initial classifier as well as a scratch pad memory for olfactory events  the cortex could then be the second classifier  as well as the more permanent memory  these are at present speculations that may turn out to be premature  they nevertheless are important in guiding experiments as well as in modelling  theoretical studies will have to inform us of the likelihood of this kind of processing  references 1 s l bressler and w j freeman  electroencephalogr  clin  neurophysiol    19 lrb 1980 rrb   2 w j freeman  j neurophysiol  ll  1 lrb 1968 rrb  3 w j freeman  exptl  neurol   lo  525 lrb 1964 rrb  4 w j freeman  mass action in the nervous system  lrb academic press  n y  1975 rrb  chapter 3  5 l b haberly and g m shepherd  neurophys    789 lrb 1973 rrb  6 l b haberly and j m bower  j neurophysiol  ll  90 lrb 1984 rrb  7 s ramon y cajal  histologie du systeme nerveux de l homme et des vertebres  lrb ed  maloine  paris  1911 rrb  8 w j freeman  bioi  cybernetics  3  5   21 lrb 1979 rrb  9 w rall and g m shepherd  j neurophysiol ll  884 lrb 1968 rrb  10 g m shepherd  physiol  rev 5l  864 lrb 1972 rrb  11 l b haberly and j l price  j compo neurol   l18  711 lrb 1978 rrb  12 w j freeman  exptl  neurol    70 lrb 1962 rrb  13 w j freeman  bioi  cybernetics ll  237 lrb 1979 rrb  14 f h eeckman and w j freeman  alp proc  ill  135 lrb 1986 rrb  15 c m gray  ph d thesis  baylor college of medicine lrb houston 1986 rrb 16 l b haberly  chemical senses   ll   219 lrb 1985 rrb  17 m satou et ai  j neurophysiol    1157 lrb 1982 rrb  18 s grossberg  studies in applied mathematics  vol lii  3 lrb mit press  1973 rrb p 213  19 s grossberg  siam ams proc  u  107 lrb 1981 rrb  20 j j hopfield  proc  natl acad  sci  usa 8 1  3088 lrb 1984 rrb  21 w a baird  physica 2  m  150 lrb 1986 rrb  22 w a baird  alp proceedings ill  29 lrb 1986 rrb  23 m wilson and j bower  neurosci  abstr  387 10 lrb 1987 rrb  248 24 k a grajski and w j freeman  alp proc ls l  188 lrb 1986 rrb  25 s l bressler  brain res    285 lrb 1986 rrb  26 s l bressler  brain res    294 lrb 1986 rrb 
9 en 22 learning on a general network amir f atiya department of electrical engineering california institute of technology ca 91125 abstract this paper generalizes the backpropagation method to a general network containing feedback t  onnections  the network model considered consists of interconnected groups of neurons  where each group could be fully interconnected lrb it could have feedback connections  with possibly asymmetric weights rrb  but no loops between the groups are allowed  a stochastic descent algorithm is applied  under a certain inequality constraint on each intra group weight matrix which ensures for the network to possess a unique equilibrium state for every input  introduction it has been shown in the last few years that large networks of interconnected  neuron   like elemp nts are quite suitable for performing a variety of computational and pattern recognition tasks  one of the well known neural network models is the backpropagation model lsb 1 rsb  lsb 4 rsb  it is an elegant way for teaching a layered feedforward network by a set of given input output examples  neural network models having feedback connections  on the other hand  have also been devised lrb for example the hopfield network lsb 5 rsb rrb  and are shown to be quite successful in performing some computational tasks  it is important  though  to have a method for learning by examples for a feedback network  since this is a general way of design  and thus one can avoid using an ad hoc design method for each different computational task  the existence of feedback is expected to improve the computational abilities of a given network  this is because in feedback networks the state iterates until a stable state is reached  thus processing is perforrr   ed on several steps or recursions  this  in general allows more processing abilities than the  single step  feedforward case lrb note also the fact that a feedforward network is a special case of a feedback network rrb  therefore  in this work we consider the problem of developing a general learning algorithm for feedback networks  in developing a learning algorithm for feedback networks  one has to pay attention to the following lrb see fig 1 for an example of a configuration of a feedback network rrb  the state of the network evolves in time until it goes to equilibrium  or possibly other types of behavior such as periodic or chaotic motion could occur  however  we are interested in having a steady and and fixed output for every input applied to the network  therefore  we have the following two important requirements for the network  beginning in any initial condition  the state should ultimately go to equilibrium  the other requirement is that we have to have a unique  american institute of physics 1988 23 equilibrium state  it is in fact that equilibrium state that determines the final output  the objective of the learning algorithm is to adjust the parameters lrb weights rrb of the network in small steps  so as to move the unique equilibrium state in a way that will result finally in an output as close as possible to the required one lrb for each given input rrb  the existence of more than op e equilibrium state for a given input causes the following problems  in some iterations one might be updating the weights so as to move one of the equilibrium states in a sought direction  while in other iterations lrb especially with different input examples rrb a different equilibrium state is moved  another important point is that when implementing the network lrb after the completion oflearning rrb  for a fixed input there can be more than one possible output  independently  other work appeared recently on training a feedback network lsb 6 rsb  lsb 7 rsb  lsb 8 rsb  learning algorithms were developed  but solving the problem of ensuring a unique equilibrium was not considered  this problem is addressed in this paper and an appropriate network and a learning algorithm are proposed  neuron 1 outputs inputs fig  1 a recurrent network the feedback network consider a group of n neurons which could be fully inter connected lrb see fig 1 for an example rrb  the weight matrix w can be asymmetric lrb as opposed to the hopfield network rrb  the inputs are also weighted before entering into the network lrb let v be the weight matrix rrb  let x and y be the input and output vectors respectively  in our model y is governed by the following set of differential equations  proposed by hopfield lsb 5 rsb  du tdj  wf lrb u rrb  u  vx  y  f lrb u rrb lrb 1 rrb 24 where f lrb u rrb  lrb j lrb ud    f lrb un rrb f  t denotes the transpose operator  f is a bounded and differentiable function  and   is a positive constant  for a given input  we would like the network after a short transient period to give a steady and fixed output  no matter what the initial network state was  this means that beginning any initial condition  the state is to be attracted towards a unique equilibrium  this leads to looking for a condition on the matrix w theorem  a network lrb not necessarily symmetric rrb satisfying l l w  fi i  l max lrb j rrb 2  i exhibits no other behavior except going to a unique equilibrium for a given input  proof  let udt rrb and u2 lrb t rrb be two solutions of lrb 1 rrb  let where  ii is the two norm  differentiating j with respect to time  one obtains using lrb 1 rrb  the expression becomes dj lrb t rrb   llui lrb t rrb 2  d  u2 lrb t rrb rrb 11 2 t 1  2   lrb ui lrb t rrb  u2 lrb t rrb rrb t w lsb f lrb ui lrb t rrb rrb  f lrb uz lrb t rrb rrb rsb     using schwarz s inequality  we obtain again  by schwarz s inequality  i  1    n where wi denotes the ith row of w using the mean value theorem  we get ilf lrb udt rrb rrb  f lrb u2 lrb t rrb rrb ii  lrb maxl   i rrb iiul lrb t rrb  uz lrb t rrb ll  using lrb 2 rrb  lrb 3 rrb  and the expression for j lrb t rrb  we get d   t rrb  where  aj lrb t rrb lrb 4 rrb lrb 3 rrb lrb 2 rrb 25 by hypothesis of the theorem  a is strictly positive  multiplying both sides of lrb 4 rrb by exp lrb at rrb  the inequality results  from which we obtain j lrb t rrb  j lrb o rrb e  at  from that and from the fact that j is non negative  it follows that j lrb t rrb goes to zero as t    xl  therefore  any two solutions corresponding to any two initial conditions ultimately approach each other  to show that this asymptotic solution is in fact an equilibrium  one simply takes u2 lrb t rrb  ul lrb t  t rrb  where t is a constant  and applies the above argument lrb that j lrb t rrb   0 as t    xl rrb  and hence ul lrb t  t rrb   udt rrb as t    xl for any t  and this completes the proof  for example  if the function i is of the following widely used sigmoid shaped form  1 i lrb u rrb  l  e  u  then the sum of the square of the weights should be less than 16  note that for any function i  scaling does not have an effect on the overall results  we have to work in our updating scheme subject to the constraint given in the theorem  in many cases where a large network is necessary  this constraint might be too restrictive  therefore we propose a general network  which is explained in the next section  the general network we propose the following network lrb for an example refer to fig 2 rrb  the neurons are partitioned into several groups  within each group there are no restrictions on the connections and therefore the group could be fully interconnected lrb i e it could have feedback connections rrb  the groups are connected to each other  but in a way that there are no loops  the inputs to the whole network can be connected to the inputs of any of the groups lrb each input can have several connections to several groups rrb  the outputs of the whole network are taken to be the outputs lrb or part of the outputs rrb of a certain group  say group i  the constraint given in the theorem is applied on each intra group weight matrix separately  let lrb qa  s  rrb  a  1      n be the input output vector pairs of the function to be implemented  we would like to minimize the sum of the square error  given by a  l where m e   i rrb y lcb  si rcb 2  i  l and yf is the output vector of group f upon giving input qa  and m is the dimension of vector s   the learning process is performed by feeding the input examples qu sequentially to the network  each time updating the weights in an attempt to minimize the error  26 inputs j  v outputs fig 2 an example of a general network lrb each group represents a recurrent network rrb now  consider a single group l let wi be the intra group weight matrix of group l  vrl be the matrix of weights between the outputs of group   and the inputs of group l  and yl be the output vector of group i let the respective elements be w  i  v lsb    and y   furthermore  let be the number of neurons of group l assume that the time constant l is sufficiently small so as to allow the network to settle quickly to the equilibrium state  which is given by the solution of the equation n  yl  f lrb w yl  l vrlyr rrb  lrb 5 rrb r a i where a  is the set of the indices of the groups whose outputs a re connected to the inputs of group   we would like each iteration to update the weight matrices wi and vrl so as to move the equilibrium in a direction to decrease the error  we need therefore to know the change in the error produced by a small change in the weight matrices  let        and aa    denote the matrices whose lrb i  j rrb th element are      and    respectively  let  be the column vector  1  1  r whose ith element is   we obtain the following relations  uy  8e a  8w  8e a 8v tl lsb a   lrb w  rrb t rsb 1 8ea lrb 8yl y  rrb t  a lrb r rrb t  lsb a   lrb w  rrb t rsb 1 8e 8yl y  where a  is the diagonal matrix whose ith diagonal element is l f  lrb lk w ky   lrlktj lsb kyk rrb for a derivation refer to appendix rrb  the vector  associated with group l can be obtained in terms of the vectors   feb  where b  is the set of the indices of the groups whose inputs are connected to the outputs of group   we get lrb refer to appendix rrb 8e a 8yl    lrb v  i rrb t lsb ai  lrb wi lcb r 1 8e     8y3 jlbi lrb 6 rrb the matrix a   lrb w  rrb t for any group l can never be singular  so we will not face any problem in the updating process  to prove that  let z be a vector satisfying lsb a   lrb w  f rsb z  o 27 we can write zdmaxlf  i  lw   zk  i  i    nl k where zi is the   th element of z using schwarz s inequality  we obtain i  i    nl squaring both sides and adding the inequalities for i  i    nl  we get l    max lrb j rrb 2 lrb lz  rrb ll lrb w  i rrb 2  i k since the condition lrb 7 rrb k ll lrb w k rrb 2  i max lrb j rrb 2 rrb  k is enforced  it follows that lrb 7 rrb can not be satisfied unless z is the zero vector  thus  the matrix a   lrb w  rrb t can not be singular  for each iteration we begin by updating the weights of group f lrb the group contammg the final outputs rrb  for that group  equals simply 2 lrb y lcb  si    yf t  sm  0    o rrb t rrb  then we move backwards to the groups connected to that group and obtain their corresponding vectors using lrb 6 rrb  update the weights  and proceed in the same manner until we complete updating all the groups  updating the weights is performed using the following stochastic descent algorithm for each group   j  8e a t   v   a3 8v  a4 ea r  where r is a noise matrix whose elements are characterized by independent zero mean unityvariance gaussian densities  and the a s are parameters  the purpose of adding noise is to allow escaping local minima if one gets stuck in any of them  note that the control parameter is taken to be ea  hence the variance of the added noise tends to decrease the more we approach the ideal zero error solution  this makes sense because for a large error  i e for an unsatisfactory solution  it pays more to add noise to the weight matrices in order to escape local minima  on the other hand  if the error is small  then we are possibly near the global minimum or to an acceptable solution  and hence we do not want too much noise in order not to be thrown out of that basin  note that once we reach the ideal zero error solution the added noise as well as the gradient of ea become zero for all a and hence the increments of the weight matrices become zero  if after a certain iteration w happens to violate the constraint liiwlj  constant  i max lrb j rrb 2  then its elements are scaled so as to project it back onto the surface of the hypershere  implementation example a pattern recognition example is considered  fig 3 shows a set of two dimensional training patterns from three classes  it is required to design a neural network recognizer with 28 three output neurons  each of the neurons should be on if a sample of the corresponding class is presented  and off otherwise  i e we would like to design a  winner take all  network  a singlelayer three neuron feedback network is implemented  we obtained 3 3  error  performing the same experiment on a feedforward single layer network with three neurons  we obtained 20  error  for satisfactory results  a feedforward network should be two layer  with one neuron in the first layer and three in the second layer  we got 36 7  error  finally  with two neurons in the first layer and three in the second layer  we got a match with the feedback case  with 3 3  error  z z z z z z z z z z z z z z z z zil 1 33 3 3 3 1 3 3 33 3 3 3  3 3 3 3 3 3 fig 3 a pattern recognition example conclusion a way to extend the backpropagation method to feedback networks has been proposed  a condition on the weight matrix is obtained  to insure having only one fixed point  so as to prevent having more than one possible output for a fixed input  a general structure for networks is presented  in which the network consists of a number of feedback groups connected to each other in a feedforward manner  a stochastic descent rule is used to update the weights  the lj ethod is applied to a pattern recognition example  with a single layer feedback network it obtained good results  on the other hand  the feedforward backpropagation method achieved good resuls only for the case of more than one layer  hence also with a larger number of neurons than the feedback case  29 acknow ledgement the author would like to gratefully acknowledge dr  y abu mostafa for the useful discussions  this work is supported by air force office of scientific research under grant afosr 86 0296  appendix differentiating lrb 5 rrb  one obtains ai  yj a i  w kp ai k  p  1    n  i ym p jk  f  lrb  rrb lrb     zj l wjm a i  y  6 rrb w kp m where if j  k otherwise  and we can write a    lrb a   wi rrb  lbkz  lrb a  1 rrb aw kp where b kp is the nt dimensional vector whose b  l   lcb y   0 ith component is given by ifi  k otherwise  by the chain rule  aea     ae a ay   a i  l ai  a i  w kp j yj w kp which  upon substituting from lrb a  1 rrb  can be put in the form y   gk   where gk is the column of lrb a   wt rrb  l finally  we obtain the required expression  which is ae   lsb at  lrb wi rrb t rsb aw  1 ae  lrb  rrb t ayl y  regarding a lrb rrb   i  it is obtained by differentiating lrb 5 rrb with respect to vr    we get similarly where c kl  is the nt dimensional vector whose ith component is given by if i  k otherwise  kth 30 a derivation very similar to the case of   l results in the following required expression  be a  bvrl 8 8 lsb a   lrb w  rrb t rsb 1 be a lrb r rrb t by  y 8yj j now  finally consider   let   jf b  be the matrix whose lrb k  p rrb th element is   the elements of  can be obtained by differentiating the equation for the fixed point for group  uy j  as follows  hence       lrb ai  wi rrb  iv  i lrb a  2 rrb using the chain rule  one can write be a by   t   lrb byj rrb  byl jeer be a by   we substitute from lrb a  2 rrb into the previous equation to complete the derivation by obtaining references 111 p werbos   beyond regression  new tools for prediction and analysis in behavioral sciences   harvard university dissertation  1974  lsb 21 d parker   learning logic   mit tech report tr 47  center for computational research in economics and management science  1985  lsb 31 y le cun   a learning scheme for asymmetric threshold network   proceedings of cognitiva  paris  june 1985  lsb 41 d rumelhart  g hinton  and r williams   learning internal representations by error propagation   in d rumelhart  j mclelland and the pdp research group lrb eds  rrb  parallel distributed processing  explorations in the microstructure of cognition  vol  1  mit press  cambridge  ma  1986  151 j hopfield   neurons with graded response have collective computational properties like those of two state neurons   proc  n atl  acad  sci  usa  may 1984  lsb 61 l ahneida   a learning rule for asynchronous perceptrons with feedback in a combinatorial environment   proc  of the first int  annual conf  on neural networks  san diego  june 1987  lsb 71 r rohwer  and b forrest   training time dependence in neural networks   proc  of the first int  annual conf  on neural networks  san diego  june 1987  lsb 81 f pineda   generalization of back propagation to recurrent neural networks   phys  rev lett  vol  59  no 19  9 nov 1987 
10 en 683 a mean field theory of layer iv of visual cortex and its application to artificial neural networks  christopher l scofield center for neural science and physics department brown university providence  rhode island 02912 and nestor  inc  1 richmond square  providence  rhode island  02906  abstract a single cell theory for the development of selectivity and ocular dominance in visual cortex has been presented previously by bienenstock  cooper and munrol  this has been extended to a network applicable to layer iv of visual cortex 2  in this paper we present a mean field approximation that captures in a fairly transparent manner the qualitative  and many of the quantitative  results of the network theory  finally  we consider the application of this theory to artificial neural networks and show that a significant reduction in architectural complexity is possible  a single layer network and the mean field approximation we consider a single layer network of ideal neurons which receive signals from outside of the layer and from cells within the layer lrb figure 1 rrb  the activity of the ith cell in the network is c  1  m  1 d      t   c   j j j lrb 1 rrb each cell d is a vector of afferent signals to the network  receives input from n fibers outside of the cortical network through the matrix of synapses mi  intra layer input to each cell is then transmitted through the matrix of cortico cortical synapses l  american institute of physics 1988 684 afferent signals     m2 m1 mn  r       d  l  1   2      c     figure 1  the general single layer recurrent network  light circles are the lgn  cortical synapses  dark circles are the lrb nonmodifiable rrb cortico cortical synapses  we now expand the response of the i th cell into individual terms describing the number of cortical synapses traversed by the signal d before arriving through synapses lij at cell i expanding cj in lrb 1 rrb  the response of cell i becomes ci  mi d  l   j mj d  l   jl ljk mk d  2   j 2ljk l lkn mn d   lrb 2 rrb j j k j k  n note that each term contains a factor of the form this factor describes the first order effect  on cell q  of the cortical transformation of the signal d  the mean field approximation consists of estimating this factor to be a constant  independant of cell location lrb 3 rrb 685 this assumption does not imply that each cell in the network is selective to the same pattern  lrb and thus that mi  mj rrb  rather  the assumption is that the vector sum is a constant this amounts to assuming that each cell in the network is surrounded by a population of cells which represent  on average  all possible pattern preferences  thus the vector sum of the afferent synaptic states describing these pattern preferences is a constant independent of location  finally  if we assume that the lateral connection strengths are a function only of i j then lij becomes a circular matrix so that r lij     j lji  lo  constan t 1 then the response of the cell i becomes lrb 4 rrb for i  i  1 where we define the spatial average of cortical cell activity c  in d  and n is the average number of intracortical synapses  here  in a manner similar to that in the theory of magnetism  we have replaced the effect of individual cortical cells by their average effect lrb as though all other cortical cells can be replaced by an  effective  cell  figure 2 rrb  note that we have retained all orders of synaptic traversal of the signal d thus  we now focus on the activity of the layer after  relaxation  to equilibrium  in the mean field approximation we can therefore write lrb 5 rrb where the mean field a with  am 686 and we asume that inhibitory rrb  afferent signals d lo  0 lrb the network is  on average   figure 2  the single layer mean field network  detailed connectivity between all cells of the network is replaced with a single lrb nonmodifiable rrb synapse from an  effective  cell  learning in the cortical network we will first consider evolution of the network according to a synaptic modification rule that has been studied in detail  for single cells  elsewhere  3  we consider the lgn  cortical synapses to be the site of plasticity and assume for maximum simplicity that there is no modification of cortico cortical synapses  then lrb 6 rrb  lij  o  in what follows c denotes the spatial average over cortical cells  while cj denotes the time averaged activity of the i th cortical cell  the function cj  has been discussed extensively elsewhere  here we note that cj  describes a function of the cell response that has both hebbian and anti hebbian regions  687 this leads to a very complex set of non linear stochastic equations that have been analyzed partially elsewhere 2  in general  the afferent synaptic state has fixed points that are stable and selective and unstable fixed points that are nonselective   2  these arguments may now be generalized for the network  in the mean field approximation lrb 7 rrb the mean field  a has a time dependent component m  this varies as the average over all of the network modifiable synapses and  in most environmental situations  should change slowly compared to the change of the modifiable synapses to a single cell  then in this approximation we can write  lrb mi lrb a rrb  a rrb  cj  lsb mi lrb a rrb  a rsb d lrb 8 rrb we see that there is a mapping mi     mica rrb  a lrb 9 rrb such that for every mj lrb a rrb there exists a corresponding lrb mapped rrb point mj  which satisfies the original equation for the mean field zero theory  it can be shown 2  4 that for every fixed point of mj lrb a  0 rrb  there exists a corresponding fixed point mj lrb a rrb with the same selectivity and stability properties  the fixed points are available to the neurons if there is sufficient inhibition in the network lrb ilo i is sufficiently large rrb  application of the mean field network to layer iv of visual cortex neurons in the primary visual cortex of normal adult cats are sharply tuned for the orientation of an elongated slit of light and most are activated by stimulation of either eye  both of these properties  orientation selectivity and binocularity  depend on the type of visual environment experienced during a critical 688 period of early postnatal development  for example  deprivation of patterned input during this critical period leads to loss of orientation selectivity while monocular deprivation lrb md rrb results in a dramatic shift in the ocular dominance of cortical neurons such that most will be responsive exclusively to the open eye  the ocular dominance shift after md is the best known and most intensively studied type of visual cortical plasticity  the behavior of visual cortical cells in various rearing conditions suggests that some cells respond more rapidly to environmental changes than others  in monocular deprivation  for example  some cells remain responsive to the closed eye in spite of the very large shift of most cells to the open eye  singer et  al 5 found  using intracellular recording  that geniculo cortical synapses on inhibitory interneurons are more resistant to monocular deprivation than are synapses on pyramidal cell dendrites  recent work suggests that the density of inhibitory gabaergic synapses in kitten striate cortex is also unaffected by md during the cortical period 6  7  these results suggest that some lgn  cortical synapses modify rapidly  while others modify relatively slowly  with slow modification of some cortico cortical synapses  excitatory lgncortical synapses into excitatory cells may be those that modify primarily  to embody these facts we introduce two types of lgn  cortical synapses  those lrb mj rrb that modify and those lrb zk rrb that remain relatively constant  in a simple limit we have and lrb 10 rrb we assume for simplicity and consistent with the above physiological interpretation that these two types of synapses are confined to two different classes of cells and that both left and right eye have similar synapses lrb both m i or both zk rrb on a given cell  then  for binocular cells  in the mean field approximation lrb where binocular terms are in italics rrb 689 where dl lrb r rrb are the explicit left lrb right rrb eye time averaged signals arriving form the lgn  note that a1 lrb r rrb contain terms from modifiable and non modifiable synapses  al lrb r rrb  a lrb ml lrb r rrb  zl lrb r  rrb  under conditions of monocular deprivation  the animal is reared with one eye closed  for the sake of analysis assume that the right eye is closed and that only noise like signals arrive at cortex from the right eye  then the environment of the cortical cells is  d  lrb di  n rrb lrb 12 rrb further  assume that the left eye synapses have reached their 1 r selective fixed point  selective to pattern d 1  then lrb mi  m i rrb lrb m    xi rrb with ixil  lm   1  linear analysis of the the closed eye i   following the methods of bcm  a local function is employed to show that for xi  a lrb 1  rcb   a rrb  li r  lrb 13 rrb where a  nmin is the ratio of the number modifiable cells to the total number of cells in the network  that is  the asymptotic state of the closed eye synapses is a scaled function of the meanfield due to non modifiable lrb inhibitory rrb cortical cells  the scale of this state is set not only by the proportion of non modifiable cells  but in addition  by the averaged intracortical synaptic strength lo  thus contrasted with the mean field zero theory the deprived eye lgn cortical synapses do not go to zero  rather they approach the constant value dependent on the average inhibition produced by the non modifiable cells in such a way that the asymptotic output of the cortical cell is zero lrb it can not be driven by the deprived eye rrb  however lessening the effect of inhibitory synapses lrb e g by application of an inhibitory blocking agent such as bicuculine rrb reduces the magnitude of a so that one could once more obtain a response from the deprived eye  690 we find  consistent with previous theory and experiment  that most learning can occur in the lgn cortical synapse  for inhibitory lrb cortico cortical rrb synapses need not modify  some non modifiable lgn cortical synapses are required  the mean field approximation and artificial neural networks the mean field approximation may be applied to networks in which the cortico cortical feedback is a general function of cell activity  in particular  the feedback may measure the difference between the network activity and memories of network activity  in this way  a network may be used as a content addressable memory  we have been discussing the properties of a mean field network after equilibrium has been reached  we now focus on the detailed time dependence of the relaxation of the cell activity to a state of equilibrium  hopfield8 introduced a simple formalism for the analysis of the time dependence of network activity  in this model  network activity is mapped onto a physical system in which the state of neuron activity is considered as a  particle  on a potential energy surface  identification of the pattern occurs when the activity  relaxes  to a nearby minima of the energy  thus mlmma are employed as the sites of memories  for a hopfield network of n neurons  the intra layer connectivity required is of order n2  this connectivity is a significant constraint on the practical implementation of such systems for large scale problems  further  the hopfield model allows a storage capacity which is limited to m  n memories 8  9  this is a result of the proliferation of unwanted local minima in the  energy  surface  recently  bachmann et al l 0  have proposed a model for the relaxation of network activity in which memories of activity patterns are the sites of negative  charges   and the activity caused by a test pattern is a positive test  charge   then in this model  the energy function is the electrostatic energy of the lrb unit rrb test charge with the collection of charges at the memory sites e   ill  qj i j l  xj i  l  j lrb 14 rrb 691 where jl lrb 0 rrb is a vector describing the initial network activity caused by a test pattern  and xj  the site of the jth memory  l is a parameter related to the network size  this model has the advantage that storage density is not restricted by the the network size as it is in the hopfield model  and in addition  the architecture employs a connectivity of order m x n note that at each stage in the settling of jl lrb t rrb to a memory lrb of network activity rrb xj  the only feedback from the network to each cell is the scalar  j q i jl  x  i  l j j lrb 15 rrb this quantity is an integrated measure of the distance of the current network state from stored memories  importantly  this measure is the same for all cells  it is as if a single virtual cell was computing the distance in activity space between the current state and stored states  the result of the computation is this is a then broadcast to all of the cells in the network  generalization of the idea that the detailed activity of each cell in the network need not be fed back to each cell  rather some global measure  performed by a single  effective  cell is all that is sufficient in the feedback  discussion we have been discussing a formalism for the analysis of networks of ideal neurons based on a mean field approximation of the detailed activity of the cells in the network  we find that a simple assumption concerning the spatial distribution of the pattern preferences of the cells allows a great simplification of the analysis  in particular  the detailed activity of the cells of the network may be replaced with a mean field that in effect is computed by a single  effective  cell  further  the application of this formalism to the cortical layer iv of visual cortex allows the prediction that much of learning in cortex may be localized to the lgn cortical synaptic states  and that cortico cortical plasticity is relatively unimportant  we find  in agreement with experiment  that monocular deprivation of the cortical cells will drive closed eye responses to zero  but chemical blockage of the cortical inhibitory pathways would reveal non zero closed eye synaptic states  692 finally  the mean field approximation allows the development of single layer models of memory storage that are unrestricted in storage density  but require a connectivity of order mxn  this is significant for the fabrication of practical content addressable memories  acknowleooements i would like to thank leon cooper for many helpful discussions and the contributions he made to this work   this work was supported by the office of naval research and the army research office under contracts noooi4 86  k 0041 and daag 29 84  k 0202  references lsb 1 rsb bienenstock  e l  cooper  l n  munro  p w lrb 1982 rrb 1  neuroscience 2  32 48  lsb 2 rsb scofield  c l lrb i984 rrb unpublished dissertation  lsb 3 rsb cooper  l n  munro  p w  scofield  c l lrb 1985 rrb in synaptic modification  neuron selectivity and nervous system organization  ed  c levy  j a anderson  s lehmkuhle  lrb erlbaum assoc  n j rrb  lsb 4 rsb cooper  l n  scofield  c l lrb to be published rrb proc  natl acad  sci  usa   lsb 5 rsb singer  w lrb 1977 rrb brain res  134  508 000  lsb 6 rsb bear  m f  schmechel d m   ebner  f f lrb 1985 rrb 1  neurosci  5  1262 0000  lsb 7 rsb mower  g d  white  w f   rustad  r lrb 1986 rrb brain res  380  253 000  lsb 8 rsb hopfield  j j lrb 1982 rrb proc  natl a cad  sci  usa 79  2554 2558  lsb 9 rsb hopfield  j j  feinstein  d 1   palmer  r o lrb 1983 rrb nature 304  158 159  lsb 10 rsb bachmann  c m  cooper  l n  dembo  a  zeitouni  o lrb to be published rrb proc  natl acad  sci  usa 
11 en 515 microelectronic implementations of connectionist neural networks stuart mackie  hans p graf  daniel b schwartz  and john s denker at t bell labs  holmdel  nj 07733 abstract in this paper we discuss why special purpose chips are needed for useful implementations of connectionist neural networks in such applications as pattern recognition and classification  three chip designs are described  a hybrid digital analog programmable connection matrix  an analog connection matrix with adjustable connection strengths  and a digital pipe lined best match chip  the common feature of the designs is the distribution of arithmetic processing power amongst the data storage to minimize data movement  rams     distributed      co mputati on chips  0    q rrb q rrb  c  c e   iiit                   s    zo    conventional cpus     1 1 10 3 10 6 10 9 node complexity lrb no  of transistors rrb figure 1  a schematic graph of addressable node complexity and size for conventional computer chips  memories can contain millions of very simple nodes each with a very few transistors but with no processing power  cpu chips are essentially one very complex node  neural network chips are in the distributed computation region where chips contain many simple fixed instruction processors local to data storage  lrb after reece and treleaven 1 rrb  american institute of physics 1988 516 introduction it is clear that conventional computers lag far behind organic computers when it comes to dealing with very large data rates in problems such as computer vision and speech recognition  why is this  the reason is that the brain performs a huge number of operations in parallel whereas in a conventional computer there is a very fast processor that can perform a variety of instructions very quickly  but operates on only two pieces of data at a time  the rest of the many megabytes of ram is idle during any instruction cycle  the duty cycle of the processor is close to 100   but that of the stored data is very close to zero  if we wish to make better use of the data  we have to distribute processing power amongst the stored data  in a similar fashion to the brain  figure 1 illustrates where distributed computation chips lie in comparison to conventional computer chips as regard number and complexity of addressable nodes per chip  in order for a distributed strategy to work  each processing element must be small in order to accommodate many on a chip  and communication must be local and hardwired  whereas the processing element in a conventional computer may be able to execute many hundred different operations  in our scheme the processor is hard wired to perform just one  this operation should be tailored to some particular application  in neural network and pattern recognition algorithms  the dot products of an input vector with a series of stored vectors lrb referred to as features or memories rrb is often required  the general calculation is  sum of products v  f lrb i rrb  l v j f  ij j where v is the input vector and f lrb i rrb is one of the stored feature vectors  two variations of this are of particular interest  in feature extraction  we wish to find all the features for which the dot product with the input vector is greater than some threshold t  in which case we say that such features are present in the input vector  feature extraction v  f lrb i rrb  l v j f  ij j in pattern classification we wish to find the stored vector that has the largest dot product with the input vector  and we say that the the input is a member of the class represented by that feature  or simply that that stored vector is closest to input vector  classification max lrb v f lrb i rrb  lv  f   j ij j the chips described here are each designed to perform one or more of the above functions with an input vector and a number of feature vectors in parallel  the overall strategy may be summed up as follows  we recognize that in typical pattern recognition applications  the feature vectors need to be changed infrequently compared to the input 517 vectors  and the calculation that is perfonned is fixed and low precision  we therefore distribute simple fixed instruction processors throughout the data storage area  thus minimizing the data movement and optimizing the use of silicon  our ideal is to have every transistor on the chip doing something useful during every instruction cycle  analog sum or products u sing an idea slightly reminiscent of synapses and neurons from the brain  in two of the chips we store elements of features as connections from input wires on which the elements of the input vectors appear as voltages to summing wires where a sum ofproducts is perfonned  the voltage resulting from the current summing is applied to the input of an amplifier whose output is then read to determine the result of the calculation  a schematic arrangement is shown in figure 2 with the vertical inputs connected to the horizontal summing wires through resistors chosen such that the conductance is proportional to the magnitude of the feature element  when both positive and negative values are required  inverted input lines are also necessary  resistor matrices have been fabricated using amorphous silicon connections and metal linewidths  these were programmed during fabrication by electron beam lithography to store names using the distributed feedback method described by hopfield2 3  this work is described more fully elsewhere  4 5 hard wired resistor matrices are very compact  but also very inflexible  in many applications it is desirable to be able to reprogram the matrix without having to fabricate a new chip  for this reason  a series of programmable chips has been designed  input lines feature 4  t  ti  4t  t  f    1 feature 3    4ii  i        4  oc     c c  feature 2           4  1   i lrb i rrb feature 1 figure 2  a schematic arrangement for calculating parallel sum of products with a resistor matrix  features are stored as connections along summing wires and the input elements are applied as voltages on the input wires  the voltage generated by the current summing is thresholded by the amplifer whose output is read out at the end of the calculation  feedback connections may be 518 made to give mutual inhibition and allow only one feature amplifier to tum on  or allow the matrix to be used as a distributed feedback memory  programmable connection matrix figure 3 is a schematic diagram of a programmable connection using the contents of two ram cells to control current sinking or sourcing into the summing wire  the switches are pass transistors and the  resistors  are transistors with gates connected to their drains  current is sourced or sunk if the appropriate ram cell contains a  1  and the input vi is high thus closing both switches in the path  feature elements can therefore take on values lrb a o b rrb where the values of a and b are determined by the conductivities of the n  and p transistors obtained during processing  a matrix with 2916 such connections allowing full interconnection of the inputs and outputs of 54 amplifiers was designed and fabricated in 2 5 jlm cmos lrb figure 4 rrb  each connection is about 100x100jlm  the chip is 7x7mm and contains about 75 000 transistors  when loaded with 49 49 bit features lrb 7x7 kernel rrb  and presented with a 49 bit input vector  the chip performs 49 dot products in parallel in under 1jls  this is equivalent to 2 4 billion bit operations sec  the flexibility of the design allows the chip to be operated in several modes  the chip was programmed as a distributed feedback memory lrb associative memory rrb  but this did not work well because the current sinking capability of the n type transistors was 6 times that of the p types  an associative memory was implemented by using a  grandmother cell  representation  where the memories were stored along the input lines of amplifiers  as for feature extraction  but mutually inhibitory connections were also made that allowed only one output to tum on  with 10 stored vectors each 40 bits long  the best match was found in 50 600ns  depending on the data  the circuit can also be programmed to recognize sequences of vectors and to do error correction when vectors were omitted or wrong vectors were inserted into the sequences  the details of operation of the chip are described more fully elsewhere 6  this chip has been interfaced to a unix minicomputer and is in everyday use as an accelerator for feature extraction in optical character recognition of handwritten numerals  the chip speeds up this time consuming calculation by a factor of more than 1000  the use of the chip enables experiments to be done which would be too time consuming to simulate  experience with this device has led to the design of four new chips  which are currently being tested  these have no feedback capability and are intended exclusively for feature extraction  the designs each incorporate new features which are being tested separately  but all are based on a connection matrix which stores 46 vectors each 96 bits long  the chip will perform a full parallel calculation in loons  519 vdd   output lrb  rrb  vj  excitatory inhibitory v  j ivss figure 3  schematic diagram of a programmable connection  a current sourcing or sinking connection is made if a ram cell contains a  1  and the input vi is high  the currents are summed on the input wire of the amplifier   1 pads  row decoders r   3 connections itii1 amplifie rs figure 4  programmable connection matrix chip  the chip contains 75 000 transistors in 7x7mm  and was fabricated using 2 5 jlm design rules  520 adaptive connection matrix many problems require analog depth in the connection strengths  and this is especially important if the chip is to be used for learning  where small adjustments are required during training  typical approaches which use transistors sized in powers of two to give conductance variability take up an area equivalent to the same number of minimum sized transistors as the dynamic range  which is expensive in area and enables only a few connections to be put on a chip  we have designed a fully analog connection based on a dram structure that can be fabricated using conventional cmos technology  a schematic of a connection and a connection matrix is shown in figure 5  the connection strength is represented by the difference in voltages stored on two mos capacitors  the capacitors are 33jlm on edge and lose about 1  of their charge in five minutes at room temperature  the leakage rate can be reduced by three orders of magnitude by cooling the the capacitors to 50  c and by five orders of magnitude by cooling to 100  c  the output is a current proportional to the product of the input voltage and the connection strength  the output currents are summed on a wire and are sent off chip to external amplifiers  the connection strengths can be adjusted using transferring charge between the capacitors through a chain of transistors  the connections strengths may be of either polarity and it is expected that the connections will have about 7 bits of analog depth  a chip has been designed in 1 25 jlm cmos containing 1104 connections in an array with 46 inputs and 24 outputs  input weight update and decay by shifting charge 1   l 1    1    02  or  4111        1   r             input w lrb l lrb 01 02 rrb output  w  lnput     output through external amplifiers figure 5  analog connection  the connection strength is represented by the difference in voltages stored on two capacitors  the output is a current proprtional to the product of the input voltage and the connection strength  each connection is 70x240jlm  the design has been sent to foundry  and testing is expected to start in april 1988  the chip has been designed to perform a network calculation in  30ns  i e  the chip will perform at a rate of 33 billion multiplies sec  it can be used simply as a fast analog convolver for feature extraction  or as a learning 521 engine in a gradient descent algorithm using external logic for connection strength adjustment  because the inputs and outputs are true analog  larger networks may be formed by tiling chips  and layered networks may be made by cascading through amplifiers acting as hidden units  digital classifier chip the third design is a digital implementation of a classifier whose architecture is not a connectionist matrix  it is nearing completion of the design stage  and will be fabricated using 1 25 jlm cmos  it calculates the largest five v p lrb i rrb using an alldigital pipeline of identical processors  each attached to one stored word  each processor is also internally pipelined to the extent that no stage contains more than two gate delays  this is important  since the throughput of the processor is limited by the speed of the slowest stage  each processor calculates the hamming distance lrb number of difference bits rrb between an input word and its stored word  and then compares that distance with each of the smallest 5 values previously found for that input word  an updated list of 5 best matches is then passed to the next processor in the pipeline  at the end of the pipeline the best 5 matches overall are output  lrb 1 rrb features stored in data pipeline ring shift register  it   best match list pipeline tag register    it lcb lcb    ii              f   jr rcb   it  r      mifl   t  t if      t   rcb  lcb rcb rcb lsb   i   ii           i   i       i        lcb       i       i lsb hi tm    l           t   lsb 1  pf  lrb 2 rrb input and feature lrb 3 rrb accumulator are compared dumps distance bit serially into comparison register at end of input word pig  6  lrb 4 rrb comparator inserts new match and tag into list when better than old match schematic of one of the 50 processors in the digital classifier chip  the hamming distance of the input vector to the feature vector is calculated  and if better than one of the five best matches found so far  is inserted into the match list together with the tag and passed onto the next processor  at the end of the pipeline the best five matches overall are output 522 the data paths on chip are one bit wide and all calculations are bit serial  this means that the processing elements and the data paths are compact and maximizes the number of stored words per chip  the layout of a single processor is shown in fig 6  the features are stored as 128 bit words in 8 16 bit ring shift registers and associated with each feature is a 14 bit tag or name string that is stored in a static register  the input vector passes through the chip and is compared bit by bit to each stored vector  whose shift registers are cycled in tum  the total number of bits difference is summed in an accumulator  after a vector has passed through a processor  the total hamming distance is loaded into the comparison register together with the tag  at this time  the match list for the input vector arrives at the comparator  it is an ordered list of the 5 lowest hamming distances found in the pipeline so far  together with associated tag strings  the distance just calculated is compared bit serially with each of the values in the list in turn  if the current distance is smaller than one of the ones in the list  the output streams of the comparator are switched  having the effect of inserting the current match and tag into the list and deleting the previous fifth best match  after the last processor in the pipeline  the list stream contains the best five distances overall  together with the tags of the stored vectors that generated them  the data stream and the list stream are loaded into 16 bit wide registers ready for output  the design enables chips to be connected together to extend the pipeline if more than 50 stored vectors are required  the throughput is constant  irrespective of the number of chips connected together  only the latency increases as the number of chips increases  the chip has been designed to operate with an on chip clock frequency of at least l00mhz  this high speed is possible because stage sizes are very small and data paths have been kept short  the computational efficiency is not as high as in the analog chips because each processor only deals with one bit of stored data at a time  however  the overall throughput is high because of the high clock speed  assuming a clock frequency of l00mhz  the chip will produce a list of 5 best distances with tag strings every 1 3 jls  with a latency of about 2 5 jls  even if a thousand chips containing 50 000 stored vectors were pipelined together  the latency would be 2 5 ms  low enough for most real time applications  the chip is expected to perform 5 billion bit operation sec  while it is important to have high clock frequencies on the chip  it is also important to have them much lower off the chip  since frequencies above 50mhz are hard to deal on circuit boards  the 16 bit wide communication paths onto and off the chip ensure that this is not a problem here  conclusion the two approaches discussed here  analog and digital  represent opposites in computational approach  in one  a single global computation is performed for each match  in the other many local calculations are done  both the approaches have their advantages and it remains to be seen which type of circuit will be more efficient in applications  and how closely an electronic implementation of a neural network should resemble the highly interconnected nature of a biological network  these designs represent some of the first distributed computation chips  they are characterized by having simple processors distributed amongst data storage  the operation performed by the processor is tailored to the application  it is interesting to note some of the reasons why these designs can now be made  minimum linewidths on 523 circuits are now small enough that enough processors can be put on one chip to make these designs of a useful size  sophisticated design tools are now available that enable a single person to design and simulate a complete circuit in a matter of months  and fabrication costs are low enough that highly speculative circuits can be made without requiring future volume production to offset prototype costs  we expect a flurry of similar designs in the coming years  with circuits becoming more and more optimized for particular applications  however  it should be noted that the impressive speed gain achieved by putting an algorithm into custom silicon can only be done once  further gains in speed will be closely tied to mainstream technological advances in such areas as transistor size reduction and wafer scale integration  it remains to be seen what influence these kinds of custom circuits will have in useful technology since at present their functions can not even be simulated in reasonable time  what can be achieved with these circuits is very limited when compared with a three dimensional  highly complex biological system  but is a vast improvement over conventional computer architectures  the authors gratefully acknowledge the contributions made by l d jackel  and r e howard references 1 m reece and p c treleaven   parallel architectures for neural computers   neural computers  r eckmiller and c v d malsburg  eds lrb springer verlag  heidelberg  1988 rrb 2 j i hopfield  proc  nat  acad  sci  79 2554 lrb 1982 rrb  3 j s denker  physica 22d  216 lrb 1986 rrb  4 r e howard  d b schwartz  j s denker  r w epworth  h p graf  w  e hubbard  l d jackel  b l straughn  and d m tennant  ieee trans  electron devices ed 34  1553  lrb 1987 rrb 5 h p oraf and p devegvar   a cmos implementation of a neural network model   in  advanced research in vlsi   proceedings of the 1987 stanford conference  p losleben lrb ed  rrb  lrb mit press 1987 rrb  6 h p oraf and p devegvar   a cmos associative memory chip based on neural networks   tech  digest  1987 ieee international solid state circuits conference 
12 en 783 using neural networks to improve cochlear implant speech perception manoel f tenorio school of electrical engineering purdue university west lafayette  in 47907 abstract  an increasing number of profoundly deaf patients suffering from sensorineural deafness are using cochlear implants as prostheses  mter the implant  sound can be detected through the electrical stimulation of the remaining peripheral auditory nervous system  although great progress has been achieved in this area  no useful speech recognition has been attained with either single or multiple channel cochlear implants  coding evidence suggests that it is necessary for any implant which would effectively couple with the natural speech perception system to simulate the temporal dispersion and other phenomena found in the natural receptors  and currently not implemented in any cochlear implants  to this end  it is presented here a computational model using artificial neural networks lrb ann rrb to incorporate the natural phenomena in the artificial cochlear  the ann model presents a series of advantages to the implementation of such systems  first  the hardware requirements  with constraints on power  size  and processing speeds  can be taken into account together with the development of the underlining software  before the actual neural structures are totally defined  second  the ann model  since it is an abstraction of natural neurons  carries the necessary ingredients and is a close mapping for implementing the necessary functions  third  some of the processing  like sorting and majority functions  could be implemented more efficiently  requiring only local decisions  fourth  the ann model allows function modifications through parametric modification lrb no software recoding rrb  which permits a variety of fine tuning experiments  with the opinion of the patients  to be conceived  some of those will permit the user some freedom in system modification at real time  allowing finer and more subjective adjustments to fit differences on the condition and operation of individual s remaining peripheral auditory system  1  introduction the study of the model of sensory receptors can be carried out either via trying to understand how the natural receptors process incoming signals and build a representation code  or via the construction of artificial replacements  in the second case  we are interested in to what extent those artificial counterparts have the ability to replace the natural receptors  several groups are now carrying out the design of artificial sensors  artificial cochleas seem to have a number of different designs and a tradition of experiments  these make them now available for widespread use as prostheses for patients who have sensorineural deafness caused by hair cell damage   american institute of physics 1988 784 although surgery is required for such implants  their performance has reached a level of maturity to induce patients to seek out these devices voluntarily  unfortunately  only partial acoustic information is obtained by severely deaf patients with cochlear prosthesis  useful patterns for speech communication are not yet  fully recognizable through auditory prostheses  this problem with artificial receptors is true for both single implants  that stimulate large sections of the cochlea with signals that cover a large portion of the spectrum lsb 4 5 rsb  and multi channel implants  that stimulate specific regions of the cochlea with specific portions of the auditory spectrum lsb 3 13 rsb  in this paper  we tackle the problem of artificial cochlear implants through the used of neurocomputing tools  the receptor model used here was developed by gerald wasserman of the sensory coding laboratory  department of psychological sciences  purdue university lsb 20 rsb  and the implants were performed by richard miyamoto of the department of otolaryngology  indiana university medical school lsb 11 rsb  the idea is to introduce with the cochlear implant  the computation that would be performed otherwise by the natural receptors  it would therefore be possible to experimentally manipulate the properties of the implant and measure the effect of coding variations on behavior  the model was constrained to be portable  simple to implant  fast enough computationally for on line use  and built with a flexible paradigm  which would allow for modification of the different parts of the model  without having to reconstruct it entirely  in the next section  we review parts of the receptor model  and discuss the block diagram of the implant  section 3 covers the limitations associated with the technique  and discusses the r e sults obtained with a single neuron and one feedback loop  section 4 discusses the implementations of these models using feedforward neural networks  and the computational advantages for doing so  2  cochlear implants and the neuron model although patients can not reliably recognize randomly chosen spoken words to them lrb when implanted with either multichannel or single channel devices rrb  this is not to say that no information is extracted from speech  if the vocabulary is reduced to a limited set of words  patients perform significantly better than chance  at associating the word with a member of the set  for these types of experiments  single channel implants correspond to reported performance of 14  to 20  better than chance  with 62  performance being the highest reported  for multiple channels  performances of 95  were reported  so far no one has investigated the differences in performance between the two types of implants  since the two implants have so many differences  it is difficult to point out the cause for the better performance in the multiple channel case  the results of such experiments are encouraging  and point to the fact that cochlea implants need only minor improvement to be able to mediate ad lib speech perception successfully  sensory coding studies have suggested a solution to the implant problem  by showing that the representation code generated by the sensory system is task dependent  this evidence came from comparison of intracellular recordings taken from a single receptor of intact subjects  this coding evidence suggests that the temporal dispersion lrb time integration rrb found in natural receptors would be a necessary part of any 785 cochlear implant  present cochlear implants have no dispersion at all  figure 2 shows the block diagram for a representative cochlear implant  the house urban stimulator  the acoustic signal is picked up by the microphone  which sends it to an am oscillator  this modulation step is necessary to induce an electro magnetic coupling between the external and internal coil  the internal coil has been surgically implanted  and it is connected to a pair of wires implanted inside and outside the cochlea  just incorporating the temporal dispersion model to an existing device would not replicate the fact that in natural receptors  temporal dispersion appears in conjunction to other operations which are strongly non linear  there are operations like selection of a portion of the spectrum  rectification  compression  and time dispersion to be considered  in figure 3  a modified implant is shown  which takes into consideration some of these operations  it is depicted as a single channel implant  although the ultimate goal is to make it multichannel  details of the operation of this device can be found elsewhere lsb 21 rsb  here  it is important to mention that the implant would also have a compression rectification function  and it would receive a feedback from the integrator stage in order to control its gain  3  characteristics and results of the implants the above model has been implemented as an off line process  and then the patients were exposed to a preprocessed signal which emulated the operation of the device  it is not easy to define the amount of feedback needed in the system or the amount of time dispersion  it could also be that these parameters are variable across different conditions  another variance in the experiment is the amount of damage lrb and type rrb among different individuals  so  these parameters have to be determined clinically  the coupling between the artificial receptor and the natural system also presents problems  if a physical connection is used  it increases the risk of infections  when inductive methods are used  the coupling is never ideal  if portability and limited power is of concern in the implementation  then the limited energy available for coupling has to be used very effectively  the computation of the receptor model has to be made in a way to allow for fast implementation  the signal transformation is to be computed on line  also  the results from clinical studies should be able to be incorpora ted fairly easily without having to reengineer the implant  now we present the results of the implementation of the transfer function of figure 4  patients  drawn from a population described elsewhere lsb 11 12 14 rsb  were given spoken sentences processed off line  and simultaneously presented with a couple of words related to the context  only one of them was the correct answer  the patient had two buttons  one for each alternative  he she was to press the button which corresponded to the correct alternative  the results are shown in the tables below  patient 1 lrb average of the population rrb percentage of correct alternatives dispersion no disp  0 1 msec 0 3 msec 67  78  85  best performance 786 1 msec 3 msec 76  72  table i  phoneme discrimination in d two alternate task  patient 2  d  lsperslon no disp  1 0 msec percentage of correct alternatives 50  76  best performance table ii  sentence comprehension in a two alternative task  there were quite a lot of variations in the performance of the different patients  some been able to perform better at different dispersion and compression amounts than the average of the population  since one can not control the amount of damage in the system of each patient or differences in individuals  it is hard to predict the ideal values for a given patient  nevertheless  the improvements observed are of undeniable value in improving speech perception  4  the neurocomputing model in studying the implementation of such a system for on line use  yet flexible enough to produce a carry on device  we look at feedforward neurocomputer models as a possible answer  first  we wanted a model that easily produced a parallel implementation  so that the model could be expanded in a multichannel environment without compromising the speed of the system  figure 5 shows the initial idea for the implementation of the device as a single instruction multiple data lrb simd rrb architecture  the implant would be similar to the one described in figure 4  except that the transfer function of the receptor would be performed by a two layer feed forward network lrb figure 6 rrb  since there is no way of finding out the values of compression and dispersion a part from clinical trials  or even if these values do change in certain conditions  we need to create a structure that is flexible enough to modify the program structure by simple manipulation of parameters  this is also the same problem we would face when trying to expand the system to a multichannel implant  again  neuromorphic models provided a nice paradigm in which the dataflow and the function of the program could be altered by simple parameter lrb weight rrb change  for this first implementation we chose to use the no contact inductive coupling method  the drawback of this method is that all the information has to be compressed in a single channel for reliable transmission and cross talk elimination  since the inductive coupling of the implant  is critical at every cycle  the most relevant information must be picked out of the processed signal  this information is then given all the available energy  and after all the coupling loss  it should be sufficient to provide for speech pattern discrimination  in a multichannel setting  this corresponds to doing a sorting of all the n signals in the channels  selecting the m highest signals  and adding them up for modulation  in a naive single processor implementation  this could correspond to n 2 comparisons  and in a multiprocessor implementation  log lrb n rrb comparisons  both are dependent on the number of signals to be 787 sorted  we needed a scheme in which the sorting time would be constant with the number of channels  and would be easily implementable in analog circuitry  in case this became a future route  our scheme is shown in figure 7  each channel is connected to a threshold element  whose threshold can be varied externally  a monotonically decreasing function scans the threshold values  from the highest possible value of the output to the lowest  the output of these elements will be high corresponding to the values that are the highest first  these output are summed with a quasi integrator with threshold set to m  this element  when high  disables the scanning functions  and it corresponds to having found the m highest signals  this sorting is independent of the number of channels  the output of the threshold units are fed into sigma pi units which gates the signals to be modulated  the output of these units are summed and correspond to the final processed signal lrb figure 8 rrb  the user has full control of the characteristics of this device  the number of channels can be easily altered  the number of components allowed in the modulation can be changed  the amount of gain  rectificationcompression  and dispersion of each channel can also be individually controlled  the entire system is easily implementable in analog integrated circuits  once the clinical tests have determine the optimum operational characteristics  6  conclusion we have shown that the study of sensory implants can enhance our understanding of the representation schemes used for natural sensory receptors  in particular  implants can be enhanced significantly if the effects of the sensory processing and transfer functions are incorporated in the model  we have also shown that neuromorphic computing paradigm provides a parallel and easily modifiable framework for signal processing structures  with advantages that perhaps can not be offered by other technology  we will soon start the use of the first on line portable model  using a single processor  this model will provide a testbed for more extensive clinical trials of the implant  we will then move to the parallel implementation  and from there  possibly move toward analog circuitry implementation  another route for the use of neuromorphic computing in this domain is possibly the use of sensory recordings from healthy animals to train selforganizing adaptive learning networks  in order to design the implant transfer functions  references lsb 1 rsb lsb 2 rsb bilger  r c  black  f o  hopkinson  n t  and myers  e n   implanted auditory prosthesis  an evaluation of subjects presently fitted with cochlear implants   otolaryngology  1977  vol  84  pp 677682  bilger  r c  black  f o  hopkinson  n t    ers  e    payne    l  stenson  n r  vega  a  and wolf  r v  evaluation of subjects presently fitted with implanted auditory prostheses   annals of otology  rhinology  and laryngology  1977  vol  86 lrb supp  38 rrb  pp 1 176  788 lsb 3 rsb lsb 4 rsb lsb 5 rsb lsb 6 rsb eddington  d k  dobelle  w h  brackmann  d e  mladejovsky  m g  and parkin  j   place and periodicity pitch by stimulation of multiple scala tympani electrodes in deaf volunteers   american society for artificial internal organs  transactions  1978  vol  24  pp 1 5  house  w f  berliner   k  crary  w  graham  m  luckey  r   norton  n  selters  w  tobm  h  urban  j  and wexler  m  cochlear implants   annals of otology  rhinology and laryngology  1976  vol  85 lrb supp  27 rrb  pp 1 93  house  w f and urban  j   long term results of electrode implantation and electronic stimulation of the cochlea in man   annals of otology  rhinology and laryngology  1973  vol  82  no 2  pp 504 517  ifukube  t and white  r l   a speech processor with lateral inhibition for an eight channel cochlear implant and its evaluation   ieee trans  on biomedical engineering  november 1987  vol  bme 34  no  11  lsb 7 rsb lsb 8 rsb lsb 9 rsb lsb 10 rsb lsb 11 rsb lsb 12 rsb lsb 13 rsb lsb 14 rsb lsb 15 rsb lsb 16 rsb kong  k l  and wasserman  g s   changing response measures alters temporal summation in the receptor and spike potentials of the limulus lateral eye   sensory processes  1978  vol  2  pp 21 31  lrb a rrb kong  k l  and wasserman  g s   temporal summation in the receptor potential of the limulus lateral eye  comparison between retinula and eccentric cells   sensory processes  1978  vol  2  pp 9 20  lrb b rrb michelson  r p   the results of electrical stimulation of the cochlea in human sensory deafness   annals of otology  rhinology and laryngology  1971  vol  80  pp 914 919  mia dej ovsky  m g  eddington  d k  dobelle  w h  and brackmann  d e   artificial hearing for the deaf by cochlear stimulation  pitch modulation and some parametric thresholds   american society for artificial internal organs  transactions  1974  vol  21  pp 1 7  miyamoto  r t  gossett  s k  groom  g l  kienle  m l  pope  m l  and shallop  j k   cochlear implants  an auditory prosthesis for the deaf   journal of the indiana state medical association  1982  vol  75  pp 174 177  miyamoto  r t  myres  w a  pope  m l  and carotta  c a   cochlear implants for deaf children   laryngoscope  1986  vol  96  pp 990 996  pialoux  p  chouard  c h  meyer  b  and fu   ain  c   indications and results of the multichannel cochlear implant   acta otolaryngology  1979  vo   87  pp 185 189  robbins  a m i osberger  m j  miyamoto  r t  kienle  m j  and myres  w a   speech tracking performance in single channel cochlear implant subjects   journtll of speech and hearing research  1985  vol  28  pp 565 578  russell  i j and sellick  p m   the tuning properties of cochlear hair cells   in e f evans and j p wilson lrb eds  rrb  psychophysics and physiology 0 f hearing  london  academic press  1977  wasserman  g s   limulus psychophysics  temporal summation in the ventral eye   journal of experimental psychology  general  1978  vol  107  pp 276 286  789 lsb 17 rsb lsb 18 rsb lsb 19 rsb lsb 20 rsb lsb 21 rsb wasserman  g s   limulus psychophysics  increment threshold   perception  psychophysics  1981  vol  29  pp 251 260  wasserman  g s  felsten  g  and easland  g s   receptor saturation and the psychophysical function   investigative ophthalmology and visual science  1978  vol  17  p 155 lrb abstract rrb  wasserman  g s  felsten  g  and easland  g s   the psychophysical function  harmonizing fechner and stevens   science  1979  vol  204  pp 85 87  wasserman  g s   cochlear implant codes and speech perception in profoundly deaf   bulletin of psychonomic society  vol  lrb 18 rrb 3  1987  wasserman  g s  wang bennett  l t  and miyamoto  r t   temporal dispersion in natural receptors and pattern discrimination mediated by artificial receptor   proc  of the fechner centennial symposium  hans buffart lrb ed  rrb  elsevier north holland  amsterdam  1987  r i i 7  9  sensory coding data    13 receptor signal stimulus i i central analysis behavior 11 i 15 r   i l    prosthetic    i signal      r      i i    j  17 fig 1  path of natural and prosthetic signals  sound central nervous system fig 2  the house urban cochlear implant  790 amplification  1  compressive rectifier dispersion  1  integrator fig 3  receptor model sound central nervous system fig 4  modified implant model  791 portable parallel neurocomputer 16khz am modulated output m external user controlled parameters fig 5  initial concept for a simd architecture  externally controlled ampufication dispersion neuron model sorter of n signals neuron model fig 6  feedforward neuron model implant  792 sorter of n signals in 0 lrb 1 rrb i   reset scanning signals inputs function threshold setofn  externally controlled threshold control  scanning function from i imax to i j min i j max i i min fig 7  signal sorting circuit  signal selectors t    0  les 1 1 1 j   output signal in      c fig 8  sigma pi units for signal composition  793 user controlled parameters tj4 13121114131 best matches dispersion 10101312111 gain  filter bypass  processor bypass  single neuron processing microphone fig 9  parameter controls for clinical studies 
13 en 297 temporal patterns of activity in neural networks paolo gaudiano dept of aerospace engineering sciences  university of colorado  boulder co 80309  usa january 5  1988 abstract patterns of activity over real neural structures are known to exhibit timedependent behavior  it would seem that the brain may be capable of utilizing temporal behavior of activity in neural networks as a way of performing functions which can not otherwise be easily implemented  these might include the origination of sequential behavior and the recognition of time dependent stimuli  a model is presented here which uses neuronal populations with recurrent feedback connections in an attempt to observe and describe the resulting time dependent behavior  shortcomings and problems inherent to this model are discussed  current models by other researchers are reviewed and their similarities and differences discussed  methods  preliminary results in previous papers  lsb 2 3 rsb computer models were presented that simulate a net consisting of two spatially organized populations of realistic neurons  the populations are richly interconnected and are shown to exhibit internally sustained activity  it was shown that if the neurons have response times significantly shorter than the typical unit time characteristic of the input patterns lrb usually 1 msec rrb  the populations will exhibit time dependent behavior  this will typically result in the net falling into a limit cycle  by a limit cycle  it is meant that the population falls into activity patterns during which all of the active cells fire in a cyclic  periodic fashion  although the period of firing of the individual cells may be different  after a fixed time the overall population activity will repeat in a cyclic  periodic fashion  for populations organized in 7x7 grids  the limit cycle will usually start 20  200 msec after the input is turned off  and its period will be in the order of 20 100 msec  the point ofinterest is that ifthe net is allowed to undergo synaptic modifications by means of a modified hebbian learning rule while being presented with a specific spatial pattern lrb i e  cells at specific spatial locations within the net are externally stimulated rrb  subsequent presentations of the same pattern with different temporal characteristics will cause the population to recall patterns which are spatially identical lrb the same cells will be active rrb but which have different temporal qualities  in other words  the net can fall into a different limit cycle  these limit cycles seem to behave as attractors in that similar input patterns will result in the same limit cycle  and hence each distinct limit cycle appears to have a basin of attraction  hence a net which can only learn a small  american institute of physics 1988 298 number of spatially distinct patterns can recall the patterns in a number of temporal modes  if it were possible to quantitatively discriminate between such temporal modes  it would seem reasonable to speculate that different limit cycles could correspond to different memory traces  this would significantly increase estimates on the capacity of memory storage in the net  it has also been shown that a net being presented with a given pattern will fall and stay into a limit cycle until another pattern is presented which will cause the system to fall into a different basin of attraction  if no other patterns are presented  the net will remain in the same limit cycle indefinitely  furthermore  the net will fall into the same limit cycle independently of the duration of the input stimulus  so long as the input stimulus is presented for a long enough time to raise the population activity level beyond a minimum necessary to achieve self sustained activity  hence  if we suppose that the net  recognizes  the input when it falls into the corresponding limit cycle  it follows that the net will recognize a string of input patterns regardless of the duration of each input pattern  so long as each input is presented long enough for the net to fall into the appropriate limit cycle  in particular  our system is capable of falling into a limit cycle within some tens of milliseconds  this can be fast enough to encode  for example  a string of phonemes as would typically be found in continuous speech  it may be possible  for instance  to create a model similar to rumelhart and mcclelland s 1981 model on word recognition by appropriately connecting multiple layers of these networks  if the response time of the cells were increased in higher layers  it may be possible to have the lowest level respond to stimuli quickly enough to distinguish phonemes lrb or some sub phonemic basic linguistic unit rrb  then have populations from this first level feed into a slower  word recognizing population layer  and so on  such a model may be able to perform word recognition from an input consisting of continuous phoneme strings even when the phonemes may vary in duration of presentation  shortcomings unfortunately  it was noticed a short time ago that a consistent mistake had been made in the process of obtaining the above mentioned results  namely  in the process of decreasing the response time of the cells i accidentally reached a response time below the time step used in the numerical approximation that updates the state of each cell during a simulation  the equations that describe the state of each cell depend on the state of the cell at the previous time step as well as on the input at the present time  these equations are of first order in time  and an explicit discrete approximation is used in the model  unfortunately it is a known fact that care must be taken in selecting the size of the time step in order to obtain reliable results  it is infact the case that by reducing the time step to a level below the response time of the cells the dynamics of the system varied significantly  it is questionable whether it would be possible to adjust some of the population parameters within reson to obtain the same results with a smaller step size  but the following points should be taken into account  1 rrb other researchers have created similar models that show such cyclic behavior lrb see for example silverman  shaw and pearson lsb 7 rsb rrb  2 rrb biological data exists which would indicate the existance of cyclic or periodic bahvior in real neural systems lrb see for instance baird lsb 1 rsb rrb  as i just recently completed a series of studies at this university  i will not be able to perform a detailed examination of the system described here  but instead i will more 299 than likely create new models on different research equipment which will be geared more specifically towards the study of temporal behavior in neural networks  other models it should be noted that in the past few years some researchers have begun investigating the possibility of neural networks that can exhibit time dependent behavior  and i would like to report on some of the available results as they relate to the topic of temporal patterns  baird lsb l rsb reports findings from the rabbit s olfctory bulb which indicate the existance of phase locked oscillatory states corresponding to olfactory stimuli presented to the subjects  he outlines an elegant model which attributes pattern recognition abilities to competing instabilities in the dynamic activity of neural structures  he further speculates that inhomogeneous connectivity in the bulb can be selectively modified to achieve input sensitive oscillatory states  silverman  shaw and pearson lsb 7 rsb have developed a model based on a biologically inspired idealized neural structure  which they call the trion  this unit represents a localized group of neurons with a discrete firing period  it was found that small ensembles of trions with symmetric connections can exhibit quasi stable periodic firing patterns which do not require pacemakers or external driving  their results are inspired by existing physiological data and are consistent with other works  kleinfeld lsb 6 rsb  and sompolinsky and kanter lsb 8 rsb independently developed neural network models that can generate and recognize sequential or cyclic patterns  both models rely on what could be summarized as the recirculation of information through time delayed channels  very similar results are presented by jordan lsb 4 rsb who extends a typical connectionist or pdp model to include state and plan units with recurrent connections and feedback from output units through hidden units  he employs supervised learning with fuzzy constraints to induce learning of sequences in the system  from a slightly different approach  tank and hopfield lsb 9 rsb make use of patterned sets of delays which effectively compress information in time  they develop a model which recognizes patterns by falling into local minima of a state space energy function  they suggest that a systematic selection of delay functions can be done which will allow for time distortions that would be likely to occur in the input  finally  a somewhat different approach is taken by homma  atlas and marks lsb 5 rsb  who generalize a network for spatial pattern recognition to one that performs spatio temporal patterns by extending classical principles from spatial networks to dynamic networks  in particular  they replace multiplication with convolution  weights with transfer functions  and thresholding with non linear transforms  hebbian and delta learning rules are similarly generalized  the resulting models are able to perform temporal pattern recognition  the above is only a partial list of some of the relevant work in this field  and there are probably various other results i am not aware of  discussion all of the above results indicate the importance of temporal patterns in neural networks  the need is apparent for further formal models which can successfully quantify temporal behavior in neural networks  several questions must be answered to further 300 clarify the role and meaning of temporal patterns in neural nets  for instance  there is an apparent difference between a model that performs sequential tasks and one that performs recognition of dynamic patterns  it seems that appropriate selection of delay mechanisms will be necessary to account for many types of temporal pattern recognition  the question of scaling must also be explored  mechanism are known to exist in the brain which can cause delays ranging from the millisecond range lrb e g variations in synaptic cleft size rrb to the tenth of a second range lrb e g axonal transmission times rrb  on the other hand  the brain is capable of rec  ignizing sequences of stimuli that can be much longer than the typical neural event  such as for instance being able to remember a song in its entirety  these and other questions could lead to interesting new aspects of brain function which are presently unclear  references lsb 1 rsb baird  b   nonlinear dynamics of pattern formation and pattern recognition in the rabbit olfactory bulb   physica 22d  150 175  1986  lsb 2 rsb gaudiano  p   computer models of neural networks   unpublished master s thesis  university of colorado  1987  lsb 3 rsb gaudiano  p  macgregor  r j   dynamic activity and memory traces in computer simulated recurrently connected neural networks   proceedings of the first international conference on neural networks  2 177 185  1987  lsb 4 rsb jordan  m i   attractor dynamics and parallelism in a connectionist sequential machine   proceedings of the eighth annual conference of the cognitive sciences society  1986  lsb 5 rsb homma  t  atlas  l e  marks  r j ii   an artificial neural network for spatiotemporal bipolar patterns  application to phoneme classification   to appear in proceedings of neural information processing systems conference lrb alp rrb  1987  lsb 6 rsb kleinfeld  d   sequential state generation by model neural networks   proc  natl acad  sci  usa  83  9469 9473  1986  lsb 7 rsb silverman  d l  shaw  g l  pearson  l c  associative recall properties of the trion model of cortical organization   biol  cybern  53 259 271  1986  lsb 8 rsb sompolinsky  h  kanter  i  temporal association in asymmetric neural networks   phys  rev let  57 2861 2864  1986  lsb 9 rsb tank  d w  hopfield  l l  neural computation by concentrating information in time   proc  natl acad  sci  usa  84 1896 1900  1987 
14 en 301 encoding geometric invariances in higher order neural networks c l giles air force office of scientific research  bolling afb  dc 20332 r d griffin naval research laboratory  washington  dc 20375 5000 t maxwell sachs freeman associates  landover  md 20785 abstract we describe a method of constructing higher order neural networks that respond invariantly under geometric transformations on the input space  by requiring each unit to satisfy a set of constraints on the interconnection weights  a particular structure is imposed on the network  a network built using such an architecture maintains its invariant performance independent of the values the weights assume  of the learning rules used  and of the form of the nonlinearities in the network  the invariance exhibited by a firstorder network is usually of a trivial sort  e g  responding only to the average input in the case of translation invariance  whereas higher order networks can perform useful functions and still exhibit the invariance  we derive the weight constraints for translation  rotation  scale  and several combinations of these transformations  and report results of simulation studies  introduction a persistent difficulty for pattern recognition systems is the requirement that patterns or objects be recognized independent of irrelevant parameters or distortions such as orientation lrb position  rotation  aspect rrb  scale or size  background or context  doppler shift  time of occurrence  or signal duration  the remarkable performance of humans and other animals on this problem in the visual and auditory realms is often taken for granted  until one tries to build a machine with similar performance  thoufh many methods have been developed for dealing with these problems  we have classified them into two categories  1 rrb preprocessing or transformation lrb inherent rrb approaches  and 2 rrb case specific or  brute force  lrb learned rrb approaches  common transformation techniques include  fourier  hough  and related transforms  moments  and fourier descriptors of the input signal  in these approaches the signal is usually transformed so that the subsequent processing ignores arbitrary parameters such as scale  translation  etc  in addition  these techniques are usually computationally expensive and are sensitive to noise in the input signal  the  brute force  approach is exemplified by training a device  such as a perceptron  to classify a pattern independent of it s position by presenting the  american institute of physics 1988 302 training pattern at all possible positions  madaline machines 2 have been shown to perform well using such techniques  often  this type of invariance is pattern specific  does not easily generalize to other patterns  and depends on the type of learning algorithm employed  furthermore  a great deal of time and energy is spent on learning the invariance  rather than on learning the signal  we describe a method that has the advantage of inherent invariance but uses a higher order neural network approach that must learn only the desired signal  higher order units have been shown to have unique computational strengths and are quite amenable to the encoding of a priori know1edge  3  7 mathematical development our approach is similar to the group invariance approach 8 10 although we make no appeal to group theory to obtain our results  we begin by selecting a transformation on the input space  then require the output of the unit to be invariant to the transformation  the resulting equations yield constraints on the interconnection weights  and thus imply a particular form or structure for the network architecture  for the i th unit yi of order m defined on a discrete input space  let the output be given by yi lsb yim lrb x rrb  p lrb x rrb rsb  f lrb wio   wi 1 lrb x1 rrb p lrb x1 rrb    wi 2 lrb x1  x2 rrb p lrb x1 rrb p lrb x2 rrb       wi m lrb x1    xm rrb p lrb x1 rrb   p lrb xm rrb rrb  lrb 1 rrb where p lrb x rrb is the input pattern or signal function lrb sometimes called a pixel rrb evaluated at position vector x  wim lrb xl   xm rrb is the weight of order m connecting the outputs of units at xl  x2    xm to the ith unit  i e  it correlates m values  f lrb u rrb is some threshold or sigmoid output function  and the summations extend over the input space  yim lrb x rrb represents the entire set of weights associated with the i th unit  these units are equivalent to the sigma pi units a defined by rumelhart  hinton  and williams  7 systems built from these units suffer from a combinatorial explosion of terms  hence are more complicated to build and train  to reduce the severity of this problem  one can limit the range of the interconnection weights or the number of orders  or impose various other constraints  we find that  in addition to the advantages of inherent invariance  imposing an invariance constraint on eq  lrb 1 rrb reduces the number of allowed athe sigma pi neural networks are multi layer networks with higher order terms in any layer  as such  most of the neural networks described here can be considered as a special case of the sigma pi units  however  the sigma pi units as originally formulated did not have invariant weight terms  though it is quite simple to incorporate such invariances in these units  303 weights  thus simplifying the architecture and shortening the training time  we now define what we mean by invariance  the output of a unit is invariant with respect to the transformation t on the input pattern if 9 lrb 2 rrb an example of the class of invariant response defined by eq  lrb 2 rrb would be invariant detection of an object in the receptive field of a panning or zooming camera  an example of a different class would be invariant detection of an object that is moving within the field of a fixed camera  one can think of this latter case as consisting of a fixed field of  noise  plus a moving field that contains only the object of interest  if the detection system does not respond to the fixed field  then this latter case is included in eq  lrb 2 rrb  to illustrate our method we derive the weight constraints for one dimensional translation invariance  we will first switch to a continuous formulation  however  for reasons of simplicity and generality  and because it is easier to grasp the physical significance of the results  although any numerical simulation requires a discrete formulation and has significant implications for the implementation of our results  instead of an index i  we now keep track of our units with the continuous variable u with these changes eq  lrb 2 rrb now becomes y lsb u  wm lrb x rrb  p lrb x rrb rsb  f lrb wo  jrdxl wl lrb u  xl rrb p lrb xl rrb    f  jr dxl   dxm wm lrb u  xl    xm rrb p lrb xl rrb   p lrb xm rrb rrb  lrb 3 rrb the limits on the integrals are defined by the problem and are crucial in what follows  let t be a translation of the input pattern by  xo  so that lrb 4 rrb t lsb p lrb x rrb rsb  p lrb x  xo rrb where xo is the translation of the input pattern  ty lsb u  wm lrb x rrb  p lrb x rrb rsb  y lsb u  ym lrb x rrb  p lrb x  xo  rrb  then  from eq lrb 2 rrb  y lsb u  wm lrb x rrb  p lrb x rrb rsb lrb 5 rrb since p lrb x rrb is arbitrary we must impose term by term equality in the argument of the threshold function  i e  f dxl wl lrb u  xl rrb p lrb xl rrb  f dxl wl lrb u  xl rrb p lrb xl  xo rrb  lrb sa rrb jr fdxl dx2 w2 lrb u  xl  x2 rrb p lrb xl rrb p lrb x2 rrb  jr f dxl dx2 w2 lrb u  xl  x2 rrb p lrb xl  xo rrb p lrb x2  xo rrb  etc  lrb sb rrb 304 making the substitutions xl  xl xo  x2   x2 xo  etc  we find that f dxl wl lrb u  xl rrb f f dxl p lrb xl rrb  f dxl wi lrb u  xl xo rrb p lrb xi rrb  lrb 6a rrb dx2 w2 lrb u  xi  x2 rrb p lrb xi rrb p lrb x2 rrb  f f dxi dx2 w2 lrb u  xi xo  x2 xo rrb p lrb xi rrb p lrb x2 rrb  lrb 6b rrb etc  note that the limits of the integrals on the right hand side must be adjusted to satisfy the change of variables  if the limits on the integrals are infinite or if one imposes some sort of periodic boundary condition  the limits of the integrals on both sides of the equation can be set equal  we will assume in the remainder of this paper that these conditions can be met  normally this means the limits of the integrals extend to infinity  lrb in an implementation  it is usually impractical or even impossible to satisfy these requirements  but our simulation results indicate that these networks perform satisfactorily even though the regions of integration are not identical  this question must be addressed for each class of transformation  it is an integral part of the implementation design  rrb since the functions p lrb x rrb are arbitrary and the regions of integration are the same  the weight functions must be equal  this imposes a constraint on the functional form of the weight functions or  in the discrete implementation  limits the allowed connections and thus the number of weights  in the case of translation invariance  the constraint on the functional form of the weight functions requires that w1 lrb u  xi rrb  wl lrb u  x rsb   xo rrb  w2 lrb u  xi  x2 rrb  w2 lrb u  xi xo  x2 xo rrb  lrb 7a rrb lrb 7b rrb etc  these equations imply that the first order weight is independent of input position  and depends only on the output position u  the second order weight is a function only of vector differences  io i e  w1 lrb u  xj rrb  j  lrb u rrb  lrb 8a rrb w2 lrb u  x rsb   x2 rrb  w2 lrb u  x rsb   xl rrb  lrb 8b rrb for a discrete implementation with n input units lrb pixels rrb fully connected to an output unit  this requirement reduces the number of second order weights from order n2 to order n  i e  only weights for differences of indexes are needed rather than all unique pair combinations  of course  this advantage is multiplied as the number of fully connected output units increases  further examples we have applied these techniques to several other transformations of interest  for the case of transformation of scale 305 define the scale operator s such that sp lrb x rrb  ailp lrb ax rrb lrb 9 rrb where a is the scale factor  and x is a vector of dimension n  the factor an is used for normalization purposes  so that a given figure always contains the same  energy  regardless of its scale  application of the same procedure to this transformation leads to the following constraints on the weights  wl lrb u  xjfa rrb   wl lrb u    w2 lrb u  x1ia  xv  a rrb   w2 lrb u   x l   rrb  w3 lrb u  xlla  x2 a  x3 a rrb  w3 lrb u  x rsb   x2  x3 rrb  etc  lrb loa rrb lrb lob rrb lrb loc rrb consider a two dimensional problem viewed in polar coordinates lrb r  t rrb  a set of solutions to these constraints is j lrb u  q  ti rrb  w1 lrb u  q rrb  w2 lrb u  rl  r2  tl  t2 rrb  w2 lrb u  rllr2  tl  t2 rrb  w3 lrb u  rl  r2  r3  tl  t2  t3 rrb  w3 lrb u  lrb rl r2 rrb  r3  tl  t2  t3 rrb  lrb lla rrb lrb llb rrb lrb llc rrb note that with increasing order comes increasing freedom in the selection of the functional form of the weights  any solution that satisfies the constraint may be used  this gives the designer additional freedom to limit the connection complexity  or to encode special behavior into the net architecture  an example of this is given later when we discuss combining translation and scale invariance in the same network  now consider a change of scale for a two dimensional system in rectangular coordinates  and consider only the second order weights  a set of solutions to the weight constraint is  w2 lrb u  xl  yl  x2  y2 rrb  w2 lrb u  xl yl  x2 y2 rrb   w2 lrb u  xl x2  yl y2 rrb  w2 lrb u  xl  yl  x2  y2 rrb  w2 lrb u  lrb xl x2 rrb  lrb yl y2 rrb rrb  etc  w2 lrb u  xl  yl  x2  y2 rrb lrb 12a rrb lrb l2b rrb lrb 12c rrb we have done a simulation using the form of eq  lrb 12b rrb  the simulation was done using a small input space lrb 8x8 rrb and one output unit  a simple least mean square lrb back propagation rrb algorithm was used for training the network  when taught to distinguish the letters t and c at one scale  it distinguished them at changes of scale of up to 4x with about 15 percent maximum degradation in the output strength  these results are quite encouraging because no special effort was required to make the system work  and no corrections or modifications were made to account for the boundary condition requirements as discussed near eq  lrb 6 rrb  this and other simulations are discussed further later  as a third example of a geometric transformation  consider the case of rotation about the origin for a two dimensional space in polar coordinates  one can readily show that the weight constraints 306 are satisfied if wl lrb u  rl  tl rrb  wl lrb u  rl rrb  w2 lrb u  rl  r2  tl  t2 rrb  w2 lrb u  rl  r2  tl t2 rrb  etc  lrb 13a rrb lrb l3b rrb these results are reminiscent of the results for translation invariance  this is not uncommon  seemingly different problems often have similar constraint requirements if the proper change of variable is made  this can be used to advantage when implementing such networks but we will not discuss it further here  an interesting case arises when one considers combinations of invariances  e g  scale and translation  this raises the question of the effect of the order of the transformations  i e  is scale followed by translation equivalent to translation followed by scale  the obvious answer is no  yet for certain cases the order is unimportant  consider first the case of change of scale by a  followed by a translation xc  the constraints on the weights up to second order are  wl lrb u  xl rrb  wl lrb u  lrb xl xo rrb  a rrb  w2 lrb u  xl  x2 rrb 0  w2 lrb u  lrb xl xo rrb  a  lrb x2 xo rrb  a rrb  lrb 14a rrb lrb l4b rrb and for translation followed by scale the constraints are  wl lrb u  xl rrb  wl lrb u  lrb xl a rrb  xo rrb  and lrb lsa rrb w2 lrb u  xl  x2 rrb  w2 lrb u  lrb xl a rrb  xo  lrb x2ia rrb  xo rrb  lrb lsb rrb consider only the second order weights for the two dimensional case  choose rectangular coordinate variables lrb x  y rrb so that the translation is given by lrb xo  yo rrb  then w2 lrb u  xl  yl  x2  y2 rrb  w2 lrb u  lrb xl a rrb  xo  lrb yl a rrb  yo  lrb x2 a rrb  xo  lrb y2 a rrb  yo rrb  lrb l6a rrb w2 lrb u  xl  yl  x2  y2 rrb w2 lrb u  lrb xl  x o rrb  a  lrb yl yo rrb  a  lrb x2  xo rrb  a  lrb y2 yo rrb  a rrb  lrb 16b rrb or if we take as our solution w2 lrb u  xl  yl  x2  y2 rrb  w2 lrb u  lrb x1 x2 rrb  lrb yl y2   lrb 17 rrb then w2 is invariant to scale and translation  and the order is unimportant  with higher order weights one can be even more adventurous  as a final example consider the case of a change of scale by a factor a and rotation about the origin by an amount to for a twodimensional system in polar coordinates  lrb note that the order of transformation makes no difference  rrb the weight constraints up to second order are  lrb 18a rrb 307 lrb 18b rrb the first order constraint requires that wi be independent of the input variables  but for the second order term one can obtain a more useful solution  lrb 19 rrb this implies that with second order weights  one can construct a unit that is insensitive to changes in scale and rotation of the input space  how useful it is depends upon the application  simulation results we have constructed several higher order neural networks that demonstrated invariant response to transformations of scale and of translation of the input patterns  the systems were small  consisting of less than 100 input units  were constructed from second and first order units  and contained only one  two  or three layers  we used a back propagation algorithm modified for the higher order lrb sigma pi rrb units  the simulation studies are still in the early stages  so the performance of the networks has not been thoroughly investigated  it seems safe to say  however  that there is much to be gained by a thorough study of these systems  for example  we have demonstrated that a small system of second order units trained to distinguish the letters t and c at one scale can continue to distinguish them over changes in scale of factors of at least four without retraining and with satisfactory performance  similar performance has been obtained for the case of translation invariance  even at this stage  some interesting facets of this approach are becoming clear  1 rrb even with the constraints imposed by the invariance  it is usually necessary to limit the range of connections in order to restrict the complexity of the network  this is often cited as a problem with higher order networks  but we take the view that one can learn a great deal more about the nature of a problem by examining it at this level rather than by simply training a network that has a general purpose architecture  2 rrb the higher order networks seem to solve problems in an elegant and simple manner  however  unless one is careful in the design of the network  it performs worse than a simpler conventional network when there is noise in the input field  3 rrb learning is often  quicker  than in a conventional approach  although this is highly dependent on the specific problem and implementation design  it seems that a tradeoff can be made  either faster learning but less noise robustness  or slower learning with more robust performance  discussion we have shown a simple way to encode geometric invariances into neural networks lrb instead of training them rrb  though to be useful the networks must be constructed of higher order units  the invariant encoding is achieved by restricting the allowable network 308 architectures and is independent of learning rules and the form of the sigmoid or threshold functions  the invariance encoding is normally for an entire layer  although it can be on an individual unit basis  it is easy to build one or more invariant layers into a multi layer net  and different layers can satisfy different invariance requirements  this is useful for operating on internal features or representations in an invariant manner  for learning in such a net  a multi layered learning rule such as generalized backpropagation 7 must be used  in our simulations we have used a generalized back propagation learning rule to train a two layer system consisting of a second order  translation invariant input layer and a first order output layer  note that we have not shown that one can not encode invariances into layered first order networks  but the analysis in this paper implies that such invariance would be dependent on the form of the sigmoid function  when invariances are encoded into higher order neural networks  the number of interconnections required is usually reduced by orders of powers of n where n is the size of the input  for example  a fully connected  first order  single layer net with a single output unit would have order n interconnections  a similar second order net  order n2  if this second order net lrb or layer rrb is made shift invariant  the order is reduced to n  the number of multiplies and adds is still of order n2  we have limited our discussion in this paper to geometric invariances  but there seems to be no reason why temporal or other invariances could not be encoded in a similar manner  references 1  d h ballard and c m brown  computer vision lrb prentice hall  englewood cliffs  nj  1982 rrb  2  b widrow  ieee first int1  conf  on neural networks  87th019l7  vol  1  p 143  san diego  ca  june 1987  3  j a feldman  biological cybernetics 46  27 lrb 1982 rrb  4  c l giles and t maxwell  app1  optics 26  4972 lrb 1987 rrb  5  g e hinton  proc  7th inti  joint conf  on artificial intelligence  ed  a drina  683 lrb 1981 rrb  6  y c lee  g doolen  h h chen  g z sun  t maxwell  h y lee  c l giles  physica 22d  276 lrb 1986 rrb  7  d e rume1hart  g e hinton  and r j williams  parallel distributed processing  vol  1  ch  8  d e rume1hart and j l mcclelland  eds  lrb mit press  cambridge  1986 rrb  309 8  t  maxwell  c l giles  y c lee  and h h chen  proc  ieee inti  conf  on systems  man  and cybernetics  86ch2364 8  p 627  atlanta  ga  october 1986  9  w pitts and w s mcculloch  bull  math  biophys  9  127 lrb 1947 rrb  10  m minsky and s  papert  perceptrons lrb mit press  cambridge  mass  1969 rrb 
15 en 270 correlational strength and computational algebra of synaptic connections between neurons eberhard e fetz department of physiology  biophysics  university of washington  seattle  wa 98195 abstract intracellular recordings in spinal cord motoneurons and cerebral cortex neurons have provided new evidence on the correlational strength of monosynaptic connections  and the relation between the shapes of postsynaptic potentials and the associated increased firing probability  in these cells  excitatory postsynaptic potentials lrb epsps rrb produce crosscorrelogram peaks which resemble in large part the derivative of the epsp  additional synaptic noise broadens the peak  but the peak area  i e  the number of above chance firings triggered per epsp  remains proportional to the epsp amplitude  a typical epsp of 100  v triggers about 01 firings per epsp  the consequences of these data for information processing by polysynaptic connections is discussed  the effects of sequential polysynaptic links can be calculated by convolving the effects of the underlying monosynaptic connections  the net effect of parallel pathways is the sum of the individual contributions  introduction interactions between neurons are determined by the strength and distribution of their synaptic connections  the strength of synaptic interactions has been measured directly in the central nervous system by two techniques  intracellular recording reveals the magnitude and time course of postsynaptic potentials lrb psps rrb produced by synaptic connections  and crosscorrelation of extracellular spike trains measures the effect of the psp s on the firing probability of the connected cells  the relation between the shape of excitatory postsynaptic potentials lrb epsps rrb and the shape of the crosscorrelogram peak they produce has been empirically investigated in cat motoneurons 2 4 5 and in neocortical cells 10  relation between epsp s and correlogram peaks synaptic interactions have been studied most thoroughly in spinal cord motoneurons  figure 1 illustrates the membrane potential of a rhythmically firing motoneuron  and the effect of epsps on its firing  an epsp occurring sufficiently close to threshold lrb 8 rrb will cause the motoneuron to fire and will advance an action potential to its rising edge lrb top rrb  mathematical analysis of this threshold crossing process predicts that an epsp with shape e lrb t rrb will produce a firing probability f lrb t rrb  which resembles  american institute of phy  ics 1988 271 ri f     i 8  i i                                      i        rrb        i i         r   i           epsp e lrb t rrb t cross  correlogram f lrb t rrb time t fig 1  the relation between epsp s and motoneuron firing  top  membrane trajectory of rhythmically firing motoneuron  showing epsp crossing threshold lrb 8 rrb and shortening the normal interspike interval by advancing a spike  v lrb t rrb is difference between membrane potential and threshold  middle  same threshold crossing process aligned with epsp  with v lrb t rrb plotted as falling trajectory  intercept lrb at upward arrow rrb indicates time of the advanced action potential  bottom  cross correlation histogram predicted by threshold crossings  the peak in the firing rate f lrb t rrb above baseline lrb fo rrb is produced by spikes advanced from baseline  as indicated by the changed counts for the illustrated trajectory  consequently  the area in the peak equals the area of the subsequent trough  272 the derivative of the epsp 4 8  specifically  for smooth membrane potential trajectories approaching threshold lrb the case of no additional synaptic noise rrb  f lrb t rrb  fo  lrb fo v rrb del dt lrb 1 rrb v where fo is the baseline firing rate of the motoneuron and is the rate of closure between motoneuron membrane potential and threshold  this relation can be derived analytically by tranforming the process to a coordinate system aligned with the epsp lrb fig 1  middle rrb and calculating the relative timing of spikes advanced by intercepts of the threshold trajectories with the epsp 4  the above relation lrb 1 rrb is also valid for the correlogram trough during the falling phase of the epsp  as long as del dt  if the epsp falls more rapidly than the trough is limited at zero firing rate lrb as illustrated for the correlogram at bottom rrb  the fact that the shape of the correlogram peak above baseline matches the epsp derivative has been empirically confirmed for large epsps in cat motoneurons 4  this relation implies that the height of the correlogram peak above baseline is proportional to the epsp rate of rise  the integral of this relationship predicts that the area between the correlogram peak and baseline is proportional to the epsp amplitude  this linear relation further implies that the effects of simultaneously arriving epsps will add linearly  the presence of additional background synaptic  noise   which is normally produced by randomly occurring synaptic inputs  tends to make the correlogram peak broader than the duration of the epsp risetime  this broadening is produced by membrane potential fluctuations which cause additional threshold crossings during the decay of the epsp by trajectories that would have missed the epsp lrb e g  the dashed trajectory in fig 1  middle rrb  on the basis of indirect empirical comparisons it has been proposed 6 7 that the broader correlogram peaks can be described by the sum of two linear functions of e lrb t rrb   v  f lrb t rrb  fo  a e lrb t rrb  b deldt  v  lrb 2 rrb this relation provides a reasonable match when the coefficients lrb a and b rrb can be optimized for each case 5 7  but direct empirical comparisons 2 4 indicate that the difference between the correlogram peak and the derivative is typically briefer than the epsp  the effect of synaptic noise on the transform  between epsp and correlogram peak has not yet been analytically derived lrb except for the case of however the threshold crossing process has been gaussian noise1 rrb  simulated by a computer model which adds synaptic noise to the trajectories intercepting the epsp 1  the correlograms generated by the simulation match the correlograms measured empirically for small epsp s in motoneurons 2  confirming the validity of the model  although synaptic noise distributes the triggered firings over a wider peak  the area of the correlogram peak  i e  the number of motoneuron firings produced by an epsp  is essentially preserved and remains proportional to epsp amplitude for moderate noise levels  for unitary epsp s lrb produced by 273 a single afferent fiber rrb in cat motoneurons  the number of firings triggered per epsp lrb np rrb was linearly related to the amplitude lrb h rrb of the epsp 2  np  lrb o l  mv rrb  h lrb mv rrb  003 lrb 3 rrb the fact that the number of triggered spikes increases in proportion to epsp amplitude has also been confirmed for neocortical neurons 10  for cells recorded in sensorimotor cortex slices lrb probably pyramidal cells rrb the coefficient of h was very similar  0 07  mv  this means that a typical unitary epsp with amplitude of 100 ilv  raises the probability that the postsynaptic cell fires by less than 01  moreover  this increase occurs during a specific time interval corresponding to the rise time of the epsp  on the order of 1  2 msec  the net increase in firing rate of the postsynaptic cell is calculated by the proportional decrease in interspike intervals produced by the triggered spikes 4  lrb while the above values are typical  unitary epsp s range in size from several hundred ilv down to undetectable levels of severalllv  and have risetimes of 2  4 msec  rrb inhibitory connections between cells  mediated by inhibitory postsynaptic potentials lrb ipsps rrb  produce a trough in the cross correlogram  this reduction of firing probability below baseline is followed by a subsequent broad  shallow peak  representing the spikes that have been delayed during the ipsp  although the effects of inhibitory connections remain to be analyzed more quantitatively  preliminary results indicate that small ipsp s in synaptic noise produce decreases in firing probability that are similar to the increases produced by epsp s 4 5  disynaptic links the effects of polysynaptic links between neurons can be understood as combinations of the underlying monosynaptic connections  a monosynaptic connection from cell a to cell b would produce a first order cross correlation peak p1 lrb bia  t rrb  representing the conditional probability that neuron b fires above chance at time t  given a spike in cell a at time t  o  as noted above  the shape of this first order correlogram peak is largely proportional to the epsp derivative lrb for cells whose interspike interval exceeds the duration of the epsp rrb  the latency of the peak is the conduction time from a to b lrb fig 2 top left rrb  in contrast  several types of disynaptic linkages betw een a and b  mediated by a third neuron c  will produce a second order correlation peak between a and b  a disynaptic link may be produced by two serial monosynaptic connections  from a to c and from c to b lrb fig 2  bottom left rrb  or by a common synaptic input from c ending on both a and b lrb fig 2  bottom right rrb  in both cases  the second order correlation between a and b produced by the disynaptic link would be the convolution of the two firstorder correlations between the monosynaptically connected cells  lrb 4 rrb 274 as indicated by the diagram  the cross correlogram peak p2 lrb bia  t rrb would be smaller and more dispersed than the peaks of the underlying first order correlation peaks  for serial connections the peak would appear to the right of the origin  at a latency that is the sum of the two monosynaptic latencies  the peak produced by a common input typically straddles the origin  since its timing reflects the difference between the underlying latencies    monosynaptic connection     t  i  t   first order correlation  lrb aib  t rrb lja     lrb  b i a   t  rrb  disynaptic connection   lrb  ia t rrb 1  serial connection second order correlation common input r  a  t t i t  i     a  lrb c i a rrb  t t t i  ll lrb aic rrb   v h   t  j  i   t      t   lrb bic rrb p lrb bia rrb     2      p lrb bic rrb 1     j t           lrb bia rrb  l fig 2  correlational effects of monosynaptic and disynaptic links between two neurons  top  monosynaptic excitatory link from a to b produces an increase in firing probability of b after a lrb left rrb  as with all correlograms this is the time inverted probability of increased firing in a relative to b lrb right rrb  bottom  two common disynaptic links between a and b are a serial connection via c lrb left rrb and a common input from c  in both cases the effect of the disynaptic link is the convolution of the underlying monosynaptic links  275 this relation means that the probability that a spike in cell a will produce a correlated spike in cell b would be the product of the two probabilities for the intervening monosynaptic connections  given a typical np of  ol epsp  this would reduce the effectiveness of a given disynaptic linkage by two orders of magnitude relative to a monosynaptic connection  however  the net strength of all the disynaptic linkages between two given cells is proportional to the number of mediating intemeurons lrb c rcb  since the effects of parallel pathways add  thus  the net potency of all the disynaptic linkages between two cells could approach that of a monosynaptic linkage if the number of mediating interneurons were sufficiently large  it should also be noted that some intemeurons may fire more than once per epsp and have a higher probability of being triggered to fire than motoneurons 11  for completeness  two other possible disynaptic links between a and b involving a third cell c may be considered  one is a serial connection from b to c to a  which is the reverse of the serial connection from a to b  this would produce a p2 lrb bia rrb with peak to the left of the origin  the fourth circuit involves convergent connections from both a and b to c  this is the only combination that would not produce any causal link between a and b  the effects of still higher order polysynaptic linkages can be computed similarly  by convolving the effects produced by the sequential connections  for example  trisynaptic linkages between four neurons are equivalent to combinations of disynaptic and monosynaptic connections  the cross correlograms between two cells have a certain symmetry  depending on which is the reference cell  the cross correlation histogram of cell b referenced to a is identical to the time inverted correlogram of a referenced to b  this is illustrated for the monosynaptic connection in fig 2  top right  but is true for all correlograms  this symmetry represents the fact that the above chance probability of b firing after a is the same as the probability of a firing before b  p lrb bia  t rrb  p lrb aib   t rrb lrb 5 rrb as a consequence  polysynaptic correlational links can be computed as the same convolution integral lrb eq  4 rrb  independent of the direction of impulse propagation  p arallel paths and feedback loops in addition to the simple combinations of pair wise connections between neurons illustrated above  additional connections between the same cells may form circuits with various kinds of loops  recurrent connections can produce feedback loops  whose correlational effects are also calculated by convolving effects of the underlying synaptic links  parallel feed forward paths can form multiple pathways between the same cells  these produce correlational effects that are the sum of the effects of the individual underlying connections  the simplest feedback loop is formed by reciprocal connections between a pair of cells  the effects of excitatory feedback can be computed by 276 successive co  1volutions of the underlying monosynaptic connections lrb fig 3 top rrb  note that such a positive feedback loop would be capable of sustaining activity only if the connections were sufficiently potent to ensure postsynaptic firing  since the probabilities of triggered firings at a single synapse are considerably less than one  reverberating activity can be sustained only if the number of interacting cells is correspondingly increased  thus  if the probability for a single link is on the order of 01  reverberating activity can be sustained if a and b are similarly interconnected with at least a hundred cells in parallel  connections between three neurons may produce various kinds of loops  feedforward parallel pathways are formed when cell a is monosynaptically connected to b and in addition has a serial disynaptic connection through c  as illustrated in fig 3 lrb bottom left rrb  the correlational effects of the two linkages from a to b would sum linearly  as shown for excitatory connections  again  the effect of a larger set of cells lcb c rcb would be additive  feedback loops could be formed with three cells by recurrent connections between any pair  the correlational consequences of the loop again are the convolution of the underlying links  three cells can form another type loop if both a and b are monosynaptically connected  and simultaneously influenced by a common interneuron c lrb fig 3 bottom right rrb  in this case the expected correlogram between a and b would be the sum of the individual components  a common input peak around the origin plus a delayed peak produced by the serial connection  feedback loop 1         l                i                           parallel jeedforward path i i  t t t common input loop i t pi lrb bia rrb  p 2 lrb bia rrb pi lrb bia rrb  p 2 lrb bia rrb         j l  fig 3  correlational effects of parallel connections between two neurons  top  feedback loop between two neurons a and b produces higher order effects equivalent to convolution of mono  aptic effects  bottom  loops formed by parallel feed forward paths lrb left rrb and by a common mput concurrent with a monosynaptic link lrb right rrb produce additive effects  277 conclusions thus  a simple computational algebra can be used to derive the correlational effects of a given network structure  effects of sequential connections can be computed by convolution and effects of parallel paths by summation  the inverse problem  of deducing the circuitry from the correlational data is more difficult  since similar correlogram features may be produced by different circuits 9  the fact that monosynaptic links produce small correlational effects on the order of 01 represents a significant constraint in the mechanisms of information processing in real neural nets  for example  secure propagation of activity through serial polysynaptic linkages requires that the small probability of triggered firing via a given link is compensated by a proportional increase in the number of parallel links  thus  reliable serial conduction would require hundreds of neurons at each level  with appropriate divergent and convergent connections  it should also be noted that the effect of intemeurons can be modulated by changing their activity  the intervening cells need to be active to mediate the correlational effects  as indicated by eq  i  the size of the correlogram peak is proportional to the firing rate lrb fo rrb of the postsynaptic cell  this allows dynamic modulation of polysynaptic linkages  the greater the number of links  the more susceptible they are to modulation  acknowledgements  the author thanks mr garrett kenyon for stimulating discussions and the cited colleagues for collaborative efforts  this work was supported in part by nll i grants ns 12542 and rr00166  references 1  bishop  b  reyes  a d  and fetz e e  soc  for neurosci abst  11 157 lrb 1985 rrb  2  cope  t c  fetz  e e  and matsumura  m  j physiol  390 161 18 lrb 1987 rrb  3  fetz  e e and cheney  p d  j neurophysiol  44 751 772 lrb 1980 rrb  4  fetz  e e and gustafsson  b  j physiol  341 387 410 lrb 1983 rrb  5  gustafsson  b  and mccrea  d  j physiol  347 431 451 lrb 1984 rrb  6  kirkwood  p a  j neurosci  meth  1 107 132 lrb 1979 rrb  7  kirkwood  p a  and sears  t  j physiol  275 103 134 lrb 1978 rrb  8  knox  c k  biophys  j 14  567 582 lrb 1974 rrb  9  moore  g p  segundo  j p  perkel  d h and levitan  h  biophys  j 10 876900 lrb 1970 rrb  10  reyes  a d  fetz e e and schwindt  p c  soc  for neurosci abst  13 157 lrb 1987 rrb  11  surmeier  d j and weinberg  r j  brain res  331 180 184 lrb 1985 rrb 
16 en 219 network generality  training required  and precision required john s denker and ben s wittner at t bell laboratories holmdel  new jersey 07733 1 keep your hand on your wallet   leon cooper  1987 abstract we show how to estimate lrb 1 rrb the number of functions that can be implemented by a particular network architecture  lrb 2 rrb how much analog precision is needed in the connections in the network  and lrb 3 rrb the number of training examples the network must see before it can be expected to form reliable generalizations  generality versus training data required consider the following objectives  first  the network should be very powerful and versatile  i e  it should implement any function lrb truth table rrb you like  and secondly  it should learn easily  forming meaningful generalizations from a small number of training examples  well  it is information theoretically impossible to create such a network  we will present here a simplified argument  a more complete and sophisticated version can be found in denker et al lrb 1987 rrb  it is customary to regard learning as a dynamical process  adjusting the weights lrb etc rrb in a single network  in order to derive the results of this paper  however  we take a different viewpoint  which we call the ensemble viewpoint  imagine making a very large number of replicas of the network  each replica has the same architecture as the original  but the weights are set differently in each case  no further adjustment takes place  the  learning process  consists of winnowing the ensemble of replicas  searching for the one lrb s rrb that satisfy our requirements  training proceeds as follows  we present each item in the training set to every network in the ensemble  that is  we use the abscissa of the training pattern as input to the network  and compare the ordinate of the training pattern to see if it agrees with the actual output of the network  for each network  we keep a score reflecting how many times lrb and how badly rrb it disagreed with a training item  networks with the lowest score are the ones that agree best with the training data  if we had complete confidence in lcurrently at nynex science and technology  500 westchester ave  white plains  ny 10604  rrb american institute of physics 1988 220 the reliability of the training set  we could at each step simply throwaway all networks that disagree  for definiteness  let us consider a typical network architecture  with no input wires and nt units in each processing layer i  for i e lcb i   l rcb  for simplicity we assume nl  1  we recognize the importance of networks with continuous valued inputs and outputs  but we will concentrate for now on training lrb and testing rrb patterns that are discrete  with n  no bits of abscissa and n l  1 bit of ordinate  this allows us to classify the networks into bins according to what boolean input output relation they implement  and simply consider the ensemble of bins  there are 22n jossible bins  if the network architecture is completely general and powerful  all 22 functions will exist in the ensemble of bins  on average  one expects that each training item will throwaway at most half of the bins  assuming maximal efficiency  if m training items are used  then when m  2n there will be only one bin remaining  and that must be the unique function that consistently describes all the data  but there are only 2n possible abscissas using n bits  therefore a truly general network can not possibly exhibit meaningful generalization  100  of the possible data is needed for training  now suppose that the network is not completely general  so that even with all possible settings of the weights we can only create functions in 250 bins  where so  2n  we call so the initial entropy of the network  a more formal and general definition is given in denker et al lrb 1987 rrb  once again  we can use the training data to winnow the ensemble  and when m  so  there will be only one remaining bin  that function will presumably generalize correctly to the remaining 2n  m possible patterns  certainly that function is the best we can do with the network architecture and the training data we were given  the usual problem with automatic learning is this  if the network is too general  so will be large  and an inordinate amount of training data will be required  the required amount of data may be simply unavailable  or it may be so large that training would be prohibitively time consuming  the shows the critical importance of building a network that is not more general than necessary  estimating the entropy in real engineering situations  it is important to be able to estimate the initial entropy of various proposed designs  since that determines the amount of training data that will be required  calculating so directly from the definition is prohibitively difficult  but we can use the definition to derive useful approximate expressions  lrb you would n t want to calculate the thermodynamic entropy of a bucket of water directly from the definition  either  rrb 221 suppose that the weights in the network at each connection i were not continuously adjustable real numbers  but rather were specified by a discrete code with bi bits  then the total number of bits required to specify the configuration of the network is lrb 1 rrb now the total number offunctions that could possibly be implemented by such a network architecture would be at most 2b  the actual number will always be smaller than this  since there are various ways in which different settings of the weights can lead to identical functions lrb bins rrb  for one thing  for each hidden layer 1 e lcb 1  l 1 rcb  the numbering of the hidden units can be permuted  and the polarity of the hidden units can be flipped  which means that 2 50 is less than 2b by a factor lrb among others rrb of iii nl  2n   in addition  if there is an inordinately large number of bits bi at each connection  there will be many settings where small changes in the connection will be immaterial  this will make 2 so smaller by an additional factor  we expect aso abi  1 when bi is small  and aso ab i  0 when bi is large  we must now figure out where the crossover occurs  the number of  useful and significant  bits of precision  which we designate b   typically scales like the logarithm of number of connections to the unit in question  this can be understood as follows  suppose there are n connections into a given unit  and an input signal to that unit of some size a is observed to be significant lrb the exact value of a drops out of the present calculation rrb  then there is no point in having a weight with magnitude much larger than a  nor much smaller than a n  that is  the dynamic range should be comparable to the number of connections  lrb this argument is not exact  and it is easy to devise exceptions  but the conclusion remains useful  rrb if only a fraction 1  s of the units in the previous layer are active lrb nonzero rrb at a time  the needed dynamic range is reduced  this implies b   log lrb n s rrb  note  our calculation does not involve the dynamics of the learning process  some numerical methods lrb including versions of back propagation rrb commonly require a number of temporary  guard bits  on each weight  as pointed out by llichard durbin lrb private communication rrb  another log n bits ought to suffice  these bits are not needed after learning is complete  and do not contribute to so  if we combine these ideas and apply them to a network with n units in each layer  fully connected  we arrive at the following expression for the number of different boolean functions that can be implemented by such a network  lrb 2 rrb where b  ln 2 log n lrb 3 rrb these results depend on the fact that we are considering only a very restricted type of processing unit  the output is a monotone function of a weighted sum of inputs  cover 222 lrb 1965 rrb discussed in considerable depth the capabilities of such units  valiant lrb 1986 rrb has explored the learning capabilities of various models of computation  abu mustafa has emphasized the principles of information and entropy and applied them to measuring the properties of the training set  at this conference  formulas similar to equation 3 arose in the work of baum  psaltis  and venkatesh  in the context of calculating the number of different training patterns a network should be able to memorize  we originally proposed equation 2 as an estimate of the number of patterns the network would have to memorize before it could form a reliable generalization  the basic idea  which has numerous consequences  is to estimate the number of lrb bins of rrb networks that can be realized  references 1  vasser abu mustafa  these proceedings  2  eric baum  these proceedings  3  t m cover   geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition   ieee trans  elec comp  ec 14  326 334  lrb june 1965 rrb 4  john denker  daniel schwartz  ben wittner  sara solla  john hopfield  richard howard  and lawrence jackel  complex systems  in press lrb 1987 rrb  5  demetri psaltis  these proceedings  6  1  g valiant  siam j comput  15 lrb 2 rrb  531 lrb 1986 rrb  and references therein  7  santosh venkatesh  these proceedings 
17 en 622 learning a color algorithm from examples anya c hurlbert and tomaso a poggio artificial intelligence laboratory and department of brain and cognitive sciences  massachusetts institute of technology  cambridge  massachusetts 02139  usa abstract a lightness algorithm that separates surface reflectance from illumination in a mondrian world is synthesized automatically from a set of examples  pairs of input lrb image irradiance rrb and desired output lrb surface reflectance rrb  the algorithm  which resembles a new lightness algorithm recently proposed by land  is approximately equivalent to filtering the image through a center surround receptive field in individual chromatic channels  the synthesizing technique  optimal linear estimation  requires only one assumption  that the operator that transforms input into output is linear  this assumption is true for a certain class of early vision algorithms that may therefore be synthesized in a similar way from examples  other methods of synthesizing algorithms from examples  or  learning   such as backpropagation  do not yield a significantly different or better lightness algorithm in the mondrian world  the linear estimation and backpropagation techniques both produce simultaneous brightness contrast effects  the problems that a visual system must solve in decoding two dimensional images into three dimensional scenes lrb inverse optics problems rrb are difficult  the information supplied by an image is not sufficient by itself to specify a unique scene  to reduce the number of possible interpretations of images  visual systems  whether artificial or biological  must make use of natural constraints  assumptions about the physical properties of surfaces and lights  computational vision scientists have derived effective solutions for some inverse optics problems lrb such as computing depth from binocular disparity rrb by determining the appropriate natural constraints and embedding them in algorithms  how might a visual system discover and exploit natural constraints on its own  we address a simpler question  given only a set of examples of input images and desired output solutions  can a visual system synthesize  or  learn   the algorithm that converts input to output  we find that an algorithm for computing color in a restricted world can be constructed from examples using standard techniques of optimal linear estimation  the computation of color is a prime example of the difficult problems of inverse optics  we do not merely discriminate betwn n different wavelengths of light  we assign  american institute of physics 1988 623 roughly constant colors to objects even though the light signals they send to our eyes change as the illumination varies across space and chromatic spectrum  the computational goal underlying color constancy seems to be to extract the invariant surface spectral reflectance properties from the image irradiance  in which reflectance and ii   lumination are mixed 1  lightness algorithms 2 8  pioneered by land  assume that the color of an object can be specified by its lightness  or relative surface reflectance  in each of three independent chromatic channels  and that lightness is computed in the same way in each channel  computing color is thereby reduced to extracting surface reflectance from the image irradiance in a single chromatic channel  the image irra diance  s   is proportional to the product of the illumination intensity e  and the surface reflectance r  in that channel  s  lrb x  y rrb  r  lrb x  y rrb e  lrb x  y rrb  lrb 1 rrb this form of the image intensity equation is true for a lambertian reflectance model  in which the irradiance s  has no specular components  and for appropriately chosen color channels 9  taking the logarithm of both sides converts it to a sum  s lrb x  y rrb  rex  y rrb  e lrb x  y rrb  lrb 2 rrb where s  loges  rrb  r  log lrb r  rrb and e  log lrb e  rrb  given s lrb x  y rrb alone  the problem of solving eq  2 for r lrb x  y rrb is underconstrained  lightness algorithms constrain the problem by restricting their domain to a world of mondrians  two dimensional surfaces covered with patches of random colors 2 and by exploiting two constraints in that world  lrb i rrb r  lrb x  y rrb is unifonn within patches but has sharp discontinuities at edges between patches and lrb ii rrb e  lrb x  y rrb varies smoothly across the mondrian  under these constraints  lightness algorithms can recover a good approximation to r lrb x  y rrb and so can recover lightness triplets that label roughly constant colors 10  we ask whether it is possible to synthesize from examples an algorithm that ex  tracts reflectance from image irradiance  and whether the synthesized algorithm will resemble existing lightness algorithms derived from an explicit analysis of the constraints  we make one assumption  that the operator that transforms irradiance into reflectance is linear  under that assumption  motivated by considerations discussed later  we use optimal linear estimation techniques to synthesize an operator from examples  the examples are pairs of images  an input image of a mondrian under illumination that varies smoothly across space and its desired output image that displays the reflectance of the mondrian without the illumination  the technique finds the linear estimator that best maps input into desired output  in the least squares sense  for computational convenience we use one dimensional  training vectors  that represent vertical scan lines across the  londrian images lrb fig 1 rrb  we generate many 624 1s0t      100    a             so 100 110 100 llo 100 lilt  input d t i  2   0 a so 101 iso zoo ui   kfhjfeirq b 0 rrb 01 ii 100 iso 100 110 p jte  f  l o 50 100 uo zoo zso 100 i    c rrb 00 1 p  xe  100 iso 100 110 lot p    olltpllt lilll l   ll a fig 1  lrb a rrb the input data  a one dimensional vector 320 pixels long  its random mondrian reflectance pattern is superimposed on a linear illumination gradient with a random slope and offset  lrb b rrb shows the corresponding output solution  on the left the illumination and on the right rebectance  we used 1500 such pairs of inputoutput examples lrb each different from the others rrb to train the operator shown in fig 2  lrb c rrb shows the result obtained by the estimated operator when it acts on the input data lrb a rrb  not part of the training set  on the left is the illumination and on the right the reflectance  to be compared with lrb b rrb  this result is fairly typical  in some cases the prediction is even better  in others it is worse  different input vectors s by adding together different random t and e vectors  according to eq  2  each vector r represents a pattern of step changes across space  corresponding to one column of a rehectance image  the step changes occur at random pixels and are of random amplitude between set minimum and maximum values  each vector t represents a smooth gradient across space with a random offset and slope  correspondin  to one column of an illumination image  we th  n arrange the training vectors sand r as the columns of two matrices sand r  resp  ti    ely  our goal is then to compute the optimal solution l of ls  r where l is a linear operator represented as a matrix  625 it is well known that the solution of this equation that is optimal in the least squares sense is lrb 4 rrb where s  is the moore penrose pseudoinverse 11  we compute the pseudoinverse by overconstraining the problem  using many more training vectors than there are number of pixels in each vector  and using the straightforward formula that applies in the overconstrained case 12  s  st lrb sst rrb  l  the operator l computed in this way recovers a good approximation to the correct output vector r when given a new s  not part of the training set  as input lrb fig  ic rrb  a second operator  estimated in the same way  recovers the illumination e acting on a random two dimensional mondrian l also yields a satisfactory approximation to the correct output image  our estimation scheme successfully synthesizes an algorithm that performs the lightness computation in a mondrian world  what is the algorithm and what is its relationship to other lightness algorithms  to answer these questions we examine the structure of the matrix l  we assume that  although the operator is not a convolution operator  it should approximate one far from the boundaries of the image  that is  in its central part  the operator should be space invariant  performing the same action on each point in the image  each row in the central part of l should therefore be the same as the row above but displaced by one element to the right  inspection of the matrix confirmes this expectation  to find the form of l in its center  we thus average the rows there  first shifting them appropriately  the result  shown in fig 2  is a space invariant filter with a narrow positive peak and a broad  shallow  negative surround  interestingly  the filter our scheme synthesizes is very similar to land s most recent retinex operator 5  which divides the image irradiance at each pixel by a weighted average of the irradiance at all pixels in a large surround and takes the logarithm of that result to yield lightness 13  the lightness triplets computed by the retinex operator agree well with human perception in a mondrian world  the retinex operator and our matrix l both differ from land s earlier retinex algorithms  which require a non linear thresholding step to eliminate smooth gradients of illumination  the shape of the filter in fig 2  particularly of its large surround  is also suggestive of the  nonclassical  receptive fields that have been found in v4  a cortical area implicated in mechanisms underlying color constancy 14 17  the form of the space invariant filter is similar to that derived in our earlier formal analysis of the lightness problem 8  it is qualitatively the same as that which results from the direct application of regularization methods exploiting the spatial constraints on reflectance and illumination described above 9 18 19  the fourier transform of the filter of fig 2 is approximately a bandpass filter that cuts out low frequencies due  626  0 lrb  rrb  c  c  80 s    0 pi xe is 2   a 80  o 80 pixels fig 2  the space invariant part of the estimated operator  obtained by shifting and averaging the rows of a 160 pixel wide central square of the matrix l  trained on a set of 1500 examples with linear illumination gradients lrb see fig 1 rrb  when logarithmic illumination gradients are used  a qualitatively similar receptive field is obtained  in a separate experiment we use a training set of one dimensional mondrians with either linear illumination gradients or slowly varying sinusoidal illumination components with random wavelength  phase and amplitude  t he resulting filter is shown in the inset  the surrounds of both filters extend beyond the range we can estimate reliably  the range we show here  to slow gradients of illumination and preserves intennediate frequencies due to step changes in reflectance  in contrast  the operator that recovers the illumination  e takes the form of a low pass filter   ve stress that the entire operator l is not a space invariant filter  in this context  it is clear that the shape of the estimated operator should vary with the type of illumination gradient in the training set  we synthesize a second operator using a new set of examples that contain equal numbers of vectors with random  sinusoidally varying illumination components and ve  lrb tors with random  linear illumination gradients  whereas the first operator  synthe   sized from examples with strictly linear illumination gradients  has a broad negative surround that remains virtually constant throughout its extent  the new operator s surround lrb fig  2  inset rrb has a smaller ext lrb 111 627 and decays smoothly towards zero from its peak negative value in its center  we also apply the operator in fig 2 to new input vectors in which the density and amplitude of the step changes of reflectance differ greatly from those on which the operator is trained  the operator performs well  for example  on an input vector representing one column of an image of a small patch of one reflectance against a uniform background of a different reflectance  the entire image under a linear illumination gradient  this result is consistent with psychophysical experiments that show that color constancy of a patch holds when its mondrian background is replaced by an equivalent grey background 20  the operator also produces simultaneous brightness contrast  as expected from the shape and sign of its surround  the output reflectance it computes for a patch of fixed input reflectance decreases linearly with increasing average irradiance of the input test vector in which the patch appears  similarly  to us  a dark patch appears darker when against a light background than against a dark one  this result takes one step towards explaining such illusions as the koffka ring 21  a uniform gray annulus against a bipartite background lrb fig 3a rrb appears to split into two halves of different lightnesses when the midline between the light and dark halves of the background is drawn across the annulus lrb fig 3b rrb  the estimated operator acting on the koffka ring of fig 3b reproduces our perception by assigning a lower output reflectance to the left half of the annulus lrb which appears darker to us rrb than to the right half 22  yet the operator gives this brightness contrast effect whether or not the midline is drawn across the annulus lrb fig 3c rrb  becau  e the opf  rator can perform only a linear transformation between the input and output images  it is not surprising that the addition of the midline in the input evokes so little change in the output  these results demonstrate that the linear operator alone can not compute lightness in all worlds and suggest that an additional operator might be necessary to mark and guide it within bounded regions  our estimation procedure is motivated by our previous observation 9 23 18 that standard regularization algorithms 19 in early vision define linear mappings between input and output and therefore can be estimated associatively under certain condi  tions  the technique of optimal linear estimation that we use is closely related to optimal bayesian estimation 9  if we were to assume from the start that the optimal linear operator is space invariant  we could considerably simplify lrb and streamline rrb the computation by using standard correlation te   hniques 9 24  how does our estimation technique compare with other methods of  learning  a lightness algorithm  we can compute the r  ularized pseudoinverse using gradient descent on a  neural  network 25 with linf  ar units  since the pseudoinverse is lhf  unique best linear approximation in the l1 norm  a gradient descent method that 628 minimizes the square error between the actual output and desired output of a fully connected linear network is guaranteed to converge  albeit slowly  thus gradient descent in weight space converges to the same result as our first technique  the global minimum  b a c  n sa 0  it  sa  ut  input data pixel        iu ii    e sa        ut  output reflectance  with edge      i  i   i            e      i   lrb ut   output reflectance  without edge fig 3  lrb a rrb koffka ring  lrb b rrb koftka ring with midline drawn across annulus  lrb c rrb horizontal scan lines across koffka ring  top  scan line starting at arrow in lrb b rrb  middle  scan line at corresponding location in the output of linear operator acting on lrb b rrb  bottom  scan line at same location in the output of operator acting on lrb a rrb  629 we also compare the linear estimation technique with a  backpropagation  network  gradient descent on a 2 layer network with sigmoid units 25 lrb 32 inputs  32  hidden units   and 32 linear outputs rrb  using training vectors 32 pixels long  the network requires an order of magnitude more time to converge to a stable configuration than does the linear estimator for the same set of 32 pixel examples  the network s performance is slightly  yet consistently  better  measured as the root mean square error in output  averaged over sets of at least 2000 new input vectors  interestingly  the backpropagation network and the linear estimator err in the same way on the same input vectors  it is possible that the backpropagation network may show considerable inprovement over the linear estimator in a world more complex than the mondrian one  we are presently examining its performance on images with real world features such as shading  shadows  and highlights26  we do not think that our results mean that color constancy may be learned during a critical period by biological organisms  it seems more reasonable to consider them simply as a demonstration on a toy world that in the course of evolution a visual system may recover and exploit natural constraints hidden in the physics of the world  the significance of our results lies in the facts that a simple statistical technique may be used to synthesize a lightness algorithm from examples  that the technique does as well as other techniques such as backpropagation  and that a similar technique may be used for other problems in early vision  furthermore  the synthesized operator resembles both land s psychophysically tested retinex operator and a neuronal nonclassical receptive field  the operator s properties suggest that simultaneous color lrb or brightness rrb contrast might be the result of the visual system s attempt to discount illumination gradients 27 references and notes 1  since we do not have perfect color constancy  our visual system must not extract reflectance exactly  the limits on color constancy might reveal limits on the underlying computation  2  3  4  and s 5  6  e h land  am  sci  52 247 lrb 1964 rrb  e h land and j j mccann  j opt  soc  am  61  1 lcb 1971 rcb  e h land  in central and peripheral mechanisms of colour vision  t ottoson zeki  eds  lrb macmillan  new york  1985 rrb  pp 5 17  e h land  proc  nat  acad  sci  usa 83  3078 lrb 1986 rrb  b k p hom  computer graphics and image processing 3  277 lrb 1974 rrb  630 7  a blake  in central and peripheral mechanisms of colour vision  t ottoson and s zeki  eds  lrb macmillan  new york  1985 rrb  pp 45 59  8  a hurlbert  j opt  soc  am  a 3 1684 lrb 1986 rrb  9  a hurlbert and t poggio  artificialintelligence laboratory memo 909  lrb m lt  cambridge  ma  1987 rrb  10  r  lcb x  y rrb can be recovered at best only to within a constant  since eq  1 is invariant under the transformation of r  int o ar  and e  into a ie   where a is a constant  11  a albert  regression and the moore penrose pseudoinllerse  lrb academic press  new york  1972 rrb  12  the pseudoinverse  and therefore l  may also be computed by recursive techniques that improve its form as more data become available l l  13  our synthesized filter is not exactly identical with land s  the filter of fig 2 subtracts from the value at each point the average value of the logarithm of irradiance at all pixels  rather than the logarithm of the average values  the estimated operator is therefore linear in the logarithms  whereas land s is not  the numerical difference between the outputs of the two filters is small in most cases lrb land  personal communication rrb  and both agree well with psychophysical results  14  r desimone  s j schein  j moran and l g ungerleider  vision res  25  441 lrb 1985 rrb  15  h m wild  s r butler  d carden and j j kulikowski  nature lrb london rrb 313  133 lrb 1985 rrb  16  s m zeki  neuroscience 9  741 lrb 1983 rrb  17  s m zeki  neuroscience 9  767 lrb 1983 rrb  18  t poggio  et  al  in proceedings image understanding workshop  l baumann  ed  lrb science applications international corporation  mclean  va  1985 rrb  pp   25 39  19  t poggio  v torre and c koch  nature lrb london rrb 317 314 lrb 1985 rrb  20  a valberg and b lange malecki  investigative ophthalmology and visual science supplement 28  92 lrb 1987 rrb  21  k koffka  principles of gestalt psychology  lrb harcourt  brace and co  new york  1935 rrb  22  note that the operator achieves this effect by subtracting a non existent illumination gradient from the input signal  23  t poggio and a hurlbert  artificial intelligence laboratory working paper 264  lrb m lt  cambridge  ma  1984 rrb  24  estimation of the operator on two dimensional examples is possible  but computationally very expensive if done in the same way  the present computer simulations require several hours when run on standard serial computers  the two dimensional case 631 will need much more time lrb our one dimensional estimation scheme runs orders of magnitude faster on a cm 1 connection machine system with 16k processors rrb  25  d e rumelhart  g e hinton and r j williams  nature lrb london rrb 323  533 lrb 1986 rrb  26  a hurlbert  the computation of color  ph d  thesis  m l t  cambridge  ma  in preparation  2i  we are grateful to e land  e hildreth   j little  f wilczek and d hillis for reading the draft and for useful discussions  a rottenberg developed the routines for matrix operations that we used on the connection machine  t breuel wrote the backpropagation simulator 
18 en 564 programmable synaptic chip for electronic neural networks a moopenn  h langenbacher  a p thakoor  and s k khanna jet propulsion laboratory california institute of technology pasadena  ca 91009 abstract a binary synaptic matrix chip has been developed for electronic neural networks  the matrix chip contains a programmable 32x32 array of  long channel  nmosfet binary connection elements implemented in a 3 um bulk cmos process  since the neurons are kept offchip  the synaptic chip serves as a  cascadable  building block for a multi chip synaptic network as large as 512x512 in size  as an alternative to the programmable nmosfet lrb long channel rrb connection elements  tailored thin film resistors are deposited  in series with fet switches  on some cmos test chips  to obtain the weak synaptic connections  although deposition and patterning of the resistors require additional processing steps  they promise substantial savings in silcon area  the performance of a synaptic chip in a 32neuron breadboard system in an associative memory test application is discussed  introduction the highly parallel and distributive architecture of neural networks offers potential advantages in fault tolerant and high speed associative information processing  for the past few years  there has been a growing interest in developing electronic hardware to investigate the computational capabilities and application potential of neural networks as well as their dynamics and collective propertiesl  5  in an electronic hardware implementation of neural networks6  7 r the neurons lrb analog processing units rrb are represented by threshold amplifiers and the synapses linking the neurons by a resistive connection network  the synaptic strengths between neurons lrb the electrical resistance of the connections rrb represent the stored information or the computing function of the neural network  because of the massive interconectivity of the neurons and the large number of the interconnects required with the increasing number of neurons  implementation of a synaptic network using current lsi vlsi technology can become very difficult  a synaptic network based on a multi chip architecture would lessen this difficulty  he have designed  fabricated  and successfully tested cmos based programmable synaptic chips which could serve as basic  cascadabl e  building blocks for a multi chip electronic neural network  the synaptic chips feature complete programmability of 1024  lrb 32x32 rrb binary synapses  since the neurons are kept offchip  the synaptic chips can be connected in parallel  to obtain multiple grey levels of the connection strengths  as well as  american institute of physics 1988 565  cascaded  to form larger synaptic arrays for an expansion to a 512neuron system in a feedback or feed forward architecture  as a research tool  such a system would offer a significant speed software based neural network improvement over conventional simulations since convergence times for the parallel hardware system would be significantly smaller  in this paper  we describe the basic design and operation of synaptic cmos chips incorporating mosfet s as binary connection elements  the design and fabrication of synaptic test chips with tailored thin film resistors as ballast resistors for controlling power dissipation are also described  finally  we describe a synaptic chip based 32 neuron breadboard system in a feedback configuration and discuss its performance in an associative memory test application  binary synaptic cmos chip with mosfet connection elements there are two important design requirements for a binary connection element in a high density synaptic chip  the first requirement is that the connection in the on state should be  weak  to ensure low overall power dissipation  the required degree of  weakness  of the on connection largely depends on the synapse density of the chip  if  for example  a synapse density larger than 1000 per chip is desired  a dynamic resistance of the on connection should be greater than  100 x ohms  the second requirement is that to obtain grey scale synapses with up to four bits of precision from binary connections  the consistency of the on state connection resistance must be better than   5 percent  to ensure proper threshold operation of the neurons  both of the requirements are generally difficult to satisfy simultaneously in conventional vlsi cmos technology  for example  doped polysilicon resistors could be used to provide the weak connections  but they are difficult to fabricate with a resistance uniformity of better than 5 percent  we have used nmosfet s as connection elements in a multi chip synaptic network  by designing the nmosfet s with long channel  both the required high uniformity and high on state resistance have been obtained  a block diagram of a binary synaptic test chip incorporating nmosfet s as programmable connection elements is shown in fig 1  a photomicrograph of the chip is shown in fig 2  the synaptic chip was fabricated through mosis lrb mos implementation bulk cmos  two level metal  p well service rrb in a 3 micron  technology  the chip contains 1024 synaptic cells arranged in a 32x32 matrix configuration  each cell consists of a long channel nmosfet connected in series with another nmosfet serving as a simple on off switch  the state of the fet switch is controlled by the output of a latch which can be externally addressed via the row col address decoders  the 32 analog input lines lrb from the neuron outputs rrb and 32 analog output lines lrb to the neuron inputs rrb allow a number of such chips to be connected together to form larger connection matrices with up to 4 bit planes  the long channel nmosfet can function as either a purely resistive or a constant current source connection element  depending 566 from neuron outputs 1  32 vg                a   ur c   i6 a9  aoor oecooer setrst  i    row  v  c       s   rst r     q    1 1   0  32 to neuron inputs  i l  d i 6    a ao m figure 1  block diagram of a 32x32 binary synnaptic chip with long channel nmosfets as connection elements        rrb      figure 2  photomicrographs of a 32x32 binary connection cmos chip  the blowup on the right shows several synaptic cells  the  s   shape structures are the long channel nmosfets  on whether analog or binary output neurons are used  as a resistive connection  the nmosfet s must operate in the linear region of the transistor s drain i v characteristics  in the linear region  the channel resistance is approximately given byb ro n  lrb 11k rrb lrb lin rrb lrb vg  vt h rrb  1  567 here  k is a proportionality constant which depends on process parameters  land ware the channel length and width respectively  vg is the gate voltage  and vth is the threshold voltage  the transistor acts as a linear resistor provided the voltage across the channel is much less than the difference of the gate and threshold voltages  and thus dictates the operating voltage range of the connection  the nmosfet s presently used in our synaptic chip design have a channel length of 244 microns and width of 12 microns  at a gate voltage of 5 volts  a channel resistance of about 200 kohms was obtained over an operating voltage range of 1 5 volts  the consistency of the transistor i v characteristics has been verified to be within   3 percent in a single chip and   5 percent for chips from different fabrication runs  in the latter case  the transistor characteristics in the linear region can be further matched to within   3  by the fine adjustment of their common gate bias  with two state neurons  current source connections may be used by operating the transistor in the saturation mode  provided the voltage across the channel is greater than lrb vg  vth rrb  the transistor behaves almost as a constant current source with the saturation current given approximately byb ion  k lrb w l rrb lrb vg  vth rrb 2  with the appropriate selection of l  w  and vg  it is possible to obtain on state currents which vary by two orders of magnitude in values  figure 3 shows a set of measured i v curves for a nmosfet with the channel dimensions  l  244 microns and w  12 microns and applied gate voltages from 2 to 4 5 volts  to ensure constant current source operation  the neuron s on state output should be greater than 3 5 volts  a consistency of the on state currents to within   5 percent has similarly been observed in a set of chip samples  with current source connections therefore  quantized grey scale synapses with up to 16 grey levels lrb 4 bits rrb can be realized using a network of binary weighted current sources  figure 3  i v characteristics of an nmosfet connection element  l  244 urn  channel dimension  w  12um for proper operation of the nmosfet connections  the analog output lines lrb to neuron inputs rrb should always be held close to ground potential  moreover  the voltages at the analog input lines must be at or above ground potential  since the current normally 568 flows from the analog input to the output  the nmosfet s may be used as either all excitatory or inhibitory type connections  however  the complementary connection function can be realized using long for a pmosfet channel pmosfet s in series with pmosfet switches  connection  the voltage of an analog input line would be at or below ground  furthermore  due to the difference in the mobilites of electrons and holes in the channel  a pmosfet used as a resistive connection has a channel resistance about twice as large as an nmosfet with the same channel dimension  this fact results in a subtantial reduction in the size of pmosfet needed  thin film resistor connections the use of mosfet s as connection elements in a cmos synaptic matrix chip has the major advantage that the complete device can be readily fabricated in a conventional cmos production run  however  the main disadvantages are the large area lrb required for the long channel rrb for the mosfet s connections and their non symmetrical inhibitory excitatory functional characteristics  the large overall gate area not only substantially limits the number of synapses that can be fabricated on a single chip  but the transistors are more susceptible to processing defects which can lead to excessive gate leakage and thus reduce chip yield considerably  an alternate approach is simply to use resistors in place of mosfet s  we have investigated one such approach where thin film resistors are deposited on top of the passivation layer of cmos processed chips as an additional special processing step to the normal cmos fabrication run  with an appropriate choice of resistive materials  a dense array of resistive connections with highly uniform resistance of up to 10 m ohms appears feasible  several candidate materials  including a cermet based on platinum aluminum oxide  and amorphous semiconductor metal alloys such as a ge  cu and a ge  al  have been examined for their applicability as thin film resistor connections  these materials are of particular interest since their resistivity can easily be tailored in the desired semiconducting range of 1 10 ohm cm by controlling the metal content   the a ge metal films are deposited by thermal evaporation of presynthesized alloys of the desired composition in high vacuum  whereas platinum aluminum oxide films are deposited by co sputtering from platinum and aluminum oxide targets in a high purity argon and oxygen gas mixture  room temperature resistivities in the 0 1 to 100 ohm cm range have been obtained by varying the metal content in these materials  other factors which would also determine their suitability include their device processing and material compatibilities and their stability with time  temperature  and extended application of normal operating electric current  the temperature coefficient of resistance lrb tcr rrb of these materials at room temperature has been measured to be in the 2000 to 6000 ppm range  because of their relatively high tcr s  the need for weak connections to reduce the effect of localized heating is especially important here  the a ge metal alloy films are observed to be relatively stable with exposure to air for temperatures below 130o c 569 the platinum aluminum oxide film stabilize with time after annealing in air for several hours at 130o c sample test arrays of thin film resistors based on the described materials have been fabricated to test their consistency  the resistors  with a nominal resistance of 1 m ohm  were deposited on a glass substrate in a 40x40 array over a o 4cm by o 4cm area  variation in the measured resistance in these test arrays has been found to be from    2 5 percent for all three materials  smaller test arrays of a ge  cu thin film resistors on cmos test chips have also been fabricated  a photo micrograph of a cmos synaptic test chip containing a 4x4 array of a ge  cu thin film resistors is shown in fig 4  windows in the passivation layer of silicon nitride lrb sin rrb were opened in the final processing step of a normal cmos fabrication run to provide access to the aluminum metal for electrical contacts  a layer of resistive material was deposited and patterned by lift off  a layer of buffer metal of platinum or nickel was then deposited by rf sputtering and also patterned by lift off  the buffer metal pads serve as a conducting bridges for connecting the aluminum electrodes to the thin film resistors  in addition to providing a reliable ohmic contact to the aluminum and resistor  it also provides conformal step coverage over the silicon nitride window edge  the resistor elements on the test chip are 100 micron long  10 micron wide with a thickness of about 1500 angstroms and a nominal resistance of 250 k ohms  resistance variations from 10 20 percent have been observed in several such test arrays  the unusually large variation is largely due to the surface roughness of the chip passivation layer  as one possible solution  a thin spin  figure 4  photomicrographs of  a cmos synaptic test chip with a 4x4 array of a ge  cu thin film resistors  the nominal resistance was 250 k ohms  570 on coating of an insulating material such as polyimide to smooth out the surface of the passivation layer prior to depositing the resistors is under investigation  synaptic chip based 32 neuron breadboard system a 32 neuron breadboard system utilizing an array of discrete neuron electronics has been fabricated to evaluate the operation of 32x32 binary synaptic cmos chips with nmosfet connection elements  each neuron consists of an operational amplifier configured as a current to voltage converter lrb with virtual ground input rrb followed by a fixed gain voltage difference amplifier  the overall time constant of the neurons is approximately 10 microseconds  the neuron array is interfaced directly to the synaptic chip in a full feedback configuration  the system also contains prompt electronics consisting of a programmable array of rc discharging circuits with a relaxation time of approximately 5 microseconds  the prompt hardware allows the neuron states to be initialized by precharging the selected capacitors in the rc circuits  a microcomputer interfaced to the breadboard system is used for programming the synaptic matrix chip  controlling the prompt electronics  and reading the neuron outputs  the stability of the breadboard system is tested in an associative mellory feedback configuration  b  a dozen random dilutecoded binary vectors are stored using the following simplified outer product storage scheme   s s if l vi vj  0 1 ti j  f 10 s otherwise  in this scheme  the feedback matrix consists of only inhibitory lrb 1lor open lrb 0 rrb connections  the neurons are set to be normally on and are driven off when inhibited by another neuron via the feedback matrix  the system exhibits excellent stability and associative recall performance  convergence to a nearest stored memory in hamming distance is always observed for any given input cue  figure 5 shows some typical neuron output traces for a given test prompt and a set of stored memories  the top traces show the response of two neurons that are initially set on  the bottom traces for two other neurons initially set off  convergence times of 10 50 microseconds have been observed  depending on the prompt conditions  but are primarily governed by the speed of the neurons  conclusions synaptic cmos chips containing 1024 programmable binary synapses in a 32x32 array have been designed  fabricated  and tested  these synaptic chips are designed to serve as basic building blocks for large multi chip synaptic networks  the use of long channel mosfet s as either resistive or current source connection elements meets the  weak  connection and consistency 571 figure 5  typical neuron response curves for lrb horiz scale  10 microseconds per div rrb a test prompt input  requirements  alternately  cmos based synaptic test chips with specially deposited thin film high valued resistors  in series with fet switches  offer an attractive approach to high density programmable synaptic chips  a 32 neuron breadboard system incorporating a 32x32 nmosfet synaptic chip and a feedback configuration exhibits excellent stability and associative recall performance as an associative memory  using discrete neuron array  convergence times of 10 50 microseconds have been demonstrated  with optimization of the input output wiring layout and the use of high speed neuron electronics  convergence times can certainly be reduced to less than a microsecond  acknowledgements this work was performed by the jet propulsion laboratory  california institute of technology  and was sponsored by the joint tactical fusion program office  through an agreement with the national aeronautics and space administration  the authors would like to thank john lambe for his invaluable suggestions  t duong for his assistance in the breadboard hardware development  j lamb and s thakoor for their help in the thin film resistor deposition  and r nixon and s chang for their assistance in the chip layout design  references 1  2  3  4  5  j lambe  a moopenn  and a p thakoor  proc  aiaa acm nasa  ieee computers in aerospace v  160 lrb 1985 rrb a p thakoor  j l lamb  a moopenn  and s k khanna  mrs proc  95  627 lrb 1987 rrb w hubbard  d schwartz  j denker  h p graf  r howard  l jackel  b straughn  and d tennant  aip conf  proc  151  227 lrb 1986 rrb m a sivilotti  m r emerling  and c mead  aip conf  proc  151  408 lrb 1986 rrb j p sage  k thompson  and r s withers  aip conf  proc  151  572 6  7  8  9  381 3 3  3 3  s m ley  3  l sci  lrb 19861 hopfield  proc  nat  acad  sci  81  3088 lrb 1984 rrb hopfield  proc  nat  acad  sci  79  2554 lrb 1982 rrb sze   semiconductor devices physics and technology   lrb winew york  1985 rrb p 205 lamb  a p thakoor  a moopenn  and s k khanna  3  vac  tech  a 5 lrb 4 rrb  1407 lrb 1987 rrb
19 en 474 optimizanon with artificial neural network systems  a mapping principle and a comparison to gradient based methods t harrison monfook leong research institute for advanced computer science nasa ames research center 230 5 moffett field  ca  94035 abstract general formulae for mapping optimization problems into systems of ordinary differential equations associated with artificial neural networks are presented  a comparison is made to optimization using gradient search methods  the perfonnance measure is the settling time from an initial state to a target state  a simple analytical example illustrates a situation where dynamical systems representing artificial neural network methods would settle faster than those representing gradientsearch  settling time was investigated for a more complicated optimization problem using computer simulations  the problem was a simplified version of a problem in medical imaging  determining loci of cerebral activity from electromagnetic measurements at the scalp  the simulations showed that gradient based systems typically settled 50 to 100 times faster than systems based on current neural network optimization methods  introduction solving optimization problems with systems of equations based on neurobiological principles has recently received a great deal of attention  much of this interest began when an artificial neural network was devised to find near optimal solutions to an np complete problem 13  since then  a number of problems have been mapped into the same artificial neural network and variations of it 10 13 14 17 18 19 21 23 24  in this paper  a unifying principle underlying these mappings is derived for systems of first to nth  order ordinary differential equations  this mapping principle bears similarity to the mathematical tools used to generate optimization methods based on the gradient  in view of this  it seemed important to compare the optimization efficiency of dynamical systems constructed by the neural network mapping principle with dynamical systems constructed from the gradient   the principle this paper concerns itself with networks of computational units having a state variable v  a function  that describes how a unit is driven by inputs  a linear ordinary differential operator with constant coefficients d lrb v rrb that describes the dynamical response of each unit  and a function g that describes how the output of a computational unit is detennined from its state v  in particular  the paper explores how outputs of the computational units evolve with time in tenns of a scalar function e  a single state variable for the whole network  fig  i summarizes the relationships between variables  functions  and operators associated with each computational unit  eq  lrb 1 rrb summarizes the equations of motion for a network composed of such units  d    lrb m rrb lrb v rrb  1 lrb g 1 lrb v i rrb    gn lrb vn rrb rrb lrb i rrb where the i th element of jj lrb m rrb is d lrb m rrb lrb vj rrb  superscript lrb m rrb denotes that operator d is mth order  the i th element of is  i lrb gl lrb vi rrb    gn lrb vn   and the network is comprised of n computational units  the network of hopfield 12 has m  i  functions are weighted linear sums  and functions 1 lrb where the ith element of 1 is gj lrb vj rrb rrb are all the same sigmoid function  we will examine two ways of defining functions given a function f along with these definitions will be 1 1 1 t work supported by nasa cooperative agreement no  ncc 2 408  american institute of physics 1988 475 defined corresponding functions e that will be used to describe the dynamics of eq  lrb 1 rrb  the first method corresponds to optimization methods introduced by artificial neural network research  it will be referred to as method v y lrb  dell gil rrb    vyf lrb 2a rrb with associated e function tn lsb dv  lrb s rrb jdg  lrb s rrb e  j  f lrb  g rrb  jl d lrb m rrb lrb v  lrb s     ds  i dt  dt lrb 2b rrb here  v xr denotes the gradient of h  where partials are taken with respect to variables of x  and e7 denotes the e function associated with gradient operator v7  with appropriate operator d and and g  is simply the  energy function  of hopfield 12  note that eq  lrb 2a rrb makes functions that can be derived from scalar potential functions  explicit that we will only be concerned with for example  this restriction excludes artificial neural networks that have connections between excitatory and inhibitory units such as that of freeman 8  the second method corresponds to optimization methods based on the gradient  it will be referred to as method v if lrb  dell v  rrb  1 er 1 1  vyof lrb 3a rrb with associated e function ev  n lsb dv  lrb s rrb 1 dv  lrb s rrb  fcg rrb  jl d lrb m rrb lrb v  lrb s     i dt dt t ds lrb 3b rrb i where notation is analogous to that for eqs  lrb 2 rrb  computational unit i     the critical result that allows us to map   optimization problems into transform that detennines unit i s networks described by eq  output from state variable vi lrb 1 rrb is that conditions on the constituents of the equation differential operator specifying the can be chosen so that along dynamical characteristics of unit i any solution trajectory  the e function corresponding function governing how inputs to to the system will be a unit i are combined to drive it monotonic function of time  for method v  j here are  the conditions  all functions g are 1 rrb differentiable  gl lrb v 1 rrb  tg2 lrb v  z rrb i  and 2 rrb monotonic in the same sense  only the first figure 1  schematic of a computational unit i from which netcondition is needed to works considered in this paper are constructed  triangles suggest make a similar assertion for connections between computational units  method vv  when these conditions are met and when solutions of eq  lrb 1 rrb exist  the dynamical systems can be used for optimization  the appendix contains proofs for the monotonicity of function e along solution trajectories and references necessary existence theorems  in conclusion  mapping optimization problems onto dynamical systems summarized by eq  lrb l rrb can be reduced to a matter of differentiation if a scalar function representation of the problem can be found and the integrals of eqs  lrb 2b rrb and lrb 3b rrb are ignorable  this last assumption is certainly upheld for the case where operator d has no derivatives less than m  h order  in simulations below  it will be observed to hold for the case m  1 with a nonzero o  h order derivative in d  lrb also see lapedes and falber 19  rrb perspectives of recent work 476 the fonnulations above can be used to classify the neural network optimization techniques used in several recent studies  in these studies  the functions 1 were all identical  for the most part  following hopfield s fonnulation  researchers 10 13 14 17 23 24 have used method vy to derive with ey quadratic in functions 1 and fonns of eq  lrb 1 rrb that exhibit the ability to find extrema of all functions 1 describable by sigmoid functions such as tanh lrb x rrb  however  several researchers have written about artificial neural networks associated with non quadratic e functions  method vy has been used to derive systems capable of finding extrema of non quadrntic ey 19  method vv has been used to derive systems capable of optimizing ev where ev were not necessarily quadratic in variables v 21  a sort of hybrid of the two methods was used by jeffery and rosner 18 to find extrema of functions that were not quadratic  the important distinction is that their functions j were derived from a given function fusing eq  lrb 3a rrb where  in addition  a sign definite diagonal matrix was introduced  the left side of eq  lrb 3a rrb was left multiplied by this matrix  a perspective on the relationship between all three methods to construct dynamical systems for optimization is summarized by eq  lrb 4 rrb which describes the relationship between methods vyand vyo  e t v    liag lsb a    ll l v  j lrb 4 rrb where diag lsb xi rsb is a diagonal matrix with xi as the diagonal element of row i lrb a similar equation has been derived for quadratic f s rrb the relationship between the method of jeffery and rosner and vv is simply eq  lrb 4 rrb with the time dependent diagonal matrix replaced by a constant diagonal matrix of free parameters  it is noted that jeffery and rosner presented timing results that compared simulated annealing  conjugate gradient  and artificial neural network methods for optimization  their results are not comparable to the results reported below since they used computation time as a perfonnance measure  not settling times of analog systems  the perspective provided by eq  lrb 4 rrb will be useful for anticipating the relative performance of methods v  and vv in the analytical example below and will aid in understanding the results of computer simulations  comparison of methods vt and vv when m  1 and operator d has no ofh order derivatives  method vv is the basis of gradientsearch methods of optimization  given the long history of of such methods  it is important to know what possible benefits could be achieved by the relatively ne  w optimization scheme  method vy  in the following  the optimization efficiency of methods vt and vv is compared by comparing settling times  the time required for dynamical systems described by eq  lrb 1 rrb to traverse a continuous path to local optima  to qualify this perfonnance measure  this study anticipates application to the creation of analog devices that would instantiate eq  lrb 1 rrb  hence  we are not interested in estimating the number of discrete steps that would be required to find local optima  an appropriate performance measure if the point was to develop new numerical methods  an analytical example will serve to illustrate the possibility of improvements in settling time by using method vt instead of method vv  computer simulations will be reported for more complicated problems following this example  for the analytical example  we will examine the case where all functions 1 are identical and g lrb v rrb  tanhg lrb v  th rrb lrb 5 rrb where g  0 is the gain and th is the threshold  transforms similar to this are widely used in artificial neural network research  suppose we wish to use such computational units to search a multi dimensional binary solution space  we note that  li    g sech 2g lrb v  th rrb dv lrb 6 rrb is near 0 at valid solution states lrb comers of a hypercube for the case of binary solution spaces rrb  we see from eq  lrb 4 rrb that near a valid solution state  a network based on method vy will allow computational units to recede from incorrect states and approach correct states comparatively faster  does 477 this imply faster settling time for method v  t  to obtain an analytical comparison of settling times  consider the case where m  1 and operator d has no om order derivatives and f 1  2   j  lrb    lrb tanhgv  rrb lrb tanhgv  rrb  j lrb 7 rrb  oj where matrix s is symmetric  method vy gives network equations dv  stanhgv lrb 8 rrb   diag lsb g sech 2gvj 1s tanhgv lrb 9 rrb dt and method vv gives network equations where tanhgy denotes a vector with i   component tanhgv   for method vr there is one stable point  i e where     0  at v  o  for method vv the stable points are v  0 and v  v where v is the set of vectors with component values that are either   or   further trivialization allows for comparing estimates of settling times  suppose s is diagonal  for this case  if vj  0 is on the trajectory of any computational unit i for one method  vj 0 is on the trajectory of that unit for the other method  hence  a comparison of settling times can be obtained by comparing time estimates for a computational unit to evolve from near 0 to near an extremum or  equivalently  the converse  specifically  let the interval be lsb bo  i a rsb where 0  bo  l a and o  a  1  for method v   integrating velocity over time gives the estimate  1 lsb 1  2 lsb 1 1 1  lsb 1 a 5 lrb 2 5 rrb  l aj  5 lrb 2 a rrb  00 lj t vi  g in lrb 10 rrb and for method v y the estimate is t      ln lsb    rrb  l lrb 11 rrb from these estimates  method vv will always take longer to satisfy the criterion for convergence  note that only with the largest value for bo  bo  1 5  is the first term of eq  lrb 10 rrb zero  for any smaller bo  this term is positive  unfortunately  this simple analysis can not be generalized to nondiagonal s with diagonal s  all computational units operate independently  hence  the derivation of    is irrelevant with respect to convergence rates  convergence rate depends only on the diagonal element of s having the smallest magnitude  in this sense  the problem is one dimensional  but for non diagonal s  the problem would be  in general  multi dimensional and  hence  the direction of    becomes relevant to compare settling times for non diagonal s  computer simulations were done   these are described below  computer simulanons methods the problem chosen for study was a much simplified version of a problem in medical imaging  given electromagnetic field measurements taken from the human scalp  identify the location and magnitude of cerebral activity giving rise to the fields  this problem has received much attention in the last 20 years 3 6 7  the problem  sufficient for our purposes here  was reduced to the following problem  given a few samples of the electric potential field at the surface of a spherical conductor within which reside several static electric dipoles  identify the dipole locations and moments  for this situation  there is a closed form solution for electric potential fields at the 478 spherical surface  lrb 12 rrb where  is the electric potential at the spherical conductor surface   xsamp   is the location of the sample point lrb x denotes a vector  i the corresponding unit vector  and x the corresponding vector magnitude rrb  j1  is the dipole moment of dipole i  and d  is the vector from dipole i to x  ampl  lrb this equation can be derived from one derived by brody  terry  and ideker 4 rrb  fig 2 facilitates picturing these relationships  with this analytical solution  the problem was formulated as a least squares minimization problem where the variables were dipole moments  in short  the following process was used  a dipole model was chosen  this model was used with eq  lrb 12 rrb to calculate potentials at points on a sphere which covered about 60  of the surface  a cluster of internal locations that encompassed the locations of the model was specified  the two optimization techniques were then required to determine dipole moment values at cluster locations such that the collection of dipoles at cluster locations accufigure 2  vectors of eq  lrb 12 rrb  rately reflected the dipole distribution specified by the model  this was to be done given only the potential values at the sample points and an initial guess of dipole moments at cluster locations  the optimization systems were to accomplish the task by minimizing the sum of squared differences between potentials calculated using the dipole model and potentials calculated using a guess of dipole moments at cluster locations where the sum is taken over all sample points  further simplifications of the problem included 1 rrb choosing the dipole model locations to correspond exactly to various locations of the cluster  2 rrb requiring dipole model moments to be i  0  or  i  and 3 rrb representing dipole moments at cluster locations with two bit binary numbers  to describe the dynamical systems used  it suffices to specify operator d and functions  lrb of eq  lrb 1 rrb and function f used in eqs  lrb 2a rrb and lrb 3a rrb  operator d was d  d dt  1  lrb 13 rrb eq  lrb 5 rrb with a multiplicative factor of 112 was used for all functions  lrb  hence  regarding simplification 3 rrb above  each cluster location was associated with two computational units  considering simplification 2 rrb above  dipole moment magnitude 1 would be represented by both computational units being in the high state  for  i  both in the low state  and for 0  one in the high state and one in the low state  regarding function f  f   all samp   poims s lsb  lmaslll   d lrb x rrb   ilcillomr lrb  xs rrb r  c  g lrb v rrb 2 lrb 14 rrb all compu  arioflal u  irs j where   as    d is calculated from the dipole model and eq  lrb 12 rrb lrb the subscript measured is used because the role of the dipole model is to simulate electric potentials that would be measured in a real world situation  in real world situations  we do not know the source distribution underlying   asar  d  rrb  c is an experimentally detennined constant lrb 002 was used rrb  and  cljis   r is eq  lrb 12 rrb where the sum of eq  lrb 12 rrb is taken over all cluster locations and the k  h coordinate of the i  h cluster location dipole moment is  pi     all bits b g lrb vil  b rrb  lrb 15 rrb 479 index j of eq  lrb 14 rrb corresponds to one combination of indices ikb  sample points  100 of them  were scattered semi uniformly over the spherical surface emphasized by horizontal shading in fig 3  ouster locations  11  and model dipoles  5  were scattered within the subset of the sphere emphasized by vertical shading  for the dipole model used  10 dipole moment components were non zero  hence  optimization techniques needed to hold 56 dipole moment components at zero and set 10 components to correct non zero values in order to correctly identify the dipole model underlying   qs    d the dynamical systems corresponding to 0 8 methods v   and vv  were integrated using the relative radii forward euler method lrb e g press  flannery  i i teukolsky  and vetterling 22 rrb  numerical   i i methods were observed to be convergent experi imentally  settling time and path length were i  observed to asymtotically approach stable i i values as step size of the numerical integrator i i was decreased over two orders of magnitude  settling times  path lengths  and relative directions of travel were calculated for the two optimization methods using several different initial bit patterns at the cluster locations  in figure 3  illustration of the distribution of other words  the search was started at different sample points on the surface of the sphericorners of the hypercube comprising the space cll conductor lrb horizontal shading rrb and the of acceptable solutions  one corner of the distribution of model dipole locations and hypercube was chosen to be the target solution  cluster locations within the conductor lrb note that a zero dipole moment has a degen lrb verticll shading rrb  erate two bit representation in the dynamical systems explored  the target corner was arbitrarily chosen to be one of the degenerate solutions  rrb note from eq  lrb 5 rrb that for the network to reach a hypercube corner  all elements of would have to be singular  for this reason  settling time and other measures were studied as a function of the proximity of the computational units to their extremum states  computations were done on a sequent balance  i  i i  i v 5 results graph 1 shows results for exploring settling time as a function of extremum depth  the minimum of the deviations of variables from the threshold of functions g extremum depth is reported in multiples of the width of functions g  the term transition  used in the caption of graph 1 and below  refers to the movement of a computational unit from one extremum state to the other  the calculations were done for two initial states  one where the output of 1 computational unit was set to zero and one where outputs of 13 computational units were set to zero  bence  1 and 13  respectively  half transitions were required to reach the target hypercube comer  it can be observed that settling time increases faster for method v v  than that for method vy just as we would expect from considering eqs  lrb 4 rrb and lrb 5 rrb  however  it can be observed that method vv is still an order of magnitude faster even wben extremum depth is 3 widths of functions g for the purpose of unambiguously identifying what hypercube corner the dynamical system settles v 1 4 3 i   i i     t   o o   2 extremum depth 1       4 3 graph 1  settling time as a function of extremum depth    method vr  1 half transition required    method v 13 half transitions required    method v  1 half transition required    v  13 half transitions required  r 480 to  this extremum depth is more than adequate  table 1 displays results for various initial conditions  angles are reported in degrees  these measures refer to the angle between directions of travel in v space as specified by the two optimization methods  the average angle reported is taken over all trajectory points visited by the numerical integrator  initial angle is the angle at the beginning of the path  parasite cost percentage is a measure that compares parasite cost  the integral in eqs  lrb 2b rrb and lrb 3b rrb  to the range of function f over the path   parasite cost  parasite cost  100x i f f i fi       udal transitions reauired time 1 0 16 0 0016 100 6 1 1 9 2 0 14 0 0018 78 4 7 1 9 75 3 0 15 0 0021 71 4 7 2 1 74 7 0 19 0 0032 59 4 6 2 4 63 10 0 17 0 0035 49 3 8 2 5 60 13 0 80 0 0074 110 9 2 3 2 39 relative path initial mean angle extremum time len2th anlzle lrb std dev rrb deoth 68 lrb 16 rrb parasite cost  76 lrb 3 8 rrb 76 lrb 3 5 rrb 2 3 2 3 0 22 0 039 72 lrb 4 3 rrb 73 lrb 4 1 rrb 2 5 2 5 0 055 0 016 71 lrb 3 7 rrb 72 lrb 3 0 rrb 2 3 2 5 0 051 0 0093 69 lrb 4 1 rrb 71 lrb 7 0 rrb 2 4 2 7 0 058 0 0033 63 lrb 2 8 rrb 64 lrb 4 7 rrb 2 5 2 8 o ooo6 lcb rrb 77 lrb 11 rrb 71 lrb 8 9 rrb 2 3 2 7 0 076 0 0028 0 030 table 1  settling time and other measurements for various required transitions  for each transition case  the upper row is for v y and the lower row is for v v  std dey denotes standard deviation  see text for definition of measurement terms and units  noting the differences in path length and angles reported  it is clear that the path taken to the target hypercube comer was quite different for the two methods  method v v settles from 1 to 2 orders of magnitude faster than method v  r and usually takes a path less than half as long  these relationships did not change significantly for different values for c of eq  lrb 14 rrb and coefficients of eq  lrb 13 rrb lrb both unity in eq  lrb 13   values used favored method vr parasite cost is consistently less significant for method v v and is quite small for both methods  to further compare the ability of the optimization methods to solve the brain imaging problem  a large variety of initial hypercube comers were tested  table 2 displays results that suggest the ability of each method to locate the target comer or to converge to a solution that was consistent with the dipole model  initial comers were chosen by randomly selecting a number of computational units and setting them to exti  emwn states opposite to that required by the target solution  five cases were run for each case of required transitions  it can be observed that the system based on method vv is better at finding the target comer and is much better at finding a solution that is consistent with the dipole model  discussion the simulation results seem to contradict settling time predictions of the second analytical example  it is intuitively clear that there is no contradiction when considering the analytical example as a one dimensional search and the simulations as multi dimensional searches  consider fig 4 which illustrates one dimensional search starting at point i  since both optimization methods must decrease function e monotonically  both must head along the same path to the minimum point a  now consider fig 5 which illustrates a two dimensional search starting at point i  here  the two methods need n t follow the same paths  the two dashed paths suggest that method v  can still be 481 v  transitions i required 3 4 5 i 6 vv  erent dipole different target different dipole different target comer comer solution comer comer  solution 1 4 0 5 0 0 4 1 1 1 0 3 4 4 1 1 0 0 4 1 1 2 0 2 1 4 4 1 0 0 3 1 1 5 0 0 5 5 0 0 0 i 0 2 3 0 5 0 0 i i i 7 13 20 26 33 5 0 40 5 5 5 0 46 i 53 i 0 0 0 0 0 0 3 3 2 4 2 i 2 3 1 0 i 0 0  0 table 2  solutions found starting from various initial conditions  five cases for each transition case  different dipole solution indicates that the system assigned non zero dipole moments at cluster locations that did not correspond to locations of the dipole model sources  different corner indicates the solution was consistent with the dipole model but was not the target hypercube comer  target corner indicates that the solution was the target solution  monotonically decreasing e while traversing a more circuitous route to minimum b or traversing a path to minimum a  the longer path lengths reported in table 1 for method v  suggest the occurrence of the fonner  the data of table 2 verifies the occurrence of the latter  note that for many v cases where the system based on method vv settled to the  figure 4  one dimensional search target comer  the system based on method v  settled to some other minimum  for minima  would we observe similar differences in optimization i efficiency for other optimization problems that also have binary solution spaces  a view that supports the plausibility of the affirmative is the following  consider eq  lrb 4 rrb and eq  e lrb 5 rrb  we have already made the observation that method vv would slow convergence into extrema of functions g  we have observed this experimentally via graph 1  these observations suggest that computational units of vv systems tend to stay closer to the transition regions of functions g compared to computational units of v  i systems  it seems plausible that this property may allow vv systems to avoid advancing too deeply toward ineffective solutions and  hence  allow the systems to approach effective solutions more figure 5  two dimensional search efficiently  1bis behavior might also be the explanation for for minima  the comparative success of method vv revealed in table 2  regarding the construction of electronic circuitry to instantiate eq  lrb l rrb  systems based on method vv would require the introduction of a component implementing multiplication by the derivative of functions g  this additional complexity may binder the use of method vv for the 482 construction of analog circuits for optimization  to illustrate the extent of this additional complexity  fig  input 6a shows a schematized circuit for a computational unit of method v r and fig 6b shows a schematized circuit for a computational unit of method vt the simulations reported above suggest that there may be problems for which improvements in settling time output may offset complications that might come with added circuit complexity  on the problem of imaging cerebral activity  the results above suggest the possibility of constructing analog devices to do the job  consider the problem of analyzing electric potentials from the scalp of one peroutput son  it is noted that the measured electric potentials  figure 6  schematized circuits for a com    as  rcd  appear as linear coefficients in f of eq  putational unit notation is consistem lrb 14 rrb  hence  they would appear as constant terms in with horowitz and hill is  shading of of eq  lrb 1 rrb  thus  cf rrb  asllrcd would be implemented as amplifiers is to e3iillark components amplifier biases in the circuits of figs 6  this is a referred to in the text  a rrb computational significant benefit  to understand this  note that funcunit for method vr b rrb computational tion ij of fig 1 corresponding to the optimization of  ti thod v function f of eq  lrb 14 rrb would involve a weighted umt or me  linear sum of inputs g 1 lrb v 1 rrb    gn lrb vn rrb  the weights would be the nonlinear coefficients of eq  lrb 14 rrb and correspond to the strengths of the connections shown in fig 1  these connection strengths need only be calculated once for the person ar d car  then be set in hardware using  for example  a resistor network  electric potential measurements could then be ar alyzed by simply using the measurements to bias the input to shaded amplifiers of figs 6  for initialization  the system can be initialized with all dipole moments at zero lrb the 10 transition case in table 1 rrb  this is a reasonable first guess if it is assumed that cluster locations are far denser than the loci of cerebral activity to be observed  for subsequent measurements  the solution for immediately preceding measurements would be a reasonable initial state if it is assumed that cerebral activity of interest waxes and wanes continuously  might non invasive real time imaging of cerebral activity be possible using such optimization devices  results of this study are far from adequate for answering this question  many complexities that have been avoided may nullify the practicality of the idea  among these problems are  1 rrb the experiment avoided the possibility of dipole sources actually occurring at locations other than cluster locations  the minimization of function f of eq  lrb 14 rrb may circumvent this problem by employing the superposition of dipole moments at neighboring cluster locations to give a sufficient model in the mear   2 rrb the experiment asswned a very restricted range of dipole strengths  this might be dealt with by increasing the number of bits used to represent dipole moments  3 rrb the conductor model  a homogeneously conducting sphere  may not be sufficient to model the hwnan head 16  non sphericity ar d major inhomogeneities in conductivity car  be dealt with  to a certain extent  by replacing eq  lrb 12 rrb with a generalized equation based on a numerical approximation of a boundary integral equation 20 4 rrb the cerebral activity of interest may not be observable at the scalp  5 rrb not all forms of cerebral activity give rise to dipolar sources  lrb for example  this is well known in olfactory cortex 8  rrb 6 rrb activity of interest may be overwhelmed by irrelevant activity  many methods have been devised to contend with this problem lrb for example  gevins and morgan 9  rrb clearly  much theoretical work is left to be done  lrb a rrb lrb b rrb 1 concluding remarks 483 in this study  the mapping principle underlying the application of artificial neural networks to the optimization of multi dimensional scalar functions has been stated explicitly  hopfield 12 has shown that for some scalar functions  i e functions f quadratic in functions 1  this mapping can lead to dynamical systems that can be easily implemented in hardware  notably  hardware that requires electronic components common to semiconductor technology  here  mapping principles that have been known for a considerably longer period of time  those underlying gradient based optimization  have been shown capable of leading to dynamical systems that can also be implemented using semiconductor hardware  a problem in medical imaging which requires the search of a multi dimensional surface full of local extrema has suggested the superiority of the latter mapping principle with respect to settling time of the corresponding dynamical system  1bis advantage may be quite significant when searching for global extrema using techniques such as iterated descent 2 or iterated genetic hill climbing 1 where many searches for local extrema are required  this advantage is further emphasized by the brain imaging problem  volumes of measurements can be analyzed without reconfiguring the interconnections between computational units  hence  the cost of developing problem specific hardware for finding local extrema may be justifiable  finally  simulations have contributed plausibility to a possible scheme for non invasively imaging cerebral activity  appendix to show that for a dynamical system based on method vr e   is a monotonic function of time given that all functions g are differentiable and monotonic in the same sense  we need to show that the derivative of et with respect to time is semi definite  det dt  n dft dg j n lsb m dvj rsb dg   l    l d lrb rrb lrb vj rrb    j dgj dt i dt lrb ala rrb dt substituting eq  lrb 2a rrb   det  dt n lsb i  f  dv  rsb dg   dt dt lrb alb rrb  d lrb m rrb lrb v  rrb       j  using eq  lrb 1 rrb  d   n lsb dv i dt   dt rsb 2 dgi  o lrb alc rrb av   s as needed  the appropriate inequality depends on the sense in which functions 1 are monotonic  in a similar manner  the result can be obtained for method vv   with the condition that functions 1 are differentiable  we can show that the derivative of 4 is semi definite  de    dt  v dv  n lsb dv  rsb dv  v  in  df   i  d lrb m rrb lrb vj rrb       j dvj dt j dt lrb a2a rrb dt using eqs  lrb 3a rrb and lrb 1 rrb  dev n lsb dvj dt    dt    rsb 2  0 lrb a2b rrb s as needed  in order to use the results derived above to conclude that eq  lrb 1 rrb can be used for optimization of functions 4 and et in the vicinity of some point we need to show that there exists a neighborhood of vo in which there exist solution trajectories to eq  lrb 1 rrb  the necessary existence theorems and transformations of eq  lrb 1 rrb needed in order to apply the theorems can be found in many texts on ordinary differential equations  e g guckenheimer and holmes 11  here  it is mainly important to state that the theorems require that functions   c lrb 1 rrb  functions g are differentiable  and initial conditions are specified for all derivatives of lower order than m vo  484 acknowledgements i would like to thank dr michael raugh and dr pentti kanerva for constructive criticism and support  i would like to thank bill baird and dr james keeler for reviewing this work  i would like to thank dr derek fender  dr john hopfield  and dr stanley klein for giving me opportunities that fostered this conglomeration of ideas  lsb 1 rsb lsb 2 rsb lsb 3 rsb lsb 4 rsb lsb 5 rsb lsb 6 rsb lsb 7 rsb lsb 8 rsb lsb 9 rsb lsb 10 rsb lsb 11 rsb lsb 12 rsb lsb 13 rsb lsb 14 rsb lsb 15 rsb lsb 16 rsb lsb 171 lsb 18 rsb lsb 19 rsb lsb 20 rsb lsb 21 rsb lsb 22 rsb lsb 23 rsb lsb 24 rsb references ackley d h   stochastic iterated genetic bill climbing   phd  dissertation  carnegie mellon u 1987  bawn e  neural networks for computing  ed  denker 1  s lrb alp confrnc  proc  151  ed  lerner r g rrb  p53 58  1986  brody d a  ieee trans  vbme 32  n2  pl06 110  1968  brody d a  terry f h   deker re  ieee trans  vbme 20  p141 143  1973  cohen m a  grossberg s  ieee trans  vsmc 13  p815 826  1983  cuffin b n  ieee trans  vbme 33  n9  p854 861  1986  darcey t m  air j p  fender d h  prog  brain res  v54  pi28 134  1980  freeman w j   mass action in the nervous system   academic press  inc  1975  gevins a s  morgan n h  ieee trans  vbme 33  n12  pl054 1068  1986  goles e  vichniac g y  neural networks for computing  ed  denker j s lrb alp confrnc  proc  151  ed  lerner r g rrb  p165 181  1986  guckenheimer j  holmes p   nonlinear oscillations  dynamical systems  and bifurcations of vector fields   springer verlag  1983  hopfield j i  proc  nat   acad  sci  v81  p3088 3092  1984  hopfield 1 1   tank d w  bio  cybrn  v52  p141 152  1985  hopfield 1  j  tank d w  science  v233  n4764  p625 633  1986  horowitz p  hill w   the art of electronics   cambridge u press  1983  hosek rs  sances a  jodat rw  larson s i  ieee trans  vbme 25  ns  p405 413  1978  hutchinson j m  koch c  neural networks for computing  ed  denker j s lrb alp confrnc  proc  151  ed  lerner rg  rrb  p235 240  1986  jeffery w  rosner r  astrophys  i  v310  p473 481  1986  lapedes a  farber r  neural networks for computing  ed  denker 1  s lrb alp confrnc  proc  lsi  ed  lerner rg  rrb  p283 298  1986  leong h m f   frequency dependence of electromagnetic fields  models appropriate for the brain   phd  dissertation  california institute of technology  1986  platt i c  hopfield j j  neural networks for computing  ed  denker i s lrb alp confrnc  proc  151  ed  lerner rg  rrb  p364 369  1986  press w h  flannery b p  teukolsky s a  vetterling w t   numerical recipes   cambridge u press  1986  takeda m  goodman j w  applied optics  v25  n18  p3033 3046  1986  tank d w  hopfield i j   neural computation by concentrating infornation in time   preprint  1987 
20 en 31 an artificial neural network for spatiotemporal bipolar patterns  application to phoneme classification toshiteru homma les e atlas robert j marks ii interactive systems design laboratory department of electrical engineering  ff l0 university of washington seattle  washington 98195 abstract an artificial neural network is developed to recognize spatio temporal bipolar patterns associatively  the function of a formal neuron is generalized by replacing multiplication with convolution  weights with transfer functions  and thresholding with nonlinear transform following adaptation  the hebbian learning rule and the delta learning rule are generalized accordingly  resulting in the learning of weights and delays  the neural network which was first developed for spatial patterns was thus generalized for spatio temporal patterns  it was tested using a set of bipolar input patterns derived from speech signals  showing robust classification of 30 model phonemes  1  introduction learning spatio temporal lrb or dynamic rrb patterns is of prominent importance in biological systems and in artificial neural network systems as well  in biological systems  it relates to such issues as classical and operant conditioning  temporal coordination of sensorimotor systems and temporal reasoning  in artificial systems  it addresses such real world tasks as robot control  speech recognition  dynamic image processing  moving target detection by sonars or radars  eeg diagnosis  and seismic signal processing  most of the processing elements used in neural network models for practical applications have been the formal neuron l or  its variations  these elements lack a memory flexible to temporal patterns  thus limiting most of the neural network models previously proposed to problems of spatial lrb or static rrb patterns  some past solutions have been to convert the dynamic problems to static ones using buffer lrb or storage rrb neurons  or using a layered network with without feedback  we propose in this paper to use a  dynamic formal neuron  as a processing element for learning dynamic patterns  the operation of the dynamic neuron is a temporal generalization of the formal neuron  as shown in the paper  the generalization is straightforward when the activation part of neuron operation is expressed in the frequency domain  many of the existing learning rules for static patterns can be easily generalized for dynamic patterns accordingly  we show some examples of applying these neural networks to classifying 30 model phonemes   american institute of physics 1988 32 2  formal neuron and dynamic formal neuron the formal neuron is schematically drawn in fig  l lrb a rrb  where r  lsb xl xz  xd 1 yi  i  1 2    n zi  i  1 2   n input activation output transmittance node operator neuron operation w  lsb wil wiz  wilf 11 where 11 lrb  rrb is a nonlinear memory less transform zi  11 lrb wtr  lrb 2 1 rrb note that a threshold can be implicitly included as a transmittance from a constant input  in its original form of formal neuron  xi e lcb o  i rcb and 110 is a unit step function u lrb  rrb  a variation of it is a bipolar formal neuron where xi e lcb  i  i rcb and 110 is the sign function sgn o  when the inputs and output are converted to frequency of spikes  it may be expressed as xi e rand 110 is a rectifying function ro  other node operators such as a sigmoidal function may be used  we generalize the notion of formal neuron so that the input and output are functions of time  in doing so  weights are replaced with transfer functions  multiplication with convolution  and the node operator with a nonlinear transform following adaptation as often observed in biological systems  fig 1 lrb b rrb shows a schematic diagram of a dynamic formal neuron where r lrb l rrb  lsb xl lrb t rrb xz lrb t rrb  xdt rrb f yi lrb t rrb  i  1 2    n zi lrb t rrb  i  1 2    n w lrb t rrb  lsb wjl lrb t rrb wiz lrb t rrb  wil lrb t rrb rsb t ai lrb t rrb input activation output transfer function adaptation node operator neuron operation 1l where 110 is a nonlinear memoryless transform zj lrb t rrb  ll lrb ai lrb  t rrb  w  lrb t rrb t  x lrb t  lrb 2 2 rrb for convenience  we denote  as correlation instead of convolution  note that convolving a lrb t rrb with b lrb t rrb is equivalent to correlating a lrb  t rrb with b lrb t rrb  if the fourier transforms r lrb f rrb  f lcb r lrb t rrb rcb  w  lrb f rrb  f lcb w  lrb t rrb rcb  yj lrb f rrb  f lcb yi lrb t rrb rcb  and aj lrb f rrb  f lcb ai lrb t rrb rcb exist  then yi lrb f rrb  ai lrb f rrb lsb wi lrb f ft r lrb f rrb rsb lrb 2 3 rrb where wi lrb f ft is the conjugate transpose of wi lrb t rrb  x  lrb 1 rrb i  zt 1  zt lrb i rrb  lrb b rrb fig 1  formal neuron and dynamic formal neuron  33 3  learning for formal neuron and dynamic formal neuron a number of learning rules for formal neurons has been proposed in the past  in the following paragraphs  we formulate a learning problem and describe two of the existing learning rules  namely  hebbian learning and delta learning  as examples  present to the neural network m pairs of input and desired output samples k     1 2    m  in order  let w lrb k rrb     lsb w k rrb w k rrb   wjk  t where wr rrb is the transmittance vector at the k th step of learning  likewise  let lcb x  k rrb  lrb lk rrb rcb  k lrb k rrb  lsb x  i rrb x  2 rrb  x  k rrb rsb  r lrb k rrb  lrb k rrb  lsb z  i rrb z  2 rrb   k rrb rsb   ik rrb  w lrb k rrb x   k rrb  z  k rrb and  rfl rrb t 2 rrb  d lrb k rrb  lsb lrb ll rrb lrb l2 rrb t k rrb rsb    lrb lk rrb rsb  where  n  tk   and n y  lsb t1 lrb y i rrb t1 lrb y2 rrb    t1 lrb yn rrb rsb t  the hebbian learning rule 2 is described as follows   w lrb k rrb     w lrb k i rrb  a  jc k rrb x  k rrb t lrb 3 1 rrb the delta learning lrb or lms learning rrb rule 3  4 is described as follows  w lrb k rrb  w lrb k i rrb  o lcb w lrb k l rrb t  k rrb  lrb lk rrb rcb x  k rrb t lrb 3 2 rrb the learning rules described in the previous section are generalized for the dynamic formal neuron by replacing multiplication with correlation  first  the problem is reformulated and then the generalized rules are described as follows  present to the neural network m pairs of time varing input  1 2      m  in order  let w lrb k rrb lrb t rrb  lsb wi lrb t rrb lrb k rrb lrb t rrb where w k rrb lrb t rrb is the vector whose elements w  rrb t rrb lrb t rrb are transfer functions to the neuron i at the k th step of learning  the hebbian learning rule for then lcb x  k rrb lrb t rrb  lrb lk rrb lrb t rrb rrb  k w lrb kl lrb t rrb  w lrb k i rrb lrb t rrb  0  lrb 1 rcb  lrb lk rrb lrb t rrb  x  k rrb lrb t rrb t and output samples w  k rrb lrb t rrb   wjk rrb lrb t rrb f connecting the input j the dynamic neuron is  lrb 3 3 rrb the delta learning rule for dynamic neuron is then w lrb kl lrb t rrb     w lrb k i rrb lrb t rrb  o lrb  t rrb  lcb w lrb k il lrb t rrb  x  k rrb lrb t rrb  lrb it rrb lrb t rrb rcb  x  k rrb lrb t rrb t  lrb 3 4 rrb this generalization procedure can be applied to other learning rules in some linear discriminant systems 5  the self organizing mapping system by kohonen6  the perceptron 7  the backpropagation model 3  etc  when a system includes a nonlinear operation  more careful analysis is necesssay as pointed out in the discussion section  4  delta learning  pseudo inverse and regularization this section reviews the relation of the delta learning rule to the pseudo inverse and the technique known as regularization  4  6  8  9 10 consider a minimization problem as described below  find w which minimizes r  lii ik rrb  lrb lk rrb u i   f k rrb  lrb lky  tk rrb  lrb lk  lrb 4 1 rrb subject to t k rrb  wx  k rrb  a solution by the delta rule is  using a gradient descent method  w lrb k rrb   w lrb k i rrb  o 1  r lrb k rrb aw  this interpretation assumes a strong supervising signal at the output while learning  lrb 4 2 rrb 34 where r lrb k rrb  ii y  k rrb   a  rrb 1i1  the minimum norm solution to the problem  w   is unique and can tie expressed as w   d xt lrb 4 3 rrb where   t is the moore penrose pseudo inverse of    i e  x t  lim lrb xtx  dl  rrb  lx t  limxt lrb x xt  a   o    on the condition that 0   a   o     dl  rrb  l  lrb 4 4 rrb a   where an  is the max imum eigenvalue of   t    j  k rrb and lrb jc k rrb are independent  and wcl rrb is uncorrelated with  l rrb  e lcb w  rcb  e lrb  c  rrb rcb lrb 4 5 rrb where e lcb x rcb denotes the expected value of x  one way to make use of this relation is to calculate w  for known standard data and refine it by lrb 4 2 rrb  thereby saving time in the early stage of learning  however  this solution results in an ill conditioned w often in practice  when the problem is ill posed as such  the technique known as regularization can alleviate the ill conditioning of w  the problem is reformulated by finding w which minimizes r lrb a rrb  dly  t rrb  lrb jc k rrb iil  dllii wkll 1 1 lrb 4 6 rrb k t  subject to k rrb  k rrb where w  lsb wlw2  wn rsb t  this reformulation regularizes lrb 4 3 rrb to w lrb a rrb  d   t lrb     t  a2n 1 lrb 4 7 rrb which is statistically equivalent to wc   rrb when the input has an additive noise of variance dl utlcorrelated with  l rrb  interestingly  the leaky lms algorithm ll leads to a statistically equivalent solution w lrb l rrb   wck l rrb  tx  lrb k l rrb  l rrb  whete 0    1 and 0  e lcb w lrb a rrb rcb if dl  lrb 4 8 rrb lcb jc l rrb rcb  f  l rrb t 2 a  amax  these solutions are related as  e lcb wc   rrb rcb lrb 4 9 rrb      j  when wcl rrb is uncorrelated with  f  l rrb 11 a  equation lrb 4 8 rrb can be generalized for a network using dynamic formal neurons  resulting in a equation similar to lrb 3 4 rrb  making use of lrb 4 9 rrb  lrb 4 7 rrb can be generalized for a dynamic neuron network as w lrb t  a rrb  f  1 lcb q if rrb   if ft lrb   if rrb   if rrb ct n   a2 1 rcb lrb 4 10 rrb where f 1 denotes the inverse fourier transform  s synthesis of bipolar phoneme patterns this section illustrates the scheme used to synthesize bipolar phoneme patterns and to form prototype and test patterns  the fundamental and first three formant frequencies  along with their bandwidths  of phonemes provided by klatt l2 were taken as parameters to synthesize 30 prototype phoneme patterns  the phonemes were labeled as shown in table 1  an array of l lrb  100 rrb input neurons oovered the range of 100 to 4000 hz  each neuron had a bipolar state which was 1 only when one of the frequency bands in the phoneme presented to the network was within the critical band 35 of the neuron and 1 otherwise  the center frequencies if e rrb of critical bands were obtained by dividing the 100 to 4000 hz range into a log scale by l  the critical bandwidth was a constant 100 hz up to the center frequency ie  500 hz and 0 2  e hz when ie  500 hz 13 the parameters shown in table 1 were used to construct table 1  labels of phonemes 30 prototype phoneme patterns  for 9  it was constructed as a label phoneme combination of t and 9  fl  f 2  f 3 were the first  second  and 1 lsb i y rsb third formants  and b i  b 2  and b 3  were corresponding lsb ia rsb 2 bandwidths  the fundamental frequency f 0  130 hz with b 0  3 ley rsb 10 hz was added when the phoneme was voiced  for plosives  4 lsb ea rsb there was a stop before formant traces start  the resulting bipo lsb 3e  rsb 5 lar patterns are shown in fig 2  each pattern had length of 5 6 lsb el rsb time units  composed by linearly interpolating the frequencies 7 lsb  rsb when the formant frequency was gliding  8 lsb it rsb a sequence of phonemes converted from a continuous lsb ow rsb 9 pronunciation of digits  lcb o  zero  one  two  three  four  five  six  10 lsb  i  rsb seven  eight  nine rcb  was translated into a bipolar pattern  adding 11 lsb u w rsb 12 two time units of transition between two consequtive phonemes lsb a  j 13 lsb a rsb by interpolating the frequency and bandwidth parameters lsb awl 14 linearly  a flip noise was added to the test pattern and created a 15 loy rsb noisy test pattern  the sign at every point in the original clean 16 lsb w rsb test pattern was flipped with the probability 0 2  these test pat17 lsb y rsb terns are shown in fig 3  18 lsb r rsb 19 lsb i rsb 20 lsb f rsb 21 lsb v rsb i  ildm  label i 1 5 7  ii il 15 7   ji 21 z5 17 it 2 4  i ii 11 14 16 ii ii u 14 i  ii jo 22 lsb 9 rsb ii  23 lsb  rsb 24 lsb s rsb 25 lsb z rsb 26 lsb p rsb 27 lsb t rsb 28 lsb d rsb 29 lsb k rsb 30 lsb n rsb fig 2  prototype phoneme patterns  lrb thirty phoneme patterns are shown in sequence with intervals of two time units  rrb 6  simulation of spatio temporal filters for phoneme classification the network system described below was simulated and used to classify the prototype phoneme patterns in the test patterns shown in the previoius section  it is an example of generalizing a scheme developed for static patterns 13 to that for dynamic patterns  its operation is in two stages  the first stage operation is a spatio temporal filter bank  36   z 4   e      i    i if  i     i   lu  i u  lrb b rrb lrb a rrb fig 3  test patterns  lrb a rrb clean test pattern  lrb b rrb noisy test pattern  lrb 6 1 rrb 1 lrb t rrb  w lrb t rrb  r lrb t rrb  and r lrb t rrb    lrb a lrb  t rrb y lrb t   the second stage operation is the  winner take all  lateral inhibition  lrb  lrb t rrb  zt lrb t rrb  and lrb  lrb t  a rrb   lrb  lrb  t rrb  lrb  lrb t rrb  ii rrb  lrb 6 2 rrb and a lrb t rrb  lrb 1  114   rrb  o lrb t rrb   s  fii  2  o lrb t na rrb  sn  n lrb 6 3 rrb 11  0 where ii is a constant threshold vector with elements hi  h and 0 lrb  rrb is the kronecker delta function  this operation is repeated a sufficient number of times  no 13 14 the output is lrb  lrb t  no  a rrb  two models based on different leaming rules were simulated with parameters shown below  model 1 lrb spatio temporal matched filter bank rrb let a lrb t rrb  o lrb t rrb  lrb  tk rrb  et in lrb 3 3 rrb where ek is a unit vector with its elements eki  o lrb k i rrb  lrb 6 4 rrb w lrb t rrb   lrb t rrb t 4 1 h  200  and a lrb t rrb  2 o lrb t na rrb  11  0 s model 2 lrb spatio temporal pseudo inverse filter rrb let d  l in lrb 4 10 rrb  using the alternative expression in lrb 4 4 rrb  w lrb t rrb  f  1 lcb lrb  lrb j ft  lrb j rrb  cr2n lxct rcb  h  o os  cr2  1000 0  and a lrb t rrb lrb 6 5 rrb  o lrb t rrb  this minimizes r lrb cr   rrb  di1k rrb lrb j rrb  lrb  t  rrb lrb j rrb lii  cr 22 11 k w  if rrb lii for all   lrb 6 6 rrb 37 because the time and frequency were finite and discrete in simulation  the result of the inverse discrete fourier transform in lrb 6 5 rrb may be aliased  to alleviate the aliasing  the transfer functions in the prototype matrix   lrb t rrb were padded with zeros  thereby doubling the lengths  further zero padding the transfer functions did not seem to change teh result significantly  the results are shown in fig 4 lrb a rrb  lrb d rrb  the arrows indicate the ideal response positions at the end of a phoneme  when the program was run with different thresholds and adaptation function a lrb t rrb  the result was not very sensitive to the threshold value  but was  nevertheless affected by the choice of the adaptation function  the maximum number of iterations for the lateral inhibition network to converge was observed  for the experiments shown in fig 4 lrb a rrb  lrb d rrb  the numbers were 44  69  29  and 47  respectively  model 1 missed one phoneme and falsely responded once in the clean test pattern  it missed three and had one false response in the noisy test pattern  model 2 correctly recognized all phonemes in the clean test pattern  and falsealarmed once in the noisy test pattern  7  discussion the notion of convolution or correlation used in the models presented is popular in engineering disciplines and has been applied extensively to designing filters  control systems  etc  such operations also occur in biological systems and have been applied to modeling neural networks  is 16 thus the concept of dynamic formal neuron may be helpful for the improvement of artificial neural network models as well as the understanding of biological systems  a portion of the system described by tank and hopfield 11 is similar to the matched filter bank model simulated in this paper  the matched filter bank model lrb modell rrb performs well when all phonemes lrb as above rrb are of the same duration  otherwise  it would perform poorly unless the lengths were forced to a maximum length by padding the input and transfer functions with 1  s during calculation  the pseudo inverse filter model  on the other hand  should not suffer from this problem  however  this aspect of the 11kxlel lrb model 2 rrb has not yet been explicitly simulated  given a spatio temporal pattern of size l x k  i e  l spatial elements and k temporal elements  the number of calculations required to process the first stage of filtering by both models is the same as that by a static formal neuron network in which each neuron is connected to the l x k input elements  in both cases  l x k multiplications and additions are necessary to calculate one output value  in the case of bipolar patterns  the rnutiplication used for calculation of activation can be replaced by sign bit check and addition  a future investigation is to use recursive filters or analog filters as transfer functions for faster and more efficient calculation  there are various schemes to obtain optimal recursive or analog filters t 8 19 besides the lateral inhibition scheme used in the models  there are a number of alternative procedures to realize a  winnertake all  network in analog or digital fashion  is  20  21 as pointed out in the previous section  the fourier transform in lrb 6 5 rrb requires a precaution concerning the resulting length of transfer functions  calculating the recursive correlation equation lrb 3 4 rrb also needs such preprocessing as windowing or truncation  22 the generalization of static neural networks to dynamic ones along with their learning rules is strainghtforward as shown if the neuron operation and the learning rule are linear  generalizing a system whose neuron operation and or learning rule are nonlinear requires more careful analysis and remains for future work  the system described by watrous and shastri l6 is an example of generalizing a backpropagation model  their result showed a good potential of the model and a need for more rigorous analysis of the model  generalizing a system with recurrent connections is another task to be pursued  in a system with a certain analytical nonlinearity  the signals are expressed by volterra functionals  for example  a practical learning system can then be constructed if higher kernels are neglected  for example  a cubic function can be used instead of a sigmoidal function  38 1  1 3  0  lcb   r 1   j       1  u  lcb   e lrb a rrb  z  0    t  f t 7       i i  i i i i i is  t   51 i i en time  t  l     7    1 1  e ii lrb b rrb z         1  l   j r  i u i t   i lsi tu time fig 4  performance of models  lrb a rrb modell with clean test pattern  lrb b rrb model 2 with clean test pattern  lrb c rrb modell with noisy test pattern  lrb d rrb model 2 with noisy test pattern  arrows indicate the ideal response positions at the end of phoneme  8  conclusion the formal neuron was generalized to the dynamic formal neuron to recognize spatiotemporal patterns  it is shown that existing learning rules can be generalized for dynamic formal neurons  an artificial neural network using dynamic formal neurons was applied to classifying 30 model phonemes with bipolar patterns created by using parameters of formant frequencies and their bandwidths  the model operates in two stages  in the first stage  it calculates the correlation between the input and prototype patterns stored in the transfer function matrix  and  in the second stage  a lateral inhibition network selects the output of the phoneme pattern close to the input pattern  39  lcb     3  1    j     at i  e lrb c rrb zii  c it   p  x   i i i i i t   51 u t51 nme   3   i  0     u  1   i  e ii lrb d rrb z     c  it  i i   fig 4 lrb continued  rrb two models with different transfer functions were tested  model 1 was a matched filter bank model and model 2 was a pseudo inverse filter model  a sequence of phoneme patterns corresponding to continuous pronunciation of digits was used as a test pattern  for the test pattern  modell missed to recognize one phoneme and responded falsely once while model 2 correctly recognized all the 32 phonemes in the test pattern  when the flip noise which flips the sign of the pattern with the probability 0 2  model 1 missed three phonemes and falsely responded once while model 2 recognized all the phonemes and false alarmed once  both models detected the phonerns at the correct position within the continuous stream  references 1  w s mcculloch and w pitts   a logical calculus of the ideas imminent in nervous activity   bulletin of mathematical biophysics  vol  5  pp 115 133  1943  2  d o hebb  the organization of behavior  wiley  new york  1949  40 3  d e rumelhart  g e hinton  and r j williams   learning internal representations by error propagation   in parallel distributed processing  vol  1  mit  cambridge  1986  4  b widrow and m e hoff   adaptive switching circuits   institute of radio engineers  western electronics show and convention  vol  convention record part 4  pp 96 104  1960  5  r o duda and p e hart  pattern classification and scene analysis  chapter 5  wiley  new york  1973  6  t kohonen  self organization and associative memory  springer verlag  berlin  1984  7  f rosenblatt  principles of neurodynamics  spartan books  washington  1962  8  1  m varah   a practical examination of some numerical methods for linear discrete illposed problems   siam review  vol  21  no 1  pp 100 111  1979  9  c koch  j marroquin  and a y uiile   analog neural networks in early vision   proceedings of the national academy of sciences  usa  vol  83  pp 4263 4267  1986  10  g o stone   an analysis of the delta rule and the learning of statistical associations   in parallel distributed processing   vol  1  mit  cambridge  1986  11  b widrow and s d stearns  adaptive signal processing  prentice hall  englewood cliffs  1985  12  d h klatt   software for a cascade parallel formant synthesizer   journal of acoustical society of america  vol  67  no 3  pp 971 995  1980  13  l e atlas  t homma  and r j marks ii   a neural network for vowel classification   proceedings international conference on acoustics  speech  and signal processing  1987  14  r p lippman   an introduction to computing with neural nets   ieee assp magazine  april  1987  15  s amari and m a arbib   competition and cooperation in neural nets   in systems neuroscience  ed  j metzler  pp 119 165  academic press  new york  1977  16  r l watrous and l shastri   learning acoustic features from speech data using connectionist networks   proceedings of the ninth annual conference of the cognitive science society  pp 518 530  1987  17  d tank and j j hopfield   concentrating information in time  analog neural networks with applications to speech recognition problems   proceedings of international conference on neural netoworks  san diego  1987  18  j r treichler  c r johnson  jr  and m g larimore  theory and design of adaptive filters  chapter 5  wiley  new york  1987  19  m schetzen  the volterra and wiener theories of nonlinear systems  chapter 16  wiley  new york  1980  20  s grossberg   associative and competitive principles of learning   in competition and cooperation in neural nets  ed  m a arbib  pp 295 341  springer verlag  new york  1982  21  r j marks ii  l e atlas  j j choi  s oh  k f cheung  and d c park   a performance analysis of associative memories with nonlinearities in the correlation domain   lrb submitted to applied optics rrb  1987  22  d e dudgeon and r m mersereau  multidimensional digital signal processing  pp 230 234  prentice hall  englewood cliffs  1984 
21 en 674 pa itern class degeneracy in an unrestricfed storage density memory christopher l scofield  douglas l reilly  charles elbaum  leon n cooper nestor  inc  1 richmond square  providence  rhode island  02906  abstract the study of distributed memory systems has produced a number of models which work well in limited domains  however  until recently  the application of such systems to realworld problems has been difficult because of storage limitations  and their inherent architectural lrb and for serial simulation  computational rrb complexity  recent development of memories with unrestricted storage capacity and economical feedforward architectures has opened the way to the application of such systems to complex pattern recognition problems  however  such problems are sometimes underspecified by the features which describe the environment  and thus a significant portion of the pattern environment is often non separable  we will review current work on high density memory systems and their network implementations  we will discuss a general learning algorithm for such high density memories and review its application to separable point sets  finally  we will introduce an extension of this method for learning the probability distributions of non separable point sets  introducnon information storage in distributed content addressable memories has long been the topic of intense study  early research focused on the development of correlation matrix memories 1  2  3  4  workers in the field found that memories of this sort allowed storage of a number of distinct memories no larger than the number of dimensions of the input space  further storage beyond this number caused the system to give an incorrect output for a memorized input   american institute of physics 1988 675 recent work on distributed memory systems has focused on single layer  recurrent networks  hopfield 5  6 introduced a method for the analysis of settling of activity in recurrent networks  this method defined the network as a dynamical system for which a global function called the  energy  lrb actually a liapunov function for the autonomous system describing the hopfield showed that state of the network rrb could be defined  flow in state space is always toward the fixed points of the dynamical system if the matrix of recurrent connections satisfies certain conditions  with this property  hopfield was able to define the fixed points as the sites of memories of network acti vity  like its forerunners  the hopfield network is limited in storage capacity  empirical study of the system found that for randomly chosen memories  storage capacity was limited to m  o lsn  where m is the number of memories that could be accurately recalled  and n is the dimensionality of the network lrb this has since been improved to m  n  7  8 rrb  the degradation of memory recall with increased storage density is directly related to the proliferation in the state space of unwanted local minima which serve as basins of flow  unrestricien storage density memories bachman et al 9 have studied another relaxation system similar in some respects to the hopfield network  however  in contrast to hopfield  they have focused on defining a dynamical system in which the locations of the minima are explicitly known  in particular  they have chosen a system with a liapunov function given by e   ill  qj i il  xj i  l  lrb 1 rrb j where e is the total  energy  of the describing the initial network activity and xj  the site of the jth memory  for parameter related to the network size   xj for some memory j according to network  il lrb 0 rrb is a vector caused by a test pattern  m memories in rn  l is a then 1l lrb 0 rrb relaxes to il lrb t rrb 676 lrb 2 rrb this system is isomorphic to the classical electrostatic potential between a positive lrb unit rrb test charge  and negative charges qj at the sites xj lrb for a 3 dimensional input space  and l  1 rrb  the ndimensional coulomb energy function then defines exactly m basins of attraction to the fixed points located at the charge sites xj  it can been shown that convergence to the closest distinct memory is guaranteed  independent of the number of stored memories m  for proper choice of nand l 9  to  equation 1 shows that each cell receives feedback from the network in the form of a scalar  q i jl  x i  l j j j  lrb 3 rrb importantly  this quantity is the same for all cells  it is as if a single virtual cell was computing the distance in activity space between the current state and stored states  the result of the computation is then broadcast to all of the cells in the network  a 2 layer feedforward network implementing such a system has been described elsewhere 10  the connectivity for this architecture is of order m n  where m is the number of stored memories and n is the dimensionality of layer 1  this is significant since the addition of a new memory m   m  1 will change the connectivity by the addition of n  1 connections  whereas in the hopfield network  addition of a new memory requires the addition of 2n  1 connections  an equilibrium feedforward network with similar properties has been under investigation for some time 11  this model does not employ a relaxation procedure  and thus was not originally framed in the language of liapunov functions  however  it is possible to define a similar system if we identify the locations of the  prototypes  of this model as  the locations in state space of potentials which satisfy the following conditions ej   qj lro for i j t  xj i  aj  0 for i fl  xj i  a rsb  lrb 4 rrb 677 where ro is a constant  this form of potential is often referred to as the  square well  potential  this potential may be viewed as a limit of the ndimensional coulomb potential  in which the l r lrb l  l rrb well is replaced with a square well lrb for which l  l rrb  equation 4 describes an energy landscape which consists of plateaus of zero potential outside of wells with flat  zero slope basins  since the landscape has only flat regions separated by discontinuous boundaries  the state of the network is always at equilibrium  and relaxation does not occur  for this reason  this system has been called an equilibrium model  this model  also referred to as the restricted coulomb energy lrb rce rrb 14 model  shares the property of unrestricted storage density  learning in high density memories a simple learning algorithm for the placement of the wells has been described in detail elsewhere 11  12  figurel  3 layer feedforward network  cell i computes the quantity ijl  xii and compares to internal threshold ai  678 reilly et  al have employed a three layer feedforward network lrb figure 1 rrb which allows the generalization of a content addressable memory to a pattern classification memory  because the locations of the minima are explicitly known in the equilibrium model  it is possible to dynamically program the energy function for an arbitrary energy landscape  this allows the construction of geographies of basins associated with the classes constituting the pattern environment  rapid learning of complex  non linear  disjoint  class regions is possible by this method 12  13  learning non separable class regions previous studies have focused on the acquisition of the geography and boundaries of non linearly separable point sets  however  a method by which such high density models can acquire the probability distributions of non separable sets has not been described  non separable sets are defined as point sets in the state space of a system which are labelled with multiple class affiliations  this can occur because the input space has not carried all of the features in the pattern environment  or because the pattern set itself is not separable  points may be degenerate with respect to the explicit features of the space  however they may have different probability distributions within the environment  this structure in the environment is important information for the identification of patterns by such memories 10 the presence of feature space degeneracies  we now describe one possible mechanism for the acquisition of the probability distribution of non separable points  it is assumed that all points in some region r of the state space of the network are the site of events jl lrb 0  ci rrb which are examples of pattern classes c  lcb c 1    cm rcb  a basin of attraction  xk lrb c i rrb  defined by equation 4  is placed at each site fl lrb o  ci rrb unless lrb 5 rrb that is  unless a memory at xj lrb of the class ci rrb already contains fl lrb o  ci rrb  the initial values of qo and ro at xk lrb ci rrb are a constant for all sites xj  thus as events of the classes c 1    c m occur at a particular site in r  multiple wells are placed at this location  679 if a well x  c i rrb correctly covers an event jl lrb 0  c i rrb  then the charge at that site lrb which defines the depth of the well rrb is incremented by a constant amount  q o  in this manner  the region r is covered with wells of all classes lcb c 1    c m rcb  with the depth of well xici rrb proportional to the frequency of occurence of c i at xj  the architecture of this network is exactly the same as that already described  as before  this network acquires a new cell for each well placed in the energy landscape  thus we are able to describe the meaning of wells that overlap as the competition by multiple cells in layer 2 in firing for the pattern of activity in the input layer  applications this system has been applied to a problem in the area of risk assessment in mortgage lending  the input space consisted of feature detectors with continuous firing rates proportional to the values of 23 variables in the application for a mortgage  for this set of features  a significant portion of the space was nonseparable  figures 2a and 2b illustrate the probability distributions of high and low risk applications for two of the features  it is clear that in this 2 dimensional subspace  the regions of high and low risk are non separable but have different distributions  t  llir  prob   1 0  1000 patterns prob  0 0 feature 1  0 5 1 0 figure 2a  probability distribution for high and low risk patterns for feature 1  680  prob  1 0  t 000 patterns 1  1    prob  0 0  0 5 1 0 feature 2 figure 2b  probability distribution for high and low risk patterns for feature 2  figure 3 depicts the probability distributions acquired by the system for this 2 dimensional subspace  in this image  circle radius is proportional to the degree of risk  small circles are regions of low risk  and large circles are regions of high risk  00 o o v 0 0 0 0 o 0   0 t   0 0 00 0 o o 00 0 0 0 00 0 o feature 1 figure 3  probability distribition for low and high risk  small circles indicate low risk regions and large circles indicate high risk regions  681 of particular interest is the clear clustering of high and low risk regions in the 2 d map  note that the regions are in fact nonlinearly separable  discussion we have presented a simple method for the acquisition of probability distributions in non separable point sets  this method generates an energy landscape of potential wells with depths that are proportional to the local probability density of the classes of patterns in the environment  these well depths set the probability of firing of class cells in a 3 layer feedforward network  application of this method to a problem in risk assessment has shown that even completely non separable subspaces may be modeled with surprising accuracy  this method improves pattern classification in such problems with little additional computational burden  this algorithm has been run in conjunction with the method described by reilly et  al  ii for separable regions  this combined system is able to generate non linear decision surfaces between the separable zones  and approximate the probability distributions of the non separable zones in a seemless manner  further discussion of this system will appear in future reports  current work is focused on the development of a more general method for modelling the scale of variations in the distributions  sensitivity to this scale suggests that the transition from separable to non separable regions is smooth and should not be handled with a  hard  threshold  acknowledgements we would like to thank ed collins and sushmito ghosh for their significant contributions to this work through the development of the mortgage risk assessment application  references lsb 1 rsb anderson  j  a  a simple neural network generating an interactive memory  math  biosci  14  197 220 lrb 1972 rrb  682 lsb 2 rsb cooper  l n  a possible organization of animal memory and learning  in  proceedings of the nobel symposium on collective properties of physical systems  lundquist  b  lundquist  s lrb eds  rrb  lrb 24 rrb  252 264 london  new york  academic press 1973  lsb 3 rsb kohonen  t  correlation matrix memories  ieee trans  comput  21  353 359 lrb 1972 rrb  lsb 4 rsb kohonen  t  associative memory  a system theoretical approach  berlin  heidelberg  new york  springer 1977  lsb 5 rsb hopfield  j j  neural networks and physical systems with emergent collective computational abilities  proc  natl acad  sci  usa 79  2554 2558 lrb april 1982 rrb  lsb 6 rsb hopfield  j j  neurons with graded response have collective computational properties like those of two state neurons  proc  natl acad  sci  usa 81  2088 3092 lrb may  1984 rrb  lsb 7 rsb hopfield  j j  feinstein  d i  palmer  r g   unlearning  has a stabilizing effect in collective memories  nature 304  158 159 lrb july 1983 rrb  lsb 8 rsb potter  t w  ph d  dissertation in advanced technology  s u n y binghampton  lrb unpublished rrb  lsb 9 rsb bachmann  c m  cooper  l n  dembo  a  zeitouni  0  a relaxation model for memory with high density storage  to be published in proc  nati  acad  sci  usa  lsb 10 rsb dembo  a  zeitouni  0  aro technical report  brown university  center for neural science  pr0vidence  r i  lrb 1987 rrb  also submitted to phys  rev a lsb 11 rsb reilly  d l  cooper  l n  elbaum  c  a neural model for category learning  bioi  cybern  45  35 41 lrb 1982 rrb  lsb 12 rsb reilly  d l  scofield  c  elbaum  c  cooper  l n  learning system architectures composed of multiple learning modules  to appear in proc  first in1  1  conf  on neural networks lrb 1987 rrb  lsb 13 rsb rimey  r  gouin  p  scofield  c  reilly  d l  real time 3 d object classification using a learning system  intelligent robots and computer vision  proc  spie 726 lrb 1986 rrb  lsb 14 rsb reilly  d l  scofield  c l  elbaum  c  cooper  l n  neural networks with low connectivity and unrestricted memory storage density  to be published 
22 en 201 new hardware for massive neural networks d d coon and a g u perera applied technology laboratory university of pittsburgh pittsburgh  pa 15260  abstract transient phenomena associated with forward biased silicon p   n  n  structures at 4 2 k show remarkable similarities with biological neurons  the devices play a role similar to the two terminal switching elements in hodgkin huxley equivalent circuit diagrams  the devices provide simpler and more realistic neuron emulation than transistors or op amps  they have such low power and current requirements that they could be used in massive neural networks  some observed properties of simple circuits containing the devices include action potentials  refractory periods  threshold behavior  excitation  inhibition  summation over synaptic inputs  synaptic weights  temporal integration  memory  network connectivity modification based on experience  pacemaker activity  firing thresholds  coupling to sensors with graded signal outputs and the dependence of firing rate on input current  transfer functions for simple artificial neurons with spiketrain inputs and spiketrain outputs have been measured and correlated with input coupling  introduction here we discuss the simulation of neuron phenomena by electronic processes in silicon from the point of view of hardware for new approaches to electronic processing of information which parallel the means by which information is processed in intelligent organisms  development of this hardware basis is pursued through exploratory work on circuits which exhibit some basic features of biological neural networks  fig 1 shows the basic circuit used to obtain spiketrain outputs  a distinguishing feature of this hardware basis is the spontaneous generation of action potentials as a device physics feature     o u f rrb tput rrb     r jljll figure 1  spontaneous  neuronlike spiketrain generating circuit  the spikes are nearly equal in amplitude so that information is contained in the frequency and temporal pattern of the spiketrain generation   american institute of physics 1988 202 two terminal switching elements the use of transistor based circuitry 1 is avoided because transistor electrical characteristics are not similar to neuron characteristics  the use of devices with fundamentally non neuronlike character increases the complexity of artificial neural networks  complexity would be an important drawback for massive neural networks and most neural networks in nature achieve their remarkable performance through their massive size  in addition rrb transistors have three terminals whereas the switching elements of hodgkin huxley equivalent circuits have two terminals  motivated in part by hodgkin huxley equivalent circuit diagrams rrb we employ two terminal p  n  n  devices which execute transient switching between low conductance and high conductance states  lrb see fig 2 rrb we call these devices injection mode devices lrb imds rrb  in the  off state   a typical current through the devices is   100fa mm2 rrb and in the  on state  a typical current is   10ma mm2  hence this device is an extremely good switch with a on  0 f f ratio of 1011  as in real neurons 2  the current in the device is a function of voltage and time  not only voltage  the devices require cryogenic cooling but this results in an advantageously low quiescent power drain of  1 nanowatt cm2 of chip area and the very low leakage currents mentioned above  in addition  the highly unique ability of the neural networks described here to operate in a cryogenic environment is an important advantage for infrared image processing at the focal plane lrb see fig 3 and further discussion below rrb  vision systems begin processing at the focal plane and there are many benefits to be gained from the vision system approach to ir image processing          i lrb v  t rrb i i r vd c     vv      ir   ss  ulse output 1  0  q c  q figure 2  switching element in hodgkin huxley equivalent circuits  figure 3  single stage conversion of infrared intensity to spiketrain frequency with a neuron like semiconductor device  no pre amplifiers are necessary  coding of graded input signals lrb see fig 4 rrb such as photocurrents into action potential spike trains with millimeter scale devices has been experimentally demonstrated3 with currents from 1 ila down to about 1 picoampere with coding noise referred to input of  10 femtoamperes  coding of much smaller current levels should be possible with smaller devices  figure 5 clearly shows the threshold behavior of the imd  for devices studied to date  a transition from action potential output to graded signal output is observed for input currents of the order of 0 5 picoamperes 1  203    o z o 10 4 u w lrb f rrb current lrb amperes rrb figure 4  coding of nir vismle uv intensity into firing frequency of a spiketrain and the experimentally determined firing rate vs the input current for one device  note that the dynamic range is about 107    0   o ubi rrb e2 figure 5  mustration of the threshold firing of the device in response to input step functions   pl 500 fls  div this transition is remarkably well described in von neumann s discussion 5 6 of the mixed character of neural elements which he relates to the concept of subliminal stimulation levels which are too low to produce the stereotypical all or nothing response  neural network modelers frequently adopt viewpoints which ignore this interesting mixed character  the von neumann viewpoint links the mixed character to concepts of nonlinear dynamics in a way which is not apparent in recent neural network modeling literature  the scaling down of imd size should result in even lower current requirements for all or nothing response  device physics recently  neuronlike action potential transients in imds have been the subject of considerable research3 4 7 8 9 1 o 1 l 12 13  in the simple circuits of fig 1  the imd gives rise to a spontaneous neuronlike spiketrain output  between pulses  the imd is polarized in the sense that it is in a low conductance state with a substantial voltage occurring across it  even though it is forward biased  the low conductance has been attributed to small interfacial work functions due to band offsets at the n   n and p   n interfaces 8  low temperatures inhibit thermionic injection of electrons and holes into the n region from the n   layer and p   layer impurity bands 14  pulses are caused by 204 switching to depolarized states with low diode potential drops and large injection currents which are believed to be triggered by the slow buildup of a small thermionic injection current from the n   layer into the n region  the injection current can cause impact ionization of n region donor impurities resulting in an increasingly positive space charge which further enhances the injection current to the point where the imd abruptly switches to the low conductance state with large injection current  switching times are typically under loons  charging of the load capacitance cl cuts off the large injection current and resets the diode to its low conductance state  the load capacitor cl then discharges through rl  during the cl discharging time constant rlcl the voltage across the imd itself is low and therefore the bias voltage would have to be raised substantially to cause further firing  thus  rlcl is analogous to the refractory period of a neuron  the output pulses of an imd generally have about the same amplitude while the rate of pulsing varies over a wide range depending on the bias voltage and the presence of electromagnetic radiation  7 8 10  detector array     i transient sensing     i motion sensing  tracking 2 d parallel output figure 6  lllustrative laminar architecture showing stacked wafers in 3 dimensions  laminar neural network real time parallel asynchronous processing the devices described here could form the hardware basis for a parallel asynchronous processor in much the same way that transistors form the basis for digital computers  the devices could be used to construct networks which could perform real time signal processing  pulse propagation through silicon chips lrb parallel firethrough  see fig 7 rrb as opposed to the lateral planar propagation in conventional integrated circuits has been proposed  1s this would permit the use of laminar  stacked wafer architectures  see fig 6  such architectures would eliminate the serial processing limitations of standard processors which utilize multiplexing and charge transfer  there are additional advantages in terms of elimination of pre amplifiers and reduction in power consumption  the approach would utilize the low power  low noise devices lo described here to perform input signal to frequency conversion in every processing channel  power consumption for a brain scale system the low power and low current requirements together with the electronic simplicity lrb lower parts count as compared with transistor and op amp approaches rrb and 205 inputs  1                         1 111111111111  1               1  1            1  1                   1  1                1 siwafer siwafer siwaf r siwaf r figure 7  schematic illustration of the signal flow pattern through a real time parallel asynchronous processor consisting of stacked silicon wafers  wafer  i i i i i i i i i i i i i 1sisiwaf  r             outputs the natural emulation of neuron features means that the approach described here would be especially advantageous for very large neural networks  e g systems comparable to supercomputers in which power dissipation and system complexity are important considerations  the power consumption of large scale analog 16 and digital 17 systems is always a major concern  for example  the power consumption of the cray xmp 48 is of the order of 300 kilowatts  for the devices described here  the power consumption is very low  for these devices  we have observed quiescent power drains of about 1 n w  cm 2 and pulse power consumption of about 500 nj pulse cm 2  we estimate that a system with 1011 active 10  m x 10  m elements lrb comparable to the number of neurons in the brain 18 rrb all firing with an average pulse rate of 1 khz lrb corresponding to a high neuronal firing rates rrb would consume about 50 watts  the quiescent power drain for this system would be 0 1 milliwatts  thus  power lrb p rrb requirements for such an artificial neural network with the size scale lrb 1011 pulse generating elements rrb of the human brain and a range of activity between zero and the maximum conceivable sustained activity for neurons in the brain would be 0 1 milliwatts  p  50 watts for 10 micron technology  for comparison  we note that von neumann s estimate for the power dissipation of the brain is of order 10 to 25 watts  s 6 fabrication of a 1011 element 10  m artificial neural network would require processing of about 1500 four inch wafers  network connectivity for a network with coupling between many imd s 3 we have shown  that lrb 1 rrb where vj is the voltage across the diode and the input capacitance cj of the i th network node  rj represents a leakage resistance in parallel with cil and ij represents an external current input to the i th diode  ij  1 2 3    label different network nodes and tij incoporates coupling between network elements  equation 1 has the same form as equations which occur in the hopfield modei 2o 21 22 23 for neural networks  sejnowski has also discussed similar equations in connection with skeleton filters in 206 outputs inputs   t  o  f  r  t  t t  o t  o c rrb  o  j r o  j       fi   o  j r p  o   transmission line figure 8  a rrb main features of a typical neuron from kandel and schwartz  19 b rrb our artificial neuron rrb which shows the summation over synaptic inputs and fan out  the brain  24  25 nonlinear threshold behavior of imd rrb s enters through f lrb v rrb as it does in the neural network models  in fig 8 b a range of input capacitances is possible  this range of capacitances is related to the range of possible synaptic weights  the circuit in fig 8 accomplishes pulse height discrimination and each pulse can contribute to the charge stored on the central node capacitance c  the charge added to c during each input pulse is linearly related to the input capacitance except at extreme limits  the range of input capacitances for a particular experiment was 002 j lf to 2 j lf which differ by a factor of about 100  the effect of various input capacitance values lrb synaptic weights rrb on input output firing rates is shown in fig 9  also the fig 8 b shows many capacitive inputs outputs to from a single imd  i e fan in and fan out  for pulses which arrive at different inputs at about the same time rrb the effect of the pulses is additive  the time within which inputs are summed is just the stored charge lifetime  summation over many inputs is an important feature of neural information processing  excitation rrb inhibition rrb memory both excitatory and inhibitory input circuits are shown in fig 10  input pulses cause the accumulation of charge on c in excitatory circuits and the depletion of charge on c in inhibitory circuits  charge associated with input spiketrains is integrated stored on c  the temporally integrated charge is depleted by the firing of the imd  thus rrb the storage time is related to the firing rate  after an input spiketrain raises the potential across c to a value above the firing threshold rrb the resulting imd 207 5 o 2 rcb j f o 03 rcb jf      n i     4 figure 9  output pulse rate vs the input pulse rate for different input capacitance values ci values w t   3 w vl  1   j lrb l 2 t    j cl t  6 1 20 40 80 60 100 input pulse rate lrb hz rrb lrb 0 rrb r i np  i      r  lcb rrb l       outp ut figure 10  circuits which incorporate rectifying synaptic inputs  a rrb an excitatory input  b rrb an inhibitory input  lrb b rrb r r  inp  c  l r  l output spiketrain codes the input information  the output firing rate is linearly related to the input firing rate times the synaptic coupling strength lrb linearly related to ci rrb  see fig 9  if the input ceases  then the potential across c relaxes back to a value just below the firing threshold  when not firing  the imd has a high impedance  if there is negligible leakage of charge from c  then v can remain near v t lrb threshold voltage rrb for a long time and a new input signal will quickly take the imd over the firing threshold  see fig 11  we have observed stored charge lifetimes of 56 days and longer times may be acheivable  the lifetime of charge stored on c can be reduced by adding a resistance in parallel with c from the discussion of integration  we see that long term storage of charge on c is equivalent to long term memory  the memory can be read by seeing if a new input pulse or spiketrain produces a prompt output pulse or spiketrain  the read signal input channel in fig 8 b can be the same as or different from the channel which resulted in the charge storage  in either case memory would produce a change in the pattern of connectivity if the circuit was imbedded in a neural network  changes in patterns of connectivity are similar to hebb s ruie considerations26 in which memory is associated with increases in the strength lrb weight rrb of synaptic couplings  frequently  208   qj 13 o a   11 figure 11  firing rate vs the bias voltage  the region where the firing is negligible is associated with memory  the state of the memory is associated with the proximity to the firing threshold  input potential the increase in synaptic weights is modeled by increased conductance whereas in the circuits in figs  lo lrb a rrb and 8 b memory is achieved by integration and charge storage  note that for these particular circuits  the memory is not eraseable although volatile lrb short term rrb memory can easily be constructed by adding a resistor in parallel with c thus  a continuous range of memory lifetimes can be achieved  2 d parallel asynchronous chip to chip transmission for many imd s the output pulse heights for a circuit like that in fig 1 are  3 volts  thus  output from the first stage or any later stage of the network could easily be transmitted to other parts of an overall system  two dimensional arrays of devices on different chips could be coupled by indium bump bonding to form the laminar architecture described above  planar technology could be used for local lateral interconnections in the processor  lrb see fig 7 rrb in addition to transmission of electrical pulses  optical transmission is possible because the pulses can directly drive led s  emerging gaas on si technology is interesting as a means of fabricating two dimensional emitter arrays  optical transmission is not necessary but it might be useful lrb a rrb for processed image data transfer  lrb b rrb for coupling to an optical processor  or lrb c rrb to provide 2 0 optical interconnects between chips bearing 2 d arrays of p   n  n  diodes  note that with optical interconnects between chips  the circuits employed here would be internal receivers  the p i n diodes employed in the present work would be well suited to the receiver role  an interesting possibility would entail the use optical interconnects between chips to achieve local  lateral interaction  this would be accomplished by having each optical emitter in a 2 d array broadcast locally to multiple receivers rather than to a single receiver  similarly  each receiver would have a reeeptive field extending over multiple transmitters  it is also possible that an optical element could be placed in the gap between parallel transmitter and receiver planes to structure  control or alter 2 d patterns of interconnection  this would be an alternative to a planar technology approach to lateral interconnection  it the optical elements were active then the system would constitute a hybrid optical electronic processor  whereas if passive optical elements were employed  we would regard the system as an optoelectronic processor  in either case  we picture the processing functions of temporal integration  spatial summation over inputs  coding and pulse generation as residing on chip  209 acknowledgements the work was supported in part by u s doe under contract de  ac0280er10667 and nsf under grant  ecs 8603075  references lsb 1 rsb l d harmon  kybernetik 1 89 lrb 1961 rrb  lsb 2 rsb a l hodgkin and a f huxley  j physioll17  500 lrb 1952 rrb  lsb 3 rsb d d coon and a g u perera  int  j electronics 63  61 lrb 1987 rrb  lsb 4 rsb k m s v bandara  d d coon and r p g karunasiri  infrared  lransient sensing  to be published  lsb 5 rsb j von neumann  the computer and the brain  yale university press  new haven and london  1958  lsb 6 rsb j von neumann  collected works  pergamon press  new york  1961  lsb 7 rsb d d coon and a g u perera  int  j infrared and millimeter waves 7  1571 lrb 1986 rrb  lsb 8 rsb d d coon and s d gunapala  j appl  phys 57  5525 lrb 1985 rrb  lsb 9 rsb d d coon  s n ma and a g u perera  phys  rev let  58  1139 lrb 1987 rrb  lsb 10 rsb d d coon and a g u perera  applied physics letters 51  1711 lrb 1987 rrb  lsb 11 rsb d d coon and a g u perera  solid state electronics 29  929 lrb 1986 rrb  lsb 12 rsb d d coon and a g u perera  applied physics letters 51  1086 lrb 1987 rrb  lsb 13 rsb k m s v bandara  d d coon and r p g karunasiri  appl  phys  lett 51  961 lrb 1987 rrb  lsb 14 rsb y n yang  d d coon and p f shepard  applied physics letters 45  752 lrb 1984 rrb  lsb 15 rsb d d coon and a g u perera  int  j ir and millimeter waves 8  1037 lrb 1987 rrb  lsb 16 rsb m a sivilotti  m r emerling and c a mead  vlsi arcbitectures for implementation of neural networks  neural networks for computing  a j p  1986  pp 408 413  lsb 17 rsb r w keyes  proc  ieee 63  740 lrb 1975 rrb  lsb 18 rsb e  r kandel and j h schwartz  principles of neural science  elsevier  new york  1985  210 lsb 19 rsb e r kandel and j h schwartz  principles of neural science  elsevier  new york  1985  page 15  reproduced by permission of elsevier science publishing co  n y   lsb 20 rsb j j hopfield  proc  nat   acad  sci  u s a 81  3088 lrb 1984 rrb  lsb 21 rsb j j hopfield and d w tank  bioi  cybern 52  141 lrb 1985 rrb  lsb 22 rsb j j hopfield and d w tank  science 233 625 lrb 1986 rrb  lsb 23 rsb d w tank and j j hopfield  ieee  circuits syst  cas 33  533 lrb 1986 rrb  lsb 24 rsb t j sejnowski  j math  biology 4  303 lrb 1977 rrb  lsb 25 rsb t j sejnowski  skeleton filters in tbe brain  lawrence erlbaum  new jersey  1981  pp 189 212  edited by g e hinton and j a anderson  lsb 26 rsb j l mcclelland  d e rumelhart and the pdp research group  parallel distributed processing  the mit press  cambridge  massachusetts  1986  two volumes 
23 en 715 a computer simulation of cerebral neocortex  computational capabilities of nonlinear neural networks alexander singer  and john p donoghue   department of biophysics  johns hopkins university  baltimore  md 21218 lrb to whom all correspondence should be addressed rrb  center for neural science  brown university  providence  ri 02912  american institute of physics 1988 716 a synthetic neural network simulation of cerebral neocortex was developed based on detailed anatomy and physiology  processing elements possess temporal nonlinearities and connection patterns similar to those of cortical neurons  the network was able to replicate spatial and temporal integration properties found experimentally in neocortex  a certain level of randomness was found to be crucial for the robustness of at least some of the network s computational capabilities  emphasis was placed on how synthetic simulations can be of use to the study of both artificial and biological neural networks  a variety of fields have benefited from the use of computer simulations  this is true in spite of the fact that general theories and conceptual models are lacking in many fields and contrasts with the use of simulations to explore existing theoretical structures that are extremely complex lrb cf macgregor and lewis  1977 rrb  when theoretical superstructures are missing  simulations can be used to synthesize empirical findings into a system which can then be studied analytically in and of itself  the vast compendium of neuroanatomical and neurophysiological data that has been collected and the concomitant absence of theories of brain function lrb crick  1979  lewin  1982 rrb makes neuroscience an ideal candidate for the application of synthetic simulations  furthennore  in keeping with the spirit of this meeting  neural network simulations which synthesize biological data can make contributions to the study of artificial neural systems as general infonnation processing machines as well as to the study of the brain  a synthetic simulation of cerebral neocortex is presented here and is intended to be an example of how traffic might flow on the two way street which this conference is trying to build between artificial neural network modelers and neuroscientists  the fact that cerebral neocortex is involved in some of the highest fonns of information processing and the fact that a wide variety of neurophysiological and neuroanatomical data are amenable to simulation motivated the present development of a synthetic simulation of neocortex  the simulation itself is comparatively simple  nevertheless it is more realistic in tenns of its structure and elemental processing units than most artificial neural networks  the neurons from which our simulation is constructed go beyond the simple sigmoid or hard saturation nonlinearities of most artificial neural systems  for example  717 because inputs to actual neurons are mediated by ion currents whose driving force depends on the membrane potential of the neuron  the amplitude of a cell s response to an input  i e the amplitude of the post synaptic potential lrb psp rrb  depends not only on the strength of the synapse at which the input arrives  but also on the state of the neuron at the time of the input s arrival  this aspect of classical neuron electrophysiology has been implemented in our simulation lrb figure la rrb  and leads to another important nonlinearity of neurons  namely  current shunting  primarily effective as shunting inhibition  excitatory current can be shunted out an inhibitory synapse so that the sum of an inhibitory postsynaptic potential and an excitatory postsynaptic potential of equal amplitude does not result in mutual cancellation  instead  interactions between the ion reversal potentials  conductance values  relative timing of inputs  and spatial locations of synapses determine the amplitude of the response in a nonlinear fashion lrb figure ib rrb lrb see koch  poggio  and torre  1983 for a quantitative analysis rrb  these properties of actual neurons have been ignored by most artificial neural network designers  though detailed knowledge of them has existed for decades and in spite of the fact that they can be used to implement complex computations lrb e g torre and poggio  1978  houchin  1975 rrb  the development of action potentials and spatial interactions within the model neurons have been simplified in our simulation  action potentials involve preprogrammed  fluctuations in the membrane potential of our neurons and result in an absolute and a relative refractory period  thus  during the time a cell is firing a spike synaptic inputs are ignored  and immediately following an action potential the neuron is hyperpolarized  the modeling of spatial interactions is also limited since neurons are modeled primarily as spheres  though the spheres can be deformed through control of a synaptic weight which modulates the amplitudes of ion conductances  detailed dendritic interactions are not simulated  nonetheless  the fact that inhibition is generally closer to a cortical neuron s soma while excitation is more distal in a cell s dendritic tree is simulated through the use of stronger inhibitory synapses and relatively weaker excitatory synapses  the relative strengths of synapses in a neural network define its connectivity  though initial connectivity is random in many artificial networks  brains can be thought to contain a combination of randomness and fixed structure at distinct levels lrb szentagothai  1978 rrb  from a macroscopic perspective  all of cerebral neocortex might be structured in a modular fashion analogous to the way the barrel field of mouse somatosensory cortex is structured lrb woolsey and van der loos  1970 rrb  though speculative  arguments for the existence of some sort of anatomical modularity over the entire cortex are gaining ground 718 lrb mountcastle  1978  szentagothai  1979  shepherd  in press rrb  thus  inspired by the barrels of mice and by growing interest in functional units of 50 to 100 microns with on the order of 1000 neurons  our simulation is built up of five modules lrb 60 cells each rrb with more dense local interconnections and fewer intermodular contacts  furthermore  a wide variety of neuronal classification schemes have led us to subdivide the gross structure of each module so as to contain four classes of neurons  cortico cortical pyramids  output pyramids  spiny stellate or local excitatory cells  and gabaergic or inhibirtory cells  at this level of analysis  the impressed structure allows for control over a variety of pathways  in our simulation each class of neurons within a module is connected to every other class and intermodular connections are provided along pathways from cortico cortical pyramids to inhibitory cells  output pyramids  and cortico cortical pyramids in immediately adjacent modules  a general sense of how strong a pathway is can be inferred from the product of the number of synapses a neuron receives from a particular class and the strength of each of those synapses  the broad architecture of the simulation is further structured to emphasize a three step path  inputs to the network impact most strongly on the spiny stellate cells of the module receiving the input  these cells in tum project to cortico cortical pyramidal cells more strongly than they do to other cell types  and finally  the pathway from the cortico cortical pyramids to the output pyramidal cells of the same module is also particularly strong  this general architecture lrb figure 2 rrb has received empirical support in many regions of cortex lrb jones  1986 rrb  in distinction to this synaptic architecture  a fine grain connectivity is defined in our simulated network as well  at a more microscopic level  connectivity in the network is random  thus  within the confines of the architecture described above  the determination of which neuron of a particular class is connected to which other cell in a target class is done at random  two distinct levels of connectivity have  therefore  been established lrb figure 3 rrb  together they provide a middle ground between the completely arbitrary connectivity of many artificial neural networks and the problem specific connectivities of other artificial systems  this distinction between gross synaptic architecture and fine grain connectivity also has intuitive appeal for theories of brain development and  as we shall see  has non trivial effects on the computational capabilities of the network as a whole  with defintions for input integration within the local processors  that is within the neurons  and with the establishment of connectivity patterns  the network is complete and ready to perform as a computational unit  in order to judge the simulation s capabilities in some rough way  a qualitative analysis of its response to an input will suffice  figure 4 719 shows the response of the network to an input composed of a small burst of action potentials arriving at a single module  the data is displayed as a raster in which time is mapped along the abscissa and all the cells of the network are arranged by module and cell class along the ordinate  each marker on the graph represents a single action potential fired by the appropriate neuron at the indicated time  qualitatively  what is of importance is the fact that the network does not remain unresponsive  saturate with activity in all neurons  or oscillate in any way  of course  that the network behave this way was predetermined by the combination of the properties of the neurons with a judicious selection of synaptic weights and path strengths  the properties of the neurons were fixed from physiological data  and once a synaptic architecture was found which produced the results in figure 4  that too was fixed  a more detailed analysis of the temporal firing pattern and of the distribution of activity over the different cell classes might reveal important network properties and the relative importance of various pathways to the overall function  such an analysis of the sensitivity of the network to different path strengths and even to intracellular parameters will  however  have to be postponed  suffice it to say at this point that the network  as structured  has some nonzero  finite  non oscillatory response which  qualitatively  might not offend a physiologist judging cortical activity  though the synaptic architecture was tailored manually and fixed so as to produce  reasonable  results  the fine grain connectivity  i e the determination of exactly which cell in a class connects to which other cell  was random  an important property of artificial lrb and presumably biological rrb neural networks can be uncovered by exploiting the distinction between levels of connectivity described above  before doing so  however  a detail of neural network design must be made explicit  any network  either artificial or biological  must contend with the time it takes to communicate among the processing elements  in the brain  the time it takes for an action potential to travel from one neuron to another depends on the conduction velocity of the axon down which the spike is traveling and on the delay that occurs at the synapse connecting the cells  roughly  the total transmission time from one cortical neuron to another lies between 1 and 5 milliseconds  in our simulation two 720 paradigms were used  in one case  the transmission times between all neurons were standardized at 1 msec   alternatively  the transmission times were fixed at random  though admittedly unphysiological  values between 0 1 and 2 msec  now  if the time it takes for an action potential to travel from one neuron to another were fixed for all cells at 1 msec  different fine grain connectivity patterns are found to produce entirely distinct network responses to the same input  in spite of the fact that the gross synaptic architecture remained constant  this was true no matter what particular synaptic architecture was used  if  on the other hand  one changes the transmission times so that they vary randomly between 0 1 and 2 msec  it becomes easy to find sets of synaptic strengths that were robust with respect to changes in the fine grain connectivity  thus  a wide search of path strengths failed to produce a network which was robust to changes in fine grain connectivity in the case of identical transmission times  while a set of synaptic weights that produced robust responses was easy to find when the transmission times were randomized  figure 5 summarizes this result  in the figure overall network activity is measured simply as the total number of action potentials generated by pyramidal cells during an experiment and robustness can be judged as the relative stability of this response  the abscissa plots distinct experiments using the same synaptic architecture with different fine grain connectivity patterns  thus  though the synaptic architecture remains constant  the different trials represent changes in which particular cell is connected to which other cell  the results show quite dramatically that the network in which the transmission times are randomly distributed is more robust with respect to changes in fine grain connectivity than the network in which the transmission times are all 1 msec  it is important to note that in either case  both when the network was robust and when changes of fine grain connectivity produced gross changes in network output  the synaptic architectures produced outputs like that in figure 4 with some fine grain connectivities  if the response of the network to an input can be considered the result of  because neurons receive varying amounts of input and because integration is performed by summating excitatory and inhibitory postsynaptic potentials in a nonlinear way  the time each neuron needs to summate its inputs and produce an action potential varies from neuron to neuron and from time to time  this then allows for asynchronous fuing in spite of the identical transmission times  721 some computation  figure 5 reveals that the same computational capability is not robust with respect to changes in fine grain connectivity when transmission times between neurons are all 1 msec  but is more robust when these times are randomized  thus  a single computational capability  viz  a response like that in figure 4 to a single input  was found to exist in networks with different synaptic architectures and different transmission time paradigms  this computational capability  however  varied in terms of its robustness with respect to changes in fine grain connectivity when present in either of the transmission time paradigms  a more complex computational capability emerged from the neural network simulation we have developed and described  if we label two neighboring modules c2 and c3  an input to c2 will suppress the response of c3 to a second input at c3 if the second input is delayed  a convenient way of representing this spatio temporal integration property is given in figure 6  the ordinate plots the ratio of the normal response of one module lrb say c3 rrb to the response of the module to the same input when an input to a neighboring module lrb say c2 rrb preceeds the input to the original module lrb c3 rrb  thus  a value of one on the ordinate means the earlier spatially distinct input had no effect on the response of the module in which this property is being measured  a value less than one represents suppression  while values greater than one represent enhancement  on the abscissa  the interstimulus interval is plotted  from figure 6  it can be seen that significant suppression of the pyramidal cell output  mostly of the output pyramidal cell output  occurs when the inputs are separated by 10 to 30 msec  this response can be characterized as a sort of dynamic lateral inhibition since an input is suppressing the ability of a neighboring region to respond when the input pairs have a particular time course  this property could playa variety of role in biological and artificial neural networks  one role for this spatio temporal integration property  for example  might be in detecting the velocity of a moving stimulus  the emergent spatio temporal property of the network just described was not explicitly built into the network  moreover  no set of synaptic weights was able to give rise to this computational capability when transmission times were all set to 1 msec  thus  in addition to providing robustness  the random transmission times also enabled a more complex property to emerge  the important factor in the appearances of both the robustness and the dynamic lateral inhibition was randomization  though it was implemented as randomly varying transmission times  random spontaneous activity would have played the same role  from the viewpoint  then  of the engineer designing artificial neural networks  the neural network presented here has instructional value in spite of the 722 fact that it was designed to synthesize biological data  specifically  it motivates the consideration of randomness as a design constraint  from the prespective of the biologists attending this meeting  a simple fact will reveal the importance of synthetic simulations  the dynamic lateral inhibition presented in figure 6 is known to exist in rat somatosensory cortex lrb simons  1985 rrb  by deflecting the whiskers on a rat s face  simons was able to stimulate individual barrels of the posteromedial somatosensory barrel field in combinations which revealed similar spatio temporal interactions among the responses of the cortical neurons of the barrel field  the temporal suppression he reported even has a time course similar to that of the simulation  what the experiment did not reveal  however  was the class of cell in which suppression was seen  the simulation located most of the suppression in the output pyramidal cells  hence  for a biologist  even a simple synthetic simulation like the one presented here can make defmitive predictions  what differentiates the predictions made by synthetic simulations from those of more general artificial neural systems  of course  is that the strong biological foundations of synthetic simulations provide an easily grasped and highly relevant framework for both predictions and experimental verification  one of the advertised purposes of this meeting was to  bring together neurobiologists  cognitive psychologists  engineers  and physicists with common interest in natural and artificial neural networks   towards that end  synthetic computer simulations  i e simulations which follow known neurophysiological and neuroanatomical data as if they comprised a complex recipe  can provide an experimental medium which is useful for both biologists and engineers  the simulation of cerebral neocortex developed here has information regarding the role of randomness in the the robustness and presence of various computational capabilities as well as information regarding the value of distinct levels of connectivity to contribute to the design of artificial neural networks  at the same time  the synthetic nature of the network provides the biologist with an environment in which he can test notions of actual neural function as well as with a system which replicates known properties of biological systems and makes explicit predictions  providing twoway interactions  synthetic simulations like this one will allow future generations of artificial neural networks to benefit from the empirical findings of biologists  while the slowly evolving theories of brain function benefit from the more generalizable results and methods of engineers  723 references crick  f h c lrb 1979 rrb thinking about the brain  scientific american  241 219  232  houchin 1  lrb 1975 rrb direction specificity in cortical responses to moving stimuli  a simple model  proceedings of the physiological society  247 7  9  jones  e g lrb 1986 rrb connectivity of primate sensory motor cortex  in cerebral cortex  vol  5  e g jones and a peters lrb eds rrb  plenum press  new york  koch  c  poggio  t  and torre  v lrb 1983 rrb nonlinear interactions in a dendritic tree  localization  timing  and role in information processing  proceedings of the national academy of science  usa  80 2799  2802  lewin  r lrb 1982 rrb neuroscientists look for theories  science  216 507  macgregor  r i and lewis  e r lrb 1977 rrb neural modeling  plenum press  new york  mountcastle  v b lrb 1978 rrb an organizing principle for cerebral function  the unit module and the distributed system  in the mindful brain  g m edelman and v b mountcastle lrb eds  rrb  mit press  cambridge  ma  shepherd  g m lrb in press rrb basic circuit of cortical organization  in perspectives in memory research  m s gazzaniga lrb ed  rrb  mit press  cambridge  ma  simons  d j lrb 1985 rrb temporal and spatial integration in the rat si vibrissa cortex  journal of neurophysiology  54 615  635  szenthagothai 1  lrb 1978 rrb specificity versus lrb quasi  rrb randomness in cortical connectivity  in architectonics of the cerebral cortex  m a b brazier and h petsche lrb eds  rrb  raven press  new york  szentagothai  j lrb 1979 rrb local neuron circuits in the neocortex  in the neurosciences  fourth study program  f o schmitt and f g worden lrb eds  rrb  mit press  cambridge  ma  torre  v and poggio  t lrb 1978 rrb a synaptic mechanism possibly underlying directional selectivity to motion  proceeding of the royal society lrb london rrb b  202 409 416  woolsey  t a and van der loos  h lrb 1970 rrb structural organization of layer iv in the somatosensory region lrb si rrb of mouse cerebral cortex  brain research  17 205 242  724 shunting inhibition simultaneous epsp  ipsp ipsp figure ia  intracellular records of post synaptic potentials resulting from single excitatory and inhibitory inputs to cells at different resting potentials  psp amplitude dependence on membrane potential ipsps epsps resting potential   40 mv resting potential  60 mv resting potential  40 mv r  resting potential  80 mv l       i c        resting potential  100 mv       resting potential  120 mv  c  resting potential  20 mv r  resting potential  omv resting potential  20 mv  i c    l        resting  potential  40 mv  figure ib  illustration of the current shunting nonlinearity present in the model neurons  though the simultaneous arrival of postsynaptic potentials of equal and opposite amplitude would result in no deflection in the membrane potential of a simple linear neuron model  a variety of factors contribute to the nonlinear response of actual neurons and of the neurons modeled in the present simulation               xx              c a lls                                  luu                                                                                   output    pyramids                         rrb                                                                                                                                                                                                                                                             input figure 2  a schematic representation of the simulated cortical network  five modules are used  each containing sixty neurons  neurons are divided into four classes  numerals within the caricatured neurons represent the number of cells in that particular class that are simulated  though all cell classes are connected to all other classes  the pathway from input to spiny stellate to cortico cortical pyramids to output pyramids is particularly strong     j  01 726 path strength number of synapses x           of 6  6  6  6  6 6  6  0 0 6 0 0 0 0 6  output pyramidal cells 6   0 0 6 6  6  6  6  6  t    6  0 inhibitory cells 6   6   6 6 6 6  intracorlical pyramidal cells    0  0     spiny stellate cells figure 3  two levels of connectivity are defined in the network  gross synaptic architecture is defined among classes of cells  fine grain connectivity specifies which cell connects to which other cell and is determined at random  727 sample raster input  333 hz input  6 rns duration applied to module 3 module s module 4 module 3          module 2       cortico c ortical ramids    py inhibitory cells    spin y stellate cells  output pyr amids     module 1 i 10   time lrb ms rrb i i 20 30 figure 4  sample response of the entire network to a small burst of action potentials delivered to module 3  728 robustness with respect to connectivity pattern synaptic architecture constant 400  iii i  rrb c  300 0  c c  rrb iii a   q rrb delay times  1 ms lrb rrb iu  c 200   e    a  iu jdera  0   y times random 100  i               individual trials with different fine grain connectivity patterns figure 5  plot of an arbitrary activity measure lrb total spike activity in all pyramidal cells rrb versus various instatiations ofthe same connectional architecture  along the abscissa are represented the different fine grained patterns of connectivity within a fixed connectional architecture  in one case the conductance times between all cells was i msec and in the other case the times were selected at random from values between 0 1 msec and 2 msec  this experiment shows the greater overall stability produced by random conduction times  2 spatio temporal integration properties q rrb  outpyr fy    0c  a a    q rrb q rrb     a  a   c cpyr   sst    gaba randomized axonal conduction times oi o  20 40 60 80 100 120 interstimulus interval figure 6  spatio temporal integration within the network  plot of the time course of response suppression in the various cell classes  the ordinate plots the ratio of average cell activity lrb in terms of spikes rrb to a direct input after the presentation of an input to a neighboring mod ule  and the average reponse to an input in the absence of prior input to an adjacent module  values greater than one represent an enhancement of activity in response to the spatially distinct preceeding input  while values less than one represent a suppression of the normal reponse  the abscissa plots the interstimulus interval  note that the response suppression is most striking in only one class of cells    j  t o
24 en 95 optimal neural spike classification amir f atiya lrb  rrb and james m bower lrb  rrb lrb  rrb dept of electrical engineering lrb  rrb division of biology california institute of technology ca 91125 abstract being able to record the electrical activities of a number of neurons simultaneously is likely to be important in the study of the functional organization of networks of real neurons  using one extracellular microelectrode to record from several neurons is one approach to studying the response properties of sets of adjacent and therefore likely related neurons  however  to do this  it is necessary to correctly classify the signals generated by these different neurons  this paper considers this problem of classifying the signals in such an extracellular recording  based upon their shapes  and specifically considers the classification of signals in the case when spikes overlap temporally  introduction how single neurons in a network of neurons interact when processing information is likely to be a fundamental question central to understanding how real neural networks compute  in the mammalian nervous system we know that spatially adjacent neurons are  in general  more likely to interact  as well as receive common inputs  thus neurobiologists are interested in devising techniques that allow adjacent groups of neurons to be sampled simultaneously  unfortunately  the small scale of real neural networks makes inserting one recording electrode per cell impractical  therefore  one is forced to use single electrodes designed to sample neural signals evoked by several cells at once  while this approach provides the multi neuron recordings being sought  it also presents a rather serious waveform classification problem because the actual temporal sequence of action potentials in each individual neuron must be deciphered  this paper describes a method for classifying the activities of several individual neurons recorded simultaneously using a single electrode  description of the problem over the last two decades considerable attention 1 8 has been devoted to the problem of classification of action potentials in multi neuron recordings  these action potentials lrb also referred to as  spikes  rrb are the extracellularly recorded signal produced by a single neuron when it is passing information to other neurons lrb fig 1 rrb  fortunately  spikes recorded from the same cell are more or less similar in shape  while spikes coming from different neurons usually have somewhat different shapes  depending on the neuron type  electrode characteristics  the distance between the electrode and the neuron  and the intervening medium  fig 1 illustrates some representative variations in spike shapes  it is our objective to detect and classify different spikes based on their shapes  however  relying entirely on the shape of the spikes presents difficulties  for example spikes from different neurons can overlap temporally producing novel waveforms lrb see fig 2 for an example of an overlap rrb  to deal with these overlaps  one has first to detect the occurrence of an overlap  and then estimate the constituent spikes  unfortunately  only a few of the available spike separation algorithms consider these events  even though they are potentially very important in understanding neural networks  those few tend to rely  american institute of physics 1988 96 on heuristic rules and subtractive methods to resolve overlap cases  no currently published method we are aware of attempts to use knowledge of the likelihood of overlap events for detecting them  which is at the basis of the method we will describe  fig 1 an example of a multi neuron recording overlapping spikes fig 2 an example of a temporal overlap of action potentials general approach the first step in classifying neural waveforms is obviously to identify the typical spike shapes occurring in a particular recording  to do this we have applied a learning algorithm on the beginning portion of the recording  which in an unsupervised fashion lrb i e without the intervention of a human operator rrb estimates the shapes  after the learning stage we have the classification stage  which is applied on the remaining portion of the recording  a new classification method is proposed  which gives minimum probability of error  even in case of the occurrence of overlapping spikes  both the learning and the classification algorithms require a preprocessing step to detect the position of the spike candidate in the data record  detection  for the first task of detection most researchers use a simple level detecting algorithm  that signals a spike when recorded voltage levels cross a certain voltage threshold  however  variations in recording position due to natural brain movements during recording lrb e g respiration rrb can cause changes in relative height of the positive to the negative peak  thus  a level detector lrb using either a positive or a negative threshold rrb can miss some spikes  alternatively  we have chosen to detect an event by sliding a window of fixed length until a time when the peak to peak value within the window exceeds a certain threshold  learning  learning is performed on the beginning portion of the sampled data using the isodata clustering algorithm 9  the task is to estimate the number of neurons n whose spikes are represented in the waveform and learn the different shapes of the spikes of the various neurons  for that purpose we apply the clustering algorithm choosing only one feature 97 from the spike  the peak to peak value which we have found to be quite an effective feature  note that using the peak to peak value in the learning stage does not necessitate using it for classification lrb one might need additional or different features  especially for tackling the case of spike overlap rrb  the optimal olassification rule  once we have identified the number of different events present  the classification stage is concerned with estimating the identities of the spikes in the recording  based on the typical spike shapes obtained in the learning stage  in our classification scheme we make use of the information given by the shape of the detected spike as well as the firing rates of the different neurons  although the shape plays in general the most important role in the classification  the rates become a more significant factor when dealing with overlapping events  this is because in general overlap is considerably less frequent than single spikes  the shape information is given by set of features extracted from the waveform  let x be the feature vector of the detected spike lrb e g the samples of the spike waveform rrb  let n i    n n represent the different neurons  the detection algorithm tells us only that at least one spike occurred in the narrow interval lrb t  ti  t  t 2 rrb lrb  say 1 rrb where t is the instant of the peak of the detected spike  ti and t2 are constants chosen subjectively according to the smallest possible time separation between two consecutive spikes  identifiable as two separate lrb nonoverlapping rrb spikes  by definition  if more than one spike occurs in the interval i  then we have an overlap  as a matter of convention  the instant of the occurrence of a spike i   taken to be that of the spike peak  for simplicity  we will consider the case of two possibly overlapping spikes  though the method can be extended easily to more  the classification rule which results in minimum probability of error is the one which chooses the neuron lrb or pair of neurons in case of overlap rrb which has the maximum likelihood  we have therefore to compare the pi s and the p   s  defined as a   p lrb ni fired in ilx  a rrb  p  j  p lrb n  and n j fired in ilx  a rrb  i  1    n l  j  i    n  j  l where a represents the event that one or two spikes occurred in the interval i  in other words pi the probability that what has been detected is a single spike from neuron i  whereas p  j is the probability that we have two overlapping spikes from neurons land j lrb note that spikes from the same neuron never overlap rrb  henceforth we will use i to denote probability density  for the purpose of abbreviation let bi lrb t rrb mean  neuron ni fired at t   the classification problem can be reduced to comparing the following likelihood functions  i  1    n lrb la rrb  j  1    n  j  i lrb lb rrb lrb for a derivation refer to appendix rrb  let ii be the density of the inter spike interval and ti be the most recent firing instant of neuron ni  it we are given the fact that neuron ni has been idle for at least a period of duration t  ti  we get a disadvantage of using lrb 2 rrb is that the available  i s and t   s are only estimates  which depend on the previous classification results  further  for reliable estimation of the densities ii  one needs a large number of spikes and therefore a long learning period since we are estimating a 98 whole function  therefore  we have not used this form  but instead have used the following two schemes  in the first one  we ignore the knowledge about the previous firing pattern except for the estimated firing rates   1     n of the different neurons nl    n n respectively  then the probability of a spike coming from neuron ni in an interval of duration dt is simply   idt  hence in the second scheme we do not use any previous knowledge except for the total firing rate lrb of all neurons rrb  say a  then although the second scheme does not use as much of the information about the firing pattern as the first scheme does  it has the advantage of obtaining and using a more reliable estimate of the firing rate  because in general the overall firing rate changes less with time than the individual rates and because the estimate of a does not depend on previous classification results  however  it is useful mostly when the firing rates of the different neurons do not vary much  otherwise the firt scheme is preferred  in real recording situations  sometimes one encounters voltage signals which are much different than any of the previously learned typical spike shapes or their pairwise overlaps  this can happen for example due to a falsely detected noise event  a spike from a class not encountered in the learning stage  or to the overlap of three or more spikes  to cope with these cases we use the reject option  this means that we refuse to classify the detected spike because of the unlikeliness of the assumed event a  the reject option is therefore employed whenever p lrb alx rrb is smaller than a certain threshold  we know that p lrb alx rrb  j lrb a  x rrb  lsb j lrb a  x rrb  j lrb a c  x rrb rsb where ac is the complement of the event a  the density f lrb ac  x rrb can be approximated as uniform lrb over the possible values of x rrb because a large variety of cases are covered by the event ac  it follows that one can just compare j lrb a  x rrb to a threshold  hence the decision strategy becomes finally  reject if the sum of the likelihood functions is less than a threshold  otherwise choose the neuron lrb or pair of neurons rrb corresponding to the largest likelihood functions  note that the sum of the likelihood functions equals j lrb a  x rrb lrb refer to appendix rrb  now  let us evaluate the integrals in lrb 1 rrb  overlapping spikes are assumed to add linearly  since we intend to handle the overlap case  we have to use a set of features xm which obeys the following  given the features of two of the waveforms  then one can compute those of their overlap  a good such candidate is the set of the samples of the spike lrb or possibly also just part of the samples rrb  the added noise  partly thermal noise from the electrode and partly due to firings from distant neurons  can usually be approximated as white gaussian  let the variance be a 2  the integrals in the likelihood functions can be approximated as summations lrb note in fact that we have samples available  not a continuous waveform rrb  let yi represent the typical feature vector lrb template rrb associated with neuron n i  with the mth component being y    then m j lrb xib  lrb ki rrb  bj lrb kd rrb  lrb 21r rrb   2am exp lsb  2  2   l lrb x m  y n  k 1  y   k2 rrb 2 rsb 99 where xm is the mth component of x  and m is the dimension of x  this leads to the following likelihood functions m  l   f lrb bd k rrb rrb exp lsb  2  2 l kl   m 1 m rrb ll   f lrb b  lrb k rrb rrb f lrb bj lrb k rrb rrb l ml l kl   mlkl   ml m  l lrb xm  y  n kj 2 rsb m  l exp lsb  2  2 m l lrb x m  y n  k 1  y   kl rrb 2 rsb m  l where k is the spike instant  and the interval from  ml to m2 corresponds to the interval i defined at the beginning of the section  implementation the techniques we have just described were tested in the following way  for the first experiment we identified two spike classes in a recording from the rat cerebellum  a signal is created  composed of a number of spikes from the two classes at random instants  plus noise  to make the situation as realistic as possible  the added noise is taken from idle periods lrb i e non spiking rrb of a real recording  the reason for using such an artificially generated signal is to be able to know the class identities of the spikes  in order to test our approach quantitatively  we implement the detection and classification techniques on the obtained signal  with various values of noise amplitude  in our case the ratio of the peak to peak values of the templates turns out to be 1 375  also  the spike rate of one of the clases is twice that of the other class  fig 3a shows the results with applying the first scheme lrb i e using eq  3 rrb  the overall percentage correct classification for all spikes lrb solid curve rrb and the percentage correct classification for overlapping spikes lrb dashed curve rrb are plotted versus the standard deviation of the noise lrb rsb  normalized with respect to the peak h of the large template  notice that the overall classification accuracy is near 100  for lrb rsb  i h less than 0 15  which is actually the range of noise amplitudes we mostly encountered in our work with real recordings  observe also the good results for classifying overlapping events  we have applied also the second scheme lrb i e using eq  4 rrb and obtained similar results  we wish to mention that the thresholds for detection and for the reject option are set up so as to obtain no more than 3  falsely detected spikes  a similar experiment is performed with three waveforms lrb three classes rrb  where two of the waveforms are the same as those used in the first experiment  the third is the average of the first two  all the three neurons have the same spike rate lrb i e    1     2     3 rrb  hence both classification schemes are equivalent in this case  fig 3b shows the overall as well as the sub category of overlap classification results  one observes that the results are worse than those for the two class case  this is because the spacings between the templates are in general smaller  notice also that the accuracy in resolving overlapping events is now tangibly less than the overall accuracy  however  one can say that the results are acceptable in the range of lrb rsb  less than 0 1  the following experiment is also performed using the same data  we would like to investigate the importance of the information given by the lrb overall rrb firing rate on the problem of classifying overlapping events  in our method the summation in the likelihood functions for single spikes is multiplied by otln  while that for overlapping spikes is multiplied by lrb otln rrb 2  usually otln is considerably less than one  hence we have a factor which gives less weight for overlapping events  now  consider the case of ignoring completely the information given by the firing rate and relying solely on sha pe information  we assume that overlapping spikes from any two given classes represent  new  class of waveforms and that each of these overlap classes has the same rate as that of a single spike cla ss  in that case we can obtain expressions for the likelihood functions as consisting just the summations  i e free of the rate ih 100     1     1        51   c  e d   51      c    it   it   i i  s   1 111 i  isz   l   i i   t    1 1   i  a i  isz l  lti 1  b 1        c c         51    it   i i  1 11 t 1 1  1  i il   t c fig 3 a rrb overall lrb solid curve rrb and overlap lrb dashed curve rrb classification accuracy for a two class case b rrb overall lrb solid curve rrb and overlap lrb dashed curve rrb classification accuracy for a three class case c rrb percent of incorrect classification of single spikes as overlap solid curve  scheme utilzing the spike rate dashed curve  scheme not utilising the spike rate factor olin lrb refer to appendix rrb  an experiment is performed using that scheme lrb on the same three class data rrb  one observes that the method classifies a number of single spikes wrongly as overlaps  much more than our original scheme does lrb see fig 3c rrb  especially for the large noise case  on the other hand  the number of overlaps which are classified wrongly as single spikes is near zero for both schemes  finally  in the last experiment the techniques are implemented on real recordings from the rat cerebellum  the recorded signal is band pass filtered in the frequency range 300 hz  10 khz  then sampled with a rate of 20khz  for classification  we take 20 samples per spike as features  fig 4 shows the results ofthe proposed method  using the first scheme lrb eq  3 rrb  the number of neurons whose spikes are represented in the waveform is estimated to be four  the 101 detection threshold is set up so that spikes which are too small are disregarded  because they come from several neurons far away from the electrode and are hard to distinguish  notice the overlap of classes 1 and 2  which was detected  we used the second scheme also on the same portion and it gave similar results as those of the first scheme lrb only one of the spikes is classified differently rrb  overall  the discrepancies between classifications done by the proposed method and an experienced human observer were found to be small  3 2 3 3 4 1 2 3 3 1 2 2 3 1 3 1 3 2 4 fig 4 classification results for a recording from the rat cerebellum conclusion many researchers have considered the problem of spike classification in multi neuron recordings  but only few have tackled the case of spike overlap  which could occur frequently  particularly if the group of neurons under study is stimulated  in this work we propose a method for spike classification  which can also aid in detecting and classifying overlapping spikes  by taking into account the statistical properties of the discharges of the neurons sampled  this method minimizes the probability of classification error  the application of the method to artificial as well as real recordings confirm its effectiveness  appendix consider first p  i  we can write 102 we can also obtain r  it  t2 lsb t  t2 ij t t1 t tl f lrb x  aibi lrb t d  bj lrb t 2 rrb rrb f lrb b lrb t rrb b  lrb t rrb rrb dt dt f lrb a rrb i 1  j 2 1 2  x  now  consider the two events b1 lrb td and b j lrb t 2 rrb  in the absense of any information about their dependence  we assume that they are independent  we get within the interval i  both f lrb b  lrb tt rrb rrb and f lrb b j lrb t2 rrb rrb hardly vary because the duration of i is very small compared to a typical inter spike interval  therefore we get the following approximation  f lrb b  lrb td rrb  f lrb b  lrb t rrb rrb f lrb b j lrb t2 rrb rrb  f lrb bj lrb t rrb rrb  the expression for p  j p  j  becomes f lrb b  lrb t rrb rrb f lrb b  lrb t rrb rrb f lrb rrb j x  a lsb t  t2 t tl lsb t  t2 f lrb xib  lrb td  b j lrb t2 rrb rrb dt 1 dt 2  t tl notice that the term a was omitted from the argument of the density inside the integral  because the occurrence of two spikes at tl and t2el implies the occurrence of a  a similar derivation for  results in the term f lrb x  a rrb is common to all the pils and the pi s  hence one can simply compare the following likelihood functions  aeknow ledgement our thanks to dr yaser abu mostafa for his assistance with this work  this project was supported by the caltech program of advanced technology lrb sponsored by aerojet  gm  gte  and trw rrb  and the joseph drown foundation  references ii rsb m abeles and m goldstein  proc  ieee  65  pp 762 773  1977  12 rsb g dinning and a sanderson  ieee trans  bio  m ed  eng  bme 28  pp 804 812  1981  13 rsb e d hollander and g orban  ieee trans  bio med  eng  bme 26  pp 279 284  1979  14 rsb d mishelevich  ieee trans  bio med  eng  bmfr17  pp 147 150  1970  is rsb v prochazka and h kornhuber  electroenceph  din  neurophysiol  32  pp 91 93  1973  16 rsb w  roberts  bioi  gybernet  35  pp 73 80  1979  17 rsb w roberts and d hartline  brain res  94  pp 141 149  1975  18 rsb e schmidt  j neurosci  methods  12  pp 95 111  1984  19 rsb r duda and p hart  pattern classification and scene analysis  john wiley  1973 
25 en 422 computing motion using resistive networks christof koch  jin luo  carver mead california institute of technology  216 76  pasadena  ca 91125 james hutchinson jet propulsion laboratory  california institute of technology pasadena  ca 91125 introduction to us  and to other biological organisms  vision seems effortless  we open our eyes and we  see  the world in all its color  brightness  and movement  yet  we have great difficulties when trying to endow our machines with similar abilities  in this paper we shall describe recent developments in the theory of early vision which lead from the formulation of the motion problem as an illposed one to its solution by minimizing certain  cost  functions  these cost or energy functions can be mapped onto simple analog and digital resistive networks  thus  we shall see how the optical flow can be computed by injecting currents into resistive networks and recording the resulting stationary voltage distribution at each node  these networks can be implemented in cmos vlsi circuits and represent plausible candidates for biological vision systems  aperture problem and smoothness assumption in this study  we use intensity based schemes for recovering motion  let us derive an equation relating the change in image brightness to the motion of the image lrb see l rrb  let us assume that the brightness of the image is constant over time  di lrb   y  t rrb  dt  o on the basis of the chain rule of differentiation  this transforms into 81 d  8  dt 81 dy 81  8y dt  at  izu  iyv  it   v i  v  it  0  lrb 1 rrb where we define the velocity v as lrb u  v rrb  lrb d 1 rrb  dt  dy dt rrb  because we assume that we can compute these spatial and temporal image gradients  we are now left with a single linear equation in two unknowns  u and v  the two components of the velocity vector lrb aperture problem rrb  any measuring system with a finite aperture  whether biological or artificial  can only sense the velocity component perpendicular to the edge or along the spatial gradient lrb  it  1  v i i rrb  the component of motion perpendicular to the gradient can not  in principle  be registered  the problem remains unchanged even if we measure these velocity components at many points throughout the image  how can this problem be made well posed  that is  having a unique solution depending continuously on the data  one form of  regularizing  ill posed  american institute of physics 1988 423 problems is to restrict the class of admissible solutions by imposing appropriate constraints 2  applying this method to motion  we shall argue that in general objects are smooth except at isolated discontinuities undergoing smooth movements  thus  in general  neighboring points in the world will have similar velocities and the projected velocity field should reflect this fact  we therefore impose on the velocity field the constraint that it should be the smoothest as well as satisfying the data  as measure of smoothness we choose  the square of the velocity field gradient  the final velocity field lrb u  v rrb is the one that minimizes jj lsb lrb  rrb   lrb  rrb   lrb  rrb   lrb   rrb  rsb dz dy a       lrb 2 rrb  ii     i      al lrb b rrb fig 1  lrb a rrb the location of the horizontal lrb lfj rrb and vertical lrb iij rrb line processes relative to the motion field nngrid  lrb b rrb the hybrid resistive network  computing the optical flow in the presence of discontinuities  the conductances t c  ij connecting both grids depend on the brightness gradient  as do the conductances gij and gij connecting each node with the battery  for clarity  only two such elements are shown  the battery eij depends on both the temporal and the spatial gradient and is zero if no brightness change occurs  the  lrb resp  y rrb component of the velocity is given by the voltage in the top lrb resp  bottom rrb network  binary switches  which make or break the resistive connections between nodes  424 implement motion discontinuities  these switches could be under the control of distributed digital processors  analog cmos implementations are also feasible 3  the first term implements the constraint that the final solution should follow as closely as possible the measured data whereas the second term imposes the smoothness constraint on the solution  the degree to which one or the other terms are minimized is governed by the parameter rrb   if the data is very accurate  it should be  expensive  to violate the first term and rrb  will be small  if  conversely  the data is unreliable lrb low signal to noise rrb  much more emphasis will be placed on the smoothness term  horn and schunck1 first formulated this variational approach to the motion problem  the energy e lrb u  v rrb is quadratic in the unknown u and v  it then follows from standard calculus of variation that the associated euler lagrange equations will be linear in u and v  i  u  iziyv i z i 1i u  i  v  rrb   721   rrb   7 2 v  izit  iylt  0  o lrb 3 rrb we now have two linear equations at every point and our problem is therefore completely determined  analog resistive networks let us assume that we are formulating eqs  lrb 2 rrb and lrb 3 rrb on a discrete 2 d grid  such as the one shown in fig 1a  equation lrb 3 rrb then transforms into i  ijuij  iziji1iijvij  rrb  lrb uhlj  uij  l izijlyijuij  i  ijvij  rrb  lrb vhlj  vij  l   ui lj  uij l rrb  izijltij 4vij  vi lj  vij l rrb  iyijltij 4uij  0  0 lrb 4 rrb where we replaced the laplacian with its 5 point approximation on a rectangular grid  we shall now show that this set of linear equations can be solved naturally using a particular simple resistive network  let us apply kirchhoff s current law to the nodne i  j in the top layer of the resistive network shown in fig  lb  we then have the following update equation  du  c d    t lrb ui  lj  uij  l  gij lrb eij   4uij uij rrb  ui lj  uij l rrb  tc ij lrb vij lrb 5 rrb  uij rrb  where vij is the voltage at node i  j in the bottom network  once duij  dt  0 and dvij dt  0  this equation is seen to be identical with eq  lrb 4 rrb  if we identify 425 tc ij   izijiyij  ijlij rrb gij  iyii lrb izii  iyij rrb gij  izij lrb izij lrb 6 rrb  it eij   izii  iyij lrb a rrb lrb b rrb lrb c rrb lrb d rrb  lrb e rrb lrb f rrb fig 2  motion sequence using synthetic data  lrb a rrb and lrb b rrb two images of three high contrast squares on a homogeneous background  lrb c rrb the initial velocity data  the inside of both squares contain no data  lrb d rrb the final state 426 of the network after 240 iterations  corresponding to the smooth optical flow field  lrb e rrb optical flow in the presence of motion discontinuities lrb indicated by solid lines rrb  lrb f rrb discontinuities are strongly encouraged to form at the location of intensity edges 4  both lrb e rrb and lrb f rrb show the state of the hybrid network after six analog digital cycles  once we set the batteries and the conductances to the values indicated in eq  lrb 6 rrb  the network will settle following kirchhoff s laws into the state of least power dissipation  the associated stationary voltages correspond to the sought solution  uii is equivalent to the  c component and vii to the y component of the optical flow field  we simulated the behavior of these networks by solving the above circuit equations on parallel computers of the hypercube family  as boundary conditions we copied the initial velocity data at the edge of the image into the nodes lying directly adjacent but outside the image  the sequences in figs 2 and 3 illustrate the resulting optical flow for synthetic and natural images  as discussed by horn and schunck 1  the smoothness constraint leads to a qualitatively correct estimate of the velocity field  thus  one undifferentiated blob appears to move to the lower right and one blob to the upper left  however  at the occluding edge where both squares overlap  the smoothness assumption results in a spatial average of the two opposing velocities  and the estimated velocity is very small or zero  in parts of the image where the brightness gradient is zero and thus no initial velocity data exists lrb for instance  the interiors of the two squares rrb  the velocity estimates are simply the spatial average of the neighboring velocity estimates  these empty areas will eventually fill in from the boundary  similar to the how of heat for a uniform flat plate with  hot  boundaries  motion discontinuities the smoothness assumption of horn and schunck 1 regularizes the aperture problem and leads to the qualitatively correct velocity field inside moving objects  however  this approach fails to detect the locations at which the velocity changes abruptly or discontinuously  thus  it smoothes over the figure ground discontinuity or completely fails to detect the boundary between two objects with differing velocities because the algorithm combines velocity information across motion boundaries  a quite successful strategy for dealing with discontinuities was proposed by geman and geman 5  we shall not rigorously develop their approach  which is based on bayesian estimation theory lrb for details see 5 6 rrb  suffice it to say that a priori knowledge  for instance  that the velocity field should in general be smooth  can be formulated in terms of a markov random field model of the image  given such an image model  and given noisy data  we then estimate the  best  flow field by some likelihood criterion  the one we will use here 427 is the maximum a posteriori estimate  although other criteria are possible and have certain advantages 6  this can be shown to be equivalent to minimizing an expression such as eq  lrb 2 rrb  in order to reconstruct images consisting of piecewise constant segments  geman and geman5 further introduced the powerful idea of a line process 1  for our purposes  we will assume that a line process can be in either one of two states   on  lrb 1  1 rrb or  off  lrb 1  0 rrb  they are located on a regular lattice set between the original pixel lattice lrb see fig 1a rrb  such that each pixel i  j has a horizontallfi and a verticallij line process associated with it  if the appropriate line process is turned on  the smoothness term between the two adjacent pixels will be set to zero  in order to prevent line processes from forming everywhere and  furthermore  in order to incorporate additional knowledge regarding discontinuities into the line processes  we must include an additional term vc lrb l rrb into the new energy function  e lrb  il  v  lh  iv rrb  l lrb iz  ilii  iyvii  i rrb 2  t i i rrb  l lrb 1  it rrb lsb lrb  lli 1 i   llii rrb 2  lrb vi  li  vii rrb 2 rsb  lrb 7 rrb i i rrb  l lrb 1  iii rrb lsb lrb  llij  l   llii rrb 2  lrb vii 1  vij rrb 2 rsb  vc lrb l rrb  i i vc contains a number of different terms  penalizing or encouraging specific configurations of line processes  i  i i plus the corresponding expression for the vertical line process iii lrb obtained by interchanging i with j and iii with ifi rrb  the first term penalizes each introduction of a line process  since the cost c c has to be  payed  every time a line process is turned on  the second term prevents the formation of parallel lines  if either lfi  l or ifi 2 is turned on  this term will tend to prevent from turning on  the third term  civi  embodies the fact that in general  motion discontinuities occur along extended contours and rarely intersect lrb for more details see 7 rrb  we obtain the optical flow by minimizing the cost function in eq  lrb 7 rrb with respect to both the velocity v and the line processes ih  and iv  to find an optimal solution to this non quadratic minimization problem  we follow koch et a1  7 and use a purely deterministic algorithm  based on solving kirchhoff s equations for a mixed analogi digital network lrb see also 8 rrb  our algorithm exploits the fact that for a fixed distribution of line processes  the energy function lrb 7 rrb is quadratic  thus  we first initialize the analog resistive network lrb see fig 2b rrb according to eq  lrb 6 rrb and with no line processes on  the network then converges to it 428 the smoothest solution  subsequently  we update the line processes by deciding at each site of the line process lattice whether the overall energy can be lowered by setting or breaking the line processj that is  lfi will be turned on if e lrb u  v  lfi  1  iv rrb  e lrb u  v  ifi  0  iv rrb  otherwise  ifj  o line processes are switched on by breaking the appropriate resistive connection between the two neighboring nodes  after the completion of one such analog digital cycle  we reiterate and compute for the newly updated distribution of line processes the smoothest state of the analog network  although there is no guarantee that the system will converge to the global minimum  since we are using a gradient descent rule  it seems to find next to optimal solutions in about 10 to 15 analog digital cycles  lrb 8 rrb lrb c rrb lrb e rrb figure 3  optical flow of a moving person  lrb a rrb and lrb b rrb two 128 by 128 pixel images captured by a video camera  the person in the foreground is moving toward the right while the person in the background is stationary  the noise in the lower part of the image is a camera artifact  lrb c rrb zero crossings superimposed on the initial velocity data  lrb d rrb the smooth optical flow after 1000 iterations  note that the noise in the lower part of both images is completely smoothed away  lrb e rrb the final piecewise smooth optical flow  the velocity field is subsampled to improve visibility  the evolution of the hybrid network is shown after the 1  lrb a rrb  3  lrb b rrb  5  lrb c rrb  7  lrb d rrb  10  lrb e rrb  and 13  lrb f rrb analog digital cycle in the right part of the figure  the synthetic motion sequence in fig 2 demonstrates the effect of the line 429 processes  the optical flow outside the discontinuities approximately delineating the boundaries of the moving squares is zero  as it should be lrb fig 2e rrb  however  where the two squares overlap the velocity gradient is high and multiple intersecting discontinuities exist  to restrict further the location of discontinuities  we adopt a technique used by gamble and poggio4 to locate depth discontinuities by requiring that depth discontinuities coincide with the location of intensity edges  our rationale behind this additional constraint is that with very few exceptions  the physical processes and the geometry of the 3 dimensional scene giving rise to the motion discontinuity will also give rise to an intensity edge  as edges we use the zero crossings of a laplacian of a gaussian convolved with the original image9  we now add a new term vz cii to our energy function e  such that vz  cii is zero if iii is off or if iii is on and a zero crossing exists between locations i and j  if iii  1 in the absence of a zero crossing  v z  cii is set to 1000  this strategy effectively prevents motion discontinuities from forming at locations where no zero crossings exist  unless the data strongly suggest it  conversely  however  zero crossings by themselves will not induce the formation of discontinuities in the absence of motion gradients lrb figs 2f and 3 rrb  analog vlsi networks even with the approximations and optimizations described above  the computations involved in this and similar early vision tasks require minutes to hours on computers  it is fortunate then that modern integrated circuit technology gives us a medium in which extremely complex  analog real time implementations of these computational metaphors can be realized3  we can achieve a very compact implementation of a resistive network using an ordinary cmos process  provided the transistors are run in the sub threshold range where their characterstics are ideal for implementing low current analog functions  the effect of a resistor is achieved by a circuit configuration  such as the one shown in fig 4  rather than by using the resistance of a special layer in the process  the value of the resulting resistance can be controlled over three orders of magnitude by setting the bias voltages on the upper and lower current source transistors  the current voltage curve saturates above about 100 mvj a feature that can be used to advantage in many applications  when the voltage gradients are small  we can treat the circuit just as if it were a linear resistor  resistances with an effective negative resistance value can easily be realized  in two dimensions  the ideal configuration for a network implementation is shown in fig 4  each point on the hexagonal grid is coupled to six equivalent neighbors  each node includes the resistor apparatus  and a set of sample andhold circuits for setting the confidence and signal the input and output voltages  both the sample and hold circuits and the output buffer are addressed by a scanning mechanism  so the stored variables can be refreshed or updated  and the map of node voltages read out in real time  430  i  i v  vi lrb a rrb v lrb b rrb figure 4  circuit design for a resistive network for interpolating and smoothing noisy and sparsely sampled depth measurements  lrb a rrb circuit consisting of 8 transistors implementing a variable nonlinear resistance  lrb b rrb if the voltage gradient is below 100 mv its approximates a linear resistance  the voltage vt controls the maximum current and thus the slope of the resistance  which can vary between 1 mo and 1 go 3  this cmos circuit contains 20 by 20 grid points on a hexagonal lattice  the individual resistive elements with a variable slope controlled by vt correspond to the term governing the smoothness  a  at those locations where a depth measurement dij is present  the battery is set to this value lrb vin  d ij rrb and the value of the conductance g is set to some fixed value  if no depth data is present at that node  g is set to zero  the voltage at each node corresponds to the discrete values of the smoothed surface fitted through the noisy and sparse measurements 7  a 48 by 48 silicon retina has been constructed that uses the hexagonal network of fig 4 as a model for the horizontal cell layer in the vertebrate retinal 0  in this application  the input potentials were the outputs of logarithmic photoreceptors implemented via phototransistors and the potential difference across the conductance t formed an excellent approximation to the laplacian operator  discussion we have demonstrated in this study that the introduction of binary motion 431 discontinuities into the algorithm of horn and schunck1 leads to a dramatically improved performance  f their method  in particular for the optical flow in the presence of a number of moving non rigid objects  moreover  we have shown that the appropriate computations map onto simple resistive networks  we are now implementing these resistive networks into vlsi circuits  using subtheshold cmos technology  this approach is of general interest  because a great number of problems in early vision can be formulated in terms of similar non convex energy functions that need to be minimized  such as binocular stereo  edge detection  surface interpolation  structure from motion  etc 2 6 8  these networks share several features with biological neural networks  specifically  they do not require a system wide clock  they rely on many connections between simple computational nodes  they converge rapidly within several time constants and they are quite robust to hardware errors  another interesting feature is that our networks only consume very moderate amounts of powerj the entire retina chip requires about 100 j l w 10 acknowledgments  an early version of this model was developed and implemented in collaboration with a l yuille8  m avalos and a hsu wrote the code for the imaging technology system and e staats for the ncube  c k is supported by an onr research young investigator award and by the sloan and the powell foundations  c m is supported by onr and by the system development foundation  a portion of this research was carried out at the jet propulsion laboratory and was sponsored by nsf grant no  eet 8714710  and by nasa  references 1  horn  b k p and schunck  b g artif  intell  17 185 203 lrb 1981 rrb  2  poggio  t  torre  v and koch  c nature 317 314 319 lrb 1985 rrb  3  mead  c analog vlsi and neural systems  addison wesley  reading  ma lrb 1988 rrb  4  gamble  e and poggio  t artif  intell  lab  memo  no 970  mit  cambridge ma lrb 1987 rrb  5  geman  s and geman  d ieee trans  pami 6  721 741 lrb 1984 rrb  6  marroquin  j  mitter  s and poggio  t j am  stat  assoc 82  76 89 lrb 1987 rrb  7  koch  c  marroquin  j and yuille  a proc  natl acad  sci  usa 83  4263 4267 lrb 1986 rrb  8  yuille  a l artif  intell  lab  memo  no 987  mit  cambridge  ma lrb 1987 rrb  9  marr  d and hildreth  e c proc  r soc  lond  b 207  187 217 lrb 1980 rrb  10  sivilotti  m a  mahowald  m a and mead  c a in  1987 stanford vlsi conference  ed  p losleben  pp 295 312 lrb 1987 rrb 
26 en 387 neural net and traditional classifiers1 william y huang and richard p lippmann mit lincoln laboratory lexington  ma 02173  usa abstract  previous work on nets with continuous valued inputs led to generative procedures to construct convex decision regions with two layer perceptrons lrb one hidden layer rrb and arbitrary decision regions with three layer perceptrons lrb two hidden layers rrb  here we demonstrate that two layer perceptron classifiers trained with back propagation can form both convex and disjoint decision regions  such classifiers are robust  train rapidly  and provide good performance with simple decision regions  when complex decision regions are required  however  convergence time can be excessively long and performance is often no better than that of k nearest neighbor classifiers  three neural net classifiers are presented that provide more rapid training under such situations  two use fixed weights in the first one or two layers and are similar to classifiers that estimate probability density functions using histograms  a third  feature map classifier  uses both unsupervised and supervised training  it provides good performance with little supervised training in situations such as speech recognition where much unlabeled training data is available  the architecture of this classifier can be used to implement a neural net k nearest neighbor classifier  1  introduction neural net architectures can be used to construct many different types of classifiers lsb 7 rsb  in particular  multi layer perceptron classifiers with continuous valued inputs trained with back propagation are robust  often train rapidly  and provide performance similar to that provided by gaussian classifiers when decision regions are convex lsb 12 7 5 8 rsb  generative procedures demonstrate that such classifiers can form convex decision regions with two layer perceptrons lrb one hidden layer rrb and arbitrary decision regions with three layer perceptrons lrb two hidden layers rrb lsb 7 2 9 rsb  more recent work has demonstrated that two layer perceptrons can form non convex and disjoint decision regions  examples of hand crafted two layer networks which generate such decision regions are presented in this paper along with monte carlo simulations where complex decision regions were generated using back propagation training  these and previous simulations lsb 5 8 rsb demonstrate that convergence time with back propagation can be excessive when complex decision regions are desired and performance is often no better than that obtained with k nearest neighbor classifiers lsb 4 rsb  these results led us to explore other neural net classifiers that might provide faster convergence  three classifiers called   fixed weight    hypercube   and  feature map  classifiers  were developed and evaluated  all classifiers were tested on illustrative problems with two continuous valued inputs and two classes lrb a and b rrb  a more restricted set of classifiers was tested with vowel formant data  2  capabilities of two layer perceptrons multi layer perceptron classifiers with hard limiting nonlinearities lrb node outputs of 0 or 1 rrb and continuous valued inputs can form complex decision regions  simple constructive proofs demonstrate that a three layer perceptron lrb two hidden layers rrb can 1 this work was sponsored by the defense advanced research projects agency and the department of the air force  the views expressed are those of the authors and do not reflect the policy or position of the u s government   american institute of physics 1988 388 decision region for class a b   x2 2 b2  b4     1  1 i i i  lsb j        rrb  1 f  i i       i     i i i i o 2 3 fig 1  a two layer perceptron that form  di joint deci ion region  for cia  a lrb  haded area  rrb  connection weight  and node ojj eb are  hown in the left  hyperplane  formed by all hidden node  are drawn a  da hed line  with node labek arrow  on theu line  point to the half plane where the hidden node output i   high   form arbitrary decision regions and a two layer perceptron lrb one hidden layer rrb can form single convex decision regions lsb 7 2 9 rsb  recently  however  it has been demonstrated that two layer perceptrons can form decision regions that are not simply convex lsb 14 rsb  fig 1  for example  shows how disjoint decision regions can be generated using a two layer perceptron  the two disjoint shaded areas in this fig  represent the decision region for class a lrb output node has a  high  output  y  1 rrb  the remaining area represents the decision region for class b lrb output node has a  low  output  y  0 rrb  nodes in this fig  contain hard limiting nonlinearities  connection weights and node offsets are indicated in the left diagram  ten other complex decision regions formed using two layer perceptrons are presented in fig 2  the above examples suggest that two layer perceptrons can form decision regions with arbitrary shapes  we  however  know of no general proof of this capability  a 1965 book by nilson discusses this issue and contains a proof that two layer nets can divide a finite number of points into two arbitrary sets lrb lsb 10 rsb page 89 rrb  this proof involves separating m points using at most m  1 parallel hyperplanes formed by firstlayer nodes where no hyperplane intersects two or more points  proving that a given decision region can be formed in a two layer net involves testing to determine whether the boolean representations at the output of the first layer for all points within the decision region for class a are linearly separable from the boolean representations for class b  one test for linear separability was presented in 1962 lsb 13 rsb  a problem with forming complex decision regions with two layer perceptrons is that weights and offsets must be adjusted carefully because they interact extensively to form decision regions  fig 1 illustrates this sensitivity problem  here it can be seen that weights to one hidden node form a hyperplane which influences decision regions in an entire half plane  for example  small errors in first layer weights that results in a change in the slopes of hyperplanes bs and b6 might only slightly extend the al region but completely eliminate the a2 region  this interdependence can be eliminated in three layer perceptrons  it is possible to train two layer perceptrons to form complex decision regions using back propagation and sigmoidal nonlinearities despite weight interactions  fig 3  for example  shows disjoint decision regions formed using back propagation for the problem of fig 1  in this and all other simulations  inputs were presented alternately from classes a and b and selected from a uniform distribution covering the desired decision region  in addition  the back propagation rate of descent term  tj  was set equal to the momentum gain term  a and tj  a  01  small values for tj and a were necessary to guarantee convergence for the difficult problems in fig 2  other simulation details are 389  llll i iel i blel i 5 rrb i mj i 3 rrb  lrb 3 m1 i i i 9 rrb i i 6 rrb rm i i  10 rrb 4 rrb 1   ftfi r i i i i fig 2  ten complex deci6ion region6 formed by two layer perceptron6  the number6 a66igned to each ca6e are the  ca6e  number6 u6ed in the re6t of thi6 paper  as in lsb 5 8 rsb  also shown in fig 3 are hyperplanes formed by those first layer nodes with the strongest connection weights to the output node  these hyperplanes and weights are similar to those in the networks created by hand except for sign inversions  the occurrence of multiple similar hyperplanes formed by two nodes  and the use of node offsets with values near zero  3  comparative results of two layers vs three layers previous results lsb 5 8 rsb  as well as the weight interactions mentioned above  suggest that three layer perceptrons may be able to form complex decision regions faster with back propagation than two layer perceptrons  this was explored using monte carlo simulations for the first nine cases of fig 2  all networks have 32 nodes in the first hidden layer  the number of nodes in the second hidden layer was twice the number of convex regions needed to form the decision region lrb 2  4  6  4  6  6  8  6 and 6 for cases 1 through 9 respectively rrb  ten runs were typically averaged together to obtain a smooth curve of percentage error vs time lrb number of training trials rrb and enough trials were run lrb to a limit of 250 000 rrb until the curve appeared to flatten out with little improvement over time  the error curve was then low pass filtered to determine the convergence time  convergence time was defined as the time when the curve crossed a value 5 percentage points above the final percentage error  this definition provides a framework for comparing the convergence time of the different classifiers  it  however  is not the time after which error rates do not improve  fig 4 summarizes results in terms of convergence time and final percentage error  in those cases with disjoint decision regions  back propagation sometimes failed to form separate regions after 250 000 trials  for example  the two disjoint regions required in case 2 were never fully separated with 390     2  ji j 21     i 0   2   i 7 2      i 12 7   9 3   4 5 7 6 t   lsb   rsb  lrb 1      i i ii    i      r    r i ii 409  i i 11 9  i i i i i     i     i    i  i        o  2 2 4 6 fig 3  deci ion region  formed u  ing bacle propagation for ca e   of fig    thiele  olid line  repre ent deci  ion boundariu  da  hed line  and arrow  have the lame meaning a  in fig 1  only hyperplane  for hidden node  with large weight  to the output node are  hown  over 300 000 training trial  were required to form  eparote n gion   a two layer perceptron but were separated with a three layer perceptron  this is noted by the use of filled symbols in fig 4  fig 4 shows that there is no significant performance difference between two and three layer perceptrons when forming complex decision regions using back propagation training  both types of classifiers take an excessively long time lrb  100 000 trials rrb to form complex decision regions  a minor difference is that in cases 2 and 7 the two layer network failed to separate disjoint regions after 250 000 trials whereas the three layer network was able to do so  this  however  is not significant in terms of convergence time and error rate  problems that are difficult for the two layer networks are also difficult for the three layer networks  and vice versa  4  alternative classifiers results presented above and previous results lsb 5 8 rsb demonstrate that multi layer perceptron classifiers can take very long to converge for complex decision regions  three alternative classifiers were studied to determine whether other types of neural net classifiers could provide faster convergence  4 1  fixed weight classifiers fixed weight classifiers attempt to reduce training time by adapting only weights between upper layers of multi layer perceptrons  weights to the first layer are fixed before training and remain unchanged  these weights form fixed hyperplanes which can be used by upper layers to form decision regions  performance will be good if the fixed hyperplanes are near the decision region boundaries that are required in a specific problem  weights between upper layers are trained using back propagation as described above  two methods were used to adjust weights to the first layer  weights were adjusted to place hyperplanes randomly or in a grid in the region lrb 1  xl  x2  10 rrb  all decision regions in fig 2 fall within this region  hyperplanes formed by first layer nodes for  fixed random  and  fixed grid  classifiers for case 2 of fig 2 are shown as dashed lines in fig 5  also shown in this fig  are decision regions lrb shaded areas rrb formed 391 12 o 2 1ayers 10 o         x          8 error rate 6 04 2 ol   l   l   l   l   l   l   l   200000 l     convergence time fig 4  percentage error lrb top rrb and convergence time lrb bottom rrb for ca8e6 1 through 9 of fig 2 for two and three layer perceptron clauifier6 trained u6ing back propagation  filled 6ymbol6 indicate that 6eparate di6joint region6 were not formed after 250 000 triak using back propagation to train only the upper network layers  these regions illustrate how fixed hyperplanes are combined to form decision regions  it can be seen that decision boundaries form along the available hyperplanes  a good solution is possible for the fixed grid classifier where desired decision region boundaries are near hyperplanes  the random grid classifier provides a poor solution because hyperplanes are not near desired decision boundaries  the performance of a fixed weight classifier depends both on the placement of hyperplanes and on the number of hyperplanes provided  4 2  hypercube classifier many traditional classifiers estimate probability density functions of input variables for different classes using histogram techniques lsb 41  hypercube classifiers use this technique by fixing weights in the first two layers to break the input space into hypercubes lrb squares in the case of two inputs rrb  hypercube classifiers are similar to fixed weight classifiers  except weights to the first two layers are fixed  and only weights to output nodes are trained  hypercube classifiers are also similar in structure to the cmac model described by albus lsb 11  the output of a second layer node is  high  only if the input is in the hypercube corresponding to that node  this is illustrated in fig 6 for a network with two inputs  the top layer of a hypercube classifier can be trained using back propagation  a maximum likelihood approach  however  suggests a simpler training algorithm which consists of counting  the output of second layer node hi is connected to the output node corresponding to that class with greatest frequency of occurrence of training inputs in hypercube hi  that is  if a sample falls in hypercube hi  then it is classified as class lrb j  where lrb 1 rrb nj  o  ni  o for all lrb j f   lrb j  in this equation  ni  o is the number of training tokens in hypercube hi which belong to class lrb j  this will be called maximum likelihood lrb ml rrb training  it can be implemented by connection second layer node hi only to that output node corresponding to class lrb j in eq  lrb 1 rrb  in all simulations hypercubes covered the area lrb 1  xl  x2  10 rrb  392 grid random o fig 5  deci ion region  formed with  fixed random  and  fixed grid  clal6ifier  for ca e  from fig   ruing back propagation training  line   hown are hyperplane  formed by the fird layer node   shaded area  repre ent the deci ion region for clau a four bins created by fixed layers a b rcb trained layer  2 3 2 fixed layers  1 input fig 6  a hypercube clauifier lrb left rrb i  a three layer perceptron with fixed weight  to the fird two layen  and trainable weight  to output node   weights are initialized  uch that output  of nodes hi through h lrb left rrb are  high  only when the input i  in the corre ponding hypercube lrb right rrb  393 output lrb only one high rrb select lsb class with majority in top k supervised associative learning select top lsb k exemplars calculate correlation to stored exemplars unsupervised kohonen feature map learning ii  input fio  1  feature map clauifier  4 3  feature map classifier in many speech and image classification problems a large quantity of unlabeled training data can be obtained  but little labeled data is available  in such situations unsupervised training with unlabeled training data can substantially reduce the amount of supervised training required lsb 3 rsb  the feature map classifier shown in fig 7 uses combined supervised unsupervised training  and is designed for such problems  it is similar to histogram classifiers used in discrete observation hidden markov models lsb 11 rsb and the classifier used in lsb 6 rsb  the first layer of this classifier forms a feature map using a self organizing clustering algorithm described by kohonen lsb 6 rsb  in all simulations reported in this paper 10 000 trials of unsupervised training were used  after unsupervised training  first layer feature nodes sample the input space with node density proportional to the combined probability density of all classes  first layer feature map nodes perform a function similar to that of second layer hypercube nodes except each node has maximum output for input regions that are more general than hypercubes and only the output of the node with a maximum output is fed to the output nodes  weights to output nodes are trained with supervision after the first layer has been trained  back propagation  or maximum likelihood training can be used  maximum likelihood training requires ni 8 lrb eq  1 rrb to be the number of times first layer node i has maximum output for inputs from class 8  in addition  during classification  the outputs of nodes with ni 8  0 for all 8 lrb untrained nodes rrb are not considered when the first layer node with the maximum output is selected  the network architecture of a feature map classifier can be used to implement a k nearest neighbor classifier  in this case  the feedback connections in fig 7 lrb large circular summing nodes and triangular integrators rrb used to select those k nodes with the maximum outputs must be slightly modified  k is 1 for a feature map classifier and must be adjusted to the desired value of k for a k nearest neighbor classifier  5  comparison between classifiers the results of monte carlo simulations using all classifiers for case 2 are shown in fig 8  error rates and convergence times were determined as in section 3  all alter  394 percent correct fixed weight conventional hypercube feature map 12  8 4 0 tr ials 2500 convergence time 77k 1 i 2000 2 1ay 1500 1000   500 0 knn  id i i gauss 32 3  40 120 number ot hidden nodes fig 8  comparative performance of clauifier8for ca8e 2  training time of the feature map clauifier8 doe8 not include the 10 000 un8upervi8ed training trials  native classifiers had shorter convergence times than multi layer perceptron classifiers trained with back propagation  the feature map classifier provided best performance  with 1 600 nodes  its error rate was similar to that of the k nearest neighbor classifiers but it required fewer than 100 supervised training tokens  the larger fixed weight and hypercube classifiers performed well but required more supervised training than the feature map classifiers  these classifiers will work well when the combined probability density function of all classes varies smoothly and the domain where this function is non zero is known  in this case weights and offsets can be set such that hyperplanes and hypercubes cover the domain and provide good performance  the feature map classifier automatically covers the domain  fixed weight  random  classifiers performed substantially worse than fixed weight  grid  classifiers  back propagation training lrb bp rrb was generally much slower than maximum likelihood training lrb ml rrb  6  vowel classification multi layer perceptron  feature map  and traditional classifiers were tested with vowel formant data from peterson and barney lsb 11 rsb  these data had been obtained by spectrographic analysis of vowels in  hvd  context spoken by 67 men  women and children  first and second formant data of ten vowels was split into two sets  resulting in a total of 338 training tokens and 333 testing tokens  fig 9 shows the test data and the decision regions formed by a two layer percept ron classifier trained with back propagation  the performance of classifiers is presented in table i all classifiers had similar error rates  the feature map classifier with only 100 nodes required less than 50 supervised training tokens lrb 5 samples per vowel class rrb for convergence  the perceptron classifier trained with back propagation required more than 50 000 training tokens  the first stage of the feature map classifier and the multi layer perceptron classifier were trained by randomly selecting entries from the 338 training tokens after labels had been removed and using tokens repetitively  395 4000 d        o lrb rrb d   2000 f2 lrb liz rrb      head hid hod had hawed heard heed hud vho  d  hood  1000   500 1400 0 f1 lrb hz rrb fig 9  deciljion regionlj formed by a two layer network using bp after 200 000 training tokens from peterljon  lj steadyljtate vowel data lsb peterljon  1952 rcb  alljo shown are samplelj of the teljting ljet  legend ijhow example 0  the pronunciation of the 10 vowels and the error within each vowel  i algorithm i training table i tokens i  error i performance of classifiers on ijteady ijtate vowel data  396 7  conclusions neural net architectures form a flexible framework that can be used to construct many different types of classifiers  these include gaussian  k nearest neighbor  and multi layer perceptron classifiers as well as classifiers such as the feature map classifier which use unsupervised training  here we first demonstrated that two layer perceptrons lrb one hidden layer rrb can form non convex and disjoint decision regions  back propagation training  however  can be extremely slow when forming complex decision regions with multi layer perceptrons  alternative classifiers were thus developed and tested  all provided faster training and many provided improved performance  two were similar to traditional classifiers  one lrb hypercube classifier rrb can be used to implement a histogram classifier  and another lrb feature map classifier rrb can be used to implement a k nearest neighbor classifier  the feature map classifier provided best overall performance  it used combined supervised unsupervised training and attained the same error rate as a k nearest neighbor classifier  but with fewer supervised training tokens  furthermore  it required fewer nodes then a k nearest neighbor classifier  references lsb 1 rsb j s albus  brains  behavior  and robotics  mcgraw hill  petersborough  n h  1981  lsb 2 rsb d j burr   a neural network digit recognizer   in proceedings of the international conference on systems  man  and cybernetics  ieee  1986  lsb 3 rsb d b cooper and j h freeman   on the asymptotic improvement in the outcome of supervised learning provided by additional nonsupervised learning   ieee transactions on computers  vol  c 19  pp 1055 63  november 1970  lsb 4 rsb r o duda and p e hart  pattern classification and scene analysis  john wiley   sons  new york  1973  lsb 5 rsb w y huang and r p lippmann   comparisons between conventional and neural net classifiers   in 1st international conference on neural network  ieee  june 1987  lsb 6 rsb t kohonen  k makisara  and t saramaki   phonotopic maps  insightful representation of phonological features for speech recognition   in proceedings of the 7th international conference on pattern recognition  ieee  august 1984  lsb 7 rsb r p lippmann   an introduction to computing with neural nets   ieee a ssp magazine  vol  4  pp 4 22  april 1987  lsb 8 rsb r p lippmann and b gold   neural classifiers useful for speech recognition   in 1st international conference on neural network  ieee  june 1987  lsb 9 rsb i d longstaff and j f cross   a pattern recognition approach to understanding the multi layer perceptron   mem  3936  royal signals and radar establishment  july 1986  lsb 10 rsb n j nilsson  learning machines  mcgraw hill  n y  1965  lsb 11 rsb t parsons  voice and speech processing  mcgraw hill  new york  1986  lsb 12 rsb f rosenblatt  perceptrons and the theory of brain mechanisms  spartan books  1962  lsb 13 rsb r c singleton   a test for linear separability as applied to self organizing machines   in selforganization systems  1962  lrb m c yovits  g t jacobi  and g d goldstein  eds  rrb  pp 503524  spartan books  washington  1962  lsb 14 rsb a wieland and r leighton   geometric analysis of neural network capabilities   in 1st international conference on neural networks  ieee  june 1987 
